
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Inference acceleration: Overview &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_inference/inference_acceleration';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Basic prompt" href="../chapter_prompt/basic_prompt.html" />
    <link rel="prev" title="LLM Inference" href="inference_fundamentals.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">Seq2Seq, T5, And BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">LLM Dense Architectures Fundamentals</a></li>

<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">MOE sparse models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">LLM training fundamentals</a></li>

<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">LLM finetuning</a></li>


<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">LLM alignement and preference learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="inference_fundamentals.html">LLM Inference</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Inference acceleration: Overview</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">Basic prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">Advanced prompt techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Retrieval-Augmented Generation (RAG)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">Basic RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">Advanced rag techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">Vision transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">Information Retrieval Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">Application of LLM in IR</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/chapter_inference/inference_acceleration.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Inference acceleration: Overview</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Inference acceleration: Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-fundamental-challenge-of-llm-inference">The fundamental challenge of LLM inference</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-acceleration-quantization">Inference acceleration: Quantization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts">Basic Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-quantization-techniques">Standard quantization techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-matrix-multiplication">Quantized matrix multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-granularities">Quantization granularities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-tensor-quantization">Per-tensor quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-quantization">Per-channel quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-token-quantization">Per-token quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#groupwise-quantization">Groupwise quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-performance-trade-off-in-language-models">Quantization-performance trade-off in language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-quantization-techniques">Advanced quantization techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-int8">LLM.int8()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-quant">Smooth Quant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#awq">AWQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8">FP8</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-software">References and software</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="inference-acceleration-overview">
<h1>Inference acceleration: Overview<a class="headerlink" href="#inference-acceleration-overview" title="Link to this heading">#</a></h1>
<section id="the-fundamental-challenge-of-llm-inference">
<h2>The fundamental challenge of LLM inference<a class="headerlink" href="#the-fundamental-challenge-of-llm-inference" title="Link to this heading">#</a></h2>
<p>LLM inference is <strong>memory-IO bound, not compute bound</strong>. In other words, it currently takes more time to load 1MB of data to the GPU’s compute cores than it does for those compute cores to perform LLM computations on 1MB of data.</p>
<p>This is because LLM contains relative usually simple calculations (multiplication, addition, max-pooling, etc) that can be executed by high performing GPU parallel computing units.</p>
<p>This means that LLM inference throughput is largely determined by how large a batch you can fit into high-bandwidth GPU memory.</p>
<p>More specific reasons for LLM inference to be memory-bound is because of the following:</p>
<ul class="simple">
<li><p><strong>Model size</strong>: Modern LLMs often have billions of parameters, requiring significant memory just to store the model weights.</p></li>
<li><p><strong>Attention mechanism</strong>: Transformers, which most LLMs are based on, use attention mechanisms that can require large amounts of memory, especially for long sequences.</p></li>
<li><p><strong>Activations</strong>: Storing intermediate activations during inference can consume substantial memory.</p></li>
<li><p><strong>KV-cache</strong>: For autoregressive generation, maintaining the key-value cache for past tokens uses additional memory that grows with sequence length.</p></li>
</ul>
<p>While LLM inference is mainly memory-bound, the following factors also contributes to the computation intensity:</p>
<p>Matrix multiplications: The core operations in LLMs are matrix multiplications, which can be computationally intensive.
Nonlinear activations: Operations like softmax and layer normalization require additional computation.
Beam search: If used, beam search for text generation adds computational overhead.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="inference-acceleration-quantization">
<h1>Inference acceleration: Quantization<a class="headerlink" href="#inference-acceleration-quantization" title="Link to this heading">#</a></h1>
<section id="basic-concepts">
<h2>Basic Concepts<a class="headerlink" href="#basic-concepts" title="Link to this heading">#</a></h2>
<p><strong>Quantization</strong> is the process of using a finite number of low-precision values (usually int8) to approximate high-precision (usually float32) numbers with relatively low loss in inference precision.</p>
<p>The objective of quantization is to <strong>reduce memory usage</strong> and improve inference speed without significantly compromising performance.</p>
<p>In the development of different quantization methods, there are</p>
<ul class="simple">
<li><p>QAT (Quantization-Aware-Training), which involves retraining or fine-tuning by approximating the differential rounding operation. While QAT is popular for small neural models, it is rarely used for LLMs.</p></li>
<li><p>PTQ (Post-Training Quantization), which directly quantizes pre-trained LLM models. It requires a small amount of data for determining quantization parameters. This is the mainstream quantization method for LLMs.</p></li>
</ul>
<p>Quantization can be applied to different parts of model, including</p>
<ul class="simple">
<li><p>weights</p></li>
<li><p>activations</p></li>
<li><p>KV Cache</p></li>
</ul>
<p>with different levels of <strong>quantization granularities</strong>, including:</p>
<ul class="simple">
<li><p>per-tensor</p></li>
<li><p>per-token/per-channel</p></li>
<li><p>group-wise</p></li>
</ul>
</section>
<section id="standard-quantization-techniques">
<h2>Standard quantization techniques<a class="headerlink" href="#standard-quantization-techniques" title="Link to this heading">#</a></h2>
<p>To introduce standard quantization techniques, we take 16-bit floating-point model weight <span class="math notranslate nohighlight">\(W_{f16}\)</span> and its quantization into 8-bit integer as example.</p>
<p>The <strong>Absmax/Scale quantization</strong> technique scales <span class="math notranslate nohighlight">\(W_{f16}\)</span> into the 8-bit representation in the range of <span class="math notranslate nohighlight">\([-127,127]\)</span> via multiplying with</p>
<div class="math notranslate nohighlight">
\[s_{W} = \frac{127}{\max_{ij}|W_{f16}|},\]</div>
<p>which is equivalent to scaling the entire tensor to fit into the range of [0, 127]. Specificially, the value with minimal magnitude is mapped to 0 and the value with maximal magnitude is mapped to 127.</p>
<p>That is the 8-bit integer representation is given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-inference-acceleration-quantization-eq-absmax-quantization">
<span class="eqno">(2)<a class="headerlink" href="#equation-chapter-inference-acceleration-quantization-eq-absmax-quantization" title="Link to this equation">#</a></span>\[
W_{i8}=\operatorname{Round}\left(\frac{127 \cdot W_{f16}}{\max_{ij}|W_{f16}|}\right)=\operatorname{Round}\left(s_{W} \mathbf{W}_{f 16}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{Round}\)</span> indicates rounding to the nearest integer.</p>
<p>The <strong>de-quantization</strong> of <span class="math notranslate nohighlight">\(W_{i8}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\operatorname{DeQ}(W_{i8}) = \frac{W_{i8}}{s_{W}}.\]</div>
<p>In practice, there are chances where 8-bit integer value after the quantization is outside of the 8 bit representation. To mitigate this, we will add an additional clipping step,</p>
<div class="math notranslate nohighlight">
\[W_{i8} = \]</div>
<p><strong>Affline quantization</strong> shifts the input values such that its min value is mapped to -127 and its max value is mapped to 127.</p>
<p>and its into the full range <span class="math notranslate nohighlight">\([-127,127]\)</span> by scaling with the normalized dynamic range <span class="math notranslate nohighlight">\(n d_x\)</span> and then shifting by the zeropoint <span class="math notranslate nohighlight">\(z p_x\)</span>. With this affine transformation, any input tensors will use all bits of the data type, thus reducing the quantization error for asymmetric distributions.</p>
<p>For example, for ReLU outputs, in absmax quantization all values in <span class="math notranslate nohighlight">\([-127,0)\)</span> go unused, whereas in zeropoint quantization the full <span class="math notranslate nohighlight">\([-127,127]\)</span> range is used. Zeropoint quantization is given by the following equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{gathered}
n d_{x_{f 16}}=\frac{2 \cdot 127}{\max _{i j}\left(\mathbf{X}_{f 16}^{i j}\right)-\min _{i j}\left(\mathbf{X}_{f 16}^{i j}\right)} \\
z p_{x_{i 16}}=\left\lfloor\mathbf{X}_{f 16} \cdot \min _{i j}\left(\mathbf{X}_{f 16}^{i j}\right)\right\rceil \\
\mathbf{X}_{i 8}=\left\lfloor n d_{x_{f 16}} \mathbf{X}_{f 16}\right\rceil
\end{gathered}
\end{split}\]</div>
<p>The <strong>Round-to-Nearest (RTN) quantization</strong> is a basic method used in the process of quantizing neural networks.</p>
<p>For a given numerical value <span class="math notranslate nohighlight">\(r\)</span>, RTN applies the following quantization formula</p>
<div class="math notranslate nohighlight">
\[q = \operatorname{Clip}(\operatorname{Round}(\frac{r}{s}) + z, q_{min}, q_max)\]</div>
<p>where <span class="math notranslate nohighlight">\(s\)</span> is scaling parameter, <span class="math notranslate nohighlight">\(z\)</span> is the shifting parameter, and <span class="math notranslate nohighlight">\(q_{min}, q_{max}\)</span> are the clipping range.</p>
</section>
<section id="quantized-matrix-multiplication">
<h2>Quantized matrix multiplication<a class="headerlink" href="#quantized-matrix-multiplication" title="Link to this heading">#</a></h2>
<p>The modern GPU hardware can significantly speed up the matrix multiplication in the integer representation. As matrix multiplication involves accumulation operations, the typical convension for accumulation data types are</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Low-Precision Data Type</p></th>
<th class="head text-right"><p>Accumulation Data Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>float16</p></td>
<td class="text-right"><p>float16</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>bfloat16</p></td>
<td class="text-right"><p>float32</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>int16</p></td>
<td class="text-right"><p>int32</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>int6</p></td>
<td class="text-right"><p>int32</p></td>
</tr>
</tbody>
</table>
</div>
<p>The matrix multiplication between <span class="math notranslate nohighlight">\(X_{f16}\)</span> and <span class="math notranslate nohighlight">\(W_{f16}\)</span> can be approximated by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X_{f16}W_{f16} &amp;\approx  \operatorname{DeQ}(X_{i8}) \operatorname{DeQ}(W_{i8}) \\
                &amp;=\frac{X_{i8}}{s_{X}} \frac{W_{i8}}{s_{W}} \\
                &amp;=\frac{1}{s_{X}\cdot s_W} X_{i8}W_{i8} 
\end{aligned}
\end{split}\]</div>
<p>where the summation or accumation operation in matrix multiplication of <span class="math notranslate nohighlight">\(X_{i8}W_{i8}\)</span> will be conducted in int-32 data type.</p>
</section>
<section id="quantization-granularities">
<h2>Quantization granularities<a class="headerlink" href="#quantization-granularities" title="Link to this heading">#</a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>These different granularities offer trade-offs between quantization accuracy and computational efficiency. Per-tensor is the fastest but least accurate, per-channel/per-token is the most accurate but computationally expensive, and group-wise provides a balance between the two extremes</p>
</section>
<section id="per-tensor-quantization">
<h3>Per-tensor quantization<a class="headerlink" href="#per-tensor-quantization" title="Link to this heading">#</a></h3>
<p>Per-tensor quantization applies the same scaling factor and zero point to an entire tensor (usually a weight matrix or activation tensor). Each weight tensor and activation tensor in the model will have its own set of quantization parameters. Note that the biggest challenges for per-tensor quantization is that a single outlier can reduce the quantization precision of all other values.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 5 </span> (Per-tensor quantization)</p>
<section class="example-content" id="proof-content">
<p>Let’s consider a weight matrix <span class="math notranslate nohighlight">\(W\)</span>, the quantize <span class="math notranslate nohighlight">\(W\)</span> into int8, we have the following steps:</p>
<ol class="arabic simple">
<li><p>Find the minimum and maximum values in the entire tensor: min_val = -0.5, max_val = 0.8</p></li>
<li><p>Calculate the scale and zero point: scale = (max_val - min_val) / (2^8 - 1) = (0.8 - (-0.5)) / 255 ≈ 0.00510
zero_point = round(-min_val / scale) = round(0.5 / 0.00510) ≈ 98</p></li>
<li><p>Quantize the entire tensor using these parameters:
W_quant = round(W / scale) + zero_point</p></li>
</ol>
<p>In this case, all elements use the same scale (0.00510) and zero_point (98) for quantization and dequantization.</p>
</section>
</div></section>
<section id="per-channel-quantization">
<h3>Per-channel quantization<a class="headerlink" href="#per-channel-quantization" title="Link to this heading">#</a></h3>
<p>Per-channel (from model weights perspective) quantization applies different scaling factors and zero points to each channel in the tensor.
Explanation:
This method computes separate quantization parameters for each channel (in the case of weights) or each token (in the case of activations). This allows for more fine-grained quantization, potentially preserving more information.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 6 </span> (Per-channel quantization)</p>
<section class="example-content" id="proof-content">
<p>Let’s consider the same weight matrix <span class="math notranslate nohighlight">\(W\)</span> of shape (1024, 768), but now we’ll quantize each output channel separately.</p>
<p>To quantize this to int8 per-channel, for each of the 768 columns (channels):</p>
<ol class="arabic simple">
<li><p>Find the minimum and maximum values in that column</p></li>
<li><p>Calculate the scale and zero point for that column</p></li>
<li><p>Quantize the column using its specific parameters</p></li>
</ol>
<p>Each column in W_quant uses its own scale and zero_point for quantization and dequantization.</p>
</section>
</div></section>
<section id="per-token-quantization">
<h3>Per-token quantization<a class="headerlink" href="#per-token-quantization" title="Link to this heading">#</a></h3>
<p>Per-token (from activations perspective) quantization applies different scaling factors and zero points to each token in the tensor.</p>
<p>For a given activation tensor <span class="math notranslate nohighlight">\(A\)</span> of shape <span class="math notranslate nohighlight">\((B, S, H)\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(B\)</span> is the batch size</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> is the sequence length</p></li>
<li><p><span class="math notranslate nohighlight">\(H\)</span> is the hidden dimension</p></li>
</ul>
<p>There will be <span class="math notranslate nohighlight">\(S\)</span> sets of quantization parameters; each set of parameters is computed from the <span class="math notranslate nohighlight">\(H\)</span> hidden dimensionality values.</p>
</section>
<section id="groupwise-quantization">
<h3>Groupwise quantization<a class="headerlink" href="#groupwise-quantization" title="Link to this heading">#</a></h3>
<p>Group-wise quantization is a middle ground between per-tensor and per-channel quantization. It applies different quantization parameters to groups of channels or elements within a tensor.
Explanation:
In this approach, we divide the tensor into groups and compute separate quantization parameters for each group. This allows for more flexibility than per-tensor quantization while being more computationally efficient than per-channel quantization.
Example:
Let’s consider the same weight matrix W of shape (1024, 768), and we’ll use a group size of 32.
Original tensor (float32):
W = [[-0.5, 0.1, 0.7, …, 0.3],
[0.2, -0.4, 0.6, …, -0.1],
…
[0.8, -0.3, 0.5, …, 0.4]]
To quantize this to int8 with group-wise quantization:</p>
<p>Divide the 768 channels into 24 groups of 32 channels each.
For each group:
a. Find the minimum and maximum values in that group
b. Calculate the scale and zero point for that group
c. Quantize the group using its specific parameters</p>
<p>For example, for the first group (channels 0-31):
min_val_g1 = -0.5, max_val_g1 = 0.8
scale_g1 = (0.8 - (-0.5)) / 255 ≈ 0.00510
zero_point_g1 = round(0.5 / 0.00510) ≈ 98
For the second group (channels 32-63):
min_val_g2 = -0.6, max_val_g2 = 0.7
scale_g2 = (0.7 - (-0.6)) / 255 ≈ 0.00510
zero_point_g2 = round(0.6 / 0.00510) ≈ 118
And so on for all 24 groups.
Resulting quantized tensor (int8):
W_quant = [[98, 255, 235, …, 156, | 137, 0, 215, …, 78, | …],
[137, 0, 215, …, 78,  | 255, 51, 196, …, 176, | …],
…
[255, 51, 196, …, 176, | 98, 255, 235, …, 156, | …]]
In this case, each group of 32 channels shares the same scale and zero_point for quantization and dequantization.</p>
</section>
</section>
<section id="quantization-performance-trade-off-in-language-models">
<h2>Quantization-performance trade-off in language models<a class="headerlink" href="#quantization-performance-trade-off-in-language-models" title="Link to this heading">#</a></h2>
<p>Early research [<span id="id1">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1493" title="Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. 2021. URL: https://arxiv.org/abs/2109.12948, arXiv:2109.12948.">BNB21</a>]</span>] during the BERT era revealed significant challenges in quantizing large language models. <span id="id2">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1493" title="Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. 2021. URL: https://arxiv.org/abs/2109.12948, arXiv:2109.12948.">BNB21</a>]</span> demonstrated that applying round-to-nearest (RTN) quantization to both weights and activations of BERT models, reducing them to 8-bit precision, resulted in substantial performance deterioration on language understanding benchmarks.</p>
<p>Further ablation shows that quantization on activation is major cause of the performance drop and quantization on the model weights have minimal impact. The reason is that activation values from FFN’s input and output can have strong outliers, which can directly cause notable error in the quantization process.</p>
<p>As summary in the following table [<span id="id3">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1493" title="Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. 2021. URL: https://arxiv.org/abs/2109.12948, arXiv:2109.12948.">BNB21</a>]</span>], a strategy of quantizing only the model weights to 8-bit precision while maintaining 32-bit precision for activations (referred to as ‘W8A32’) achieved performance comparable to full-precision models. This finding highlights the importance of selective quantization strategies that preserve critical information in activations while still benefiting from the efficiency gains of weight quantization.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Configuration</p></th>
<th class="head text-center"><p>CoLA</p></th>
<th class="head text-center"><p>SST-2</p></th>
<th class="head text-center"><p>MRPC</p></th>
<th class="head text-center"><p>STS-B</p></th>
<th class="head text-center"><p>QQP</p></th>
<th class="head text-center"><p>MNLI</p></th>
<th class="head text-center"><p>QNLI</p></th>
<th class="head text-center"><p>RTE</p></th>
<th class="head text-center"><p>GLUE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FP32</p></td>
<td class="text-center"><p>57.27</p></td>
<td class="text-center"><p>93.12</p></td>
<td class="text-center"><p>88.36</p></td>
<td class="text-center"><p>89.09</p></td>
<td class="text-center"><p>89.72</p></td>
<td class="text-center"><p>84.91</p></td>
<td class="text-center"><p>91.58</p></td>
<td class="text-center"><p>70.40</p></td>
<td class="text-center"><p>83.06</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>W8A8</p></td>
<td class="text-center"><p>54.74</p></td>
<td class="text-center"><p>92.55</p></td>
<td class="text-center"><p>88.53</p></td>
<td class="text-center"><p>81.02</p></td>
<td class="text-center"><p>83.81</p></td>
<td class="text-center"><p>50.31</p></td>
<td class="text-center"><p>52.32</p></td>
<td class="text-center"><p>64.98</p></td>
<td class="text-center"><p>71.03</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>W32A8</p></td>
<td class="text-center"><p>56.70</p></td>
<td class="text-center"><p>92.43</p></td>
<td class="text-center"><p>86.98</p></td>
<td class="text-center"><p>82.87</p></td>
<td class="text-center"><p>84.70</p></td>
<td class="text-center"><p>52.80</p></td>
<td class="text-center"><p>52.44</p></td>
<td class="text-center"><p>53.07</p></td>
<td class="text-center"><p>70.25</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>W8A32</p></td>
<td class="text-center"><p>58.63</p></td>
<td class="text-center"><p>92.55</p></td>
<td class="text-center"><p>88.74</p></td>
<td class="text-center"><p>89.05</p></td>
<td class="text-center"><p>89.72</p></td>
<td class="text-center"><p>84.58</p></td>
<td class="text-center"><p>91.43</p></td>
<td class="text-center"><p>71.12</p></td>
<td class="text-center"><p>83.23</p></td>
</tr>
</tbody>
</table>
</div>
<p>As the model size continues to grow to billions of parameters, outlier features of high magnitude start to emerge in all transformer layers, causing failure of simple low-bit quantization. Dettmers et al. (2022) observed such a phenomenon for OPT models larger than 6.7B parameters. Larger models have more layers with extreme outliers and these outlier features have a significant impact on the model performance. The scale of activation outliers in a few dimensions can be <span class="math notranslate nohighlight">\(\sim 100 \times\)</span> larger than most of the other values.</p>
<p>As language models grow to encompass billions of parameters, a significant challenge emerges: the appearance of high-magnitude outlier features across all transformer layers. This phenomenon compromises the effectiveness of simple low-bit quantization techniques. <span id="id4">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1494" title="Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022. URL: https://arxiv.org/abs/2208.07339, arXiv:2208.07339.">DLBZ22</a>]</span> identified this issue in OPT models exceeding 6.7 billion parameters.</p>
<p>The problem intensifies with model size; larger models exhibit more layers with extreme outliers. These outlier features disproportionately influence model performance. In some dimensions, the scale of activation outliers can be approximately 100 times larger than the majority of other values.</p>
<p>This disparity poses a significant challenge for quantization, as traditional methods struggle to accurately represent both the outliers and the more typical values within the same low-bit format. Consequently, addressing these outliers has become a critical focus in the development of quantization techniques for large language models.</p>
</section>
<section id="advanced-quantization-techniques">
<h2>Advanced quantization techniques<a class="headerlink" href="#advanced-quantization-techniques" title="Link to this heading">#</a></h2>
<section id="llm-int8">
<h3>LLM.int8()<a class="headerlink" href="#llm-int8" title="Link to this heading">#</a></h3>
<p><span id="id5">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1494" title="Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022. URL: https://arxiv.org/abs/2208.07339, arXiv:2208.07339.">DLBZ22</a>]</span></p>
<p>Motivation</p>
<p>Basic quantization methods often resulted in significant performance degradation, especially for larger models.</p>
<p>maintain high performance while significantly reducing the memory footprint and computational requirements of large language models. This makes it possible to run these models on more modest hardware or to scale them more efficiently in production environments.</p>
<p>The key idea is:</p>
<ul class="simple">
<li><p>Group-wise quantization: Instead of quantizing the entire model uniformly, the method divides weight matrices into groups and quantizes each group separately. This allows for more fine-grained representation of the weights.</p></li>
<li><p>Outlier handling: The method identifies and separates outlier values before quantization. These outliers are stored in 16-bit precision, while the rest of the weights are quantized into 8-bit int.</p></li>
</ul>
<p>Method details:</p>
<p>For a given input matrix <span class="math notranslate nohighlight">\(\mathbf{X}_{f 16} \in \mathbb{R}^{s \times h}\)</span>,</p>
<ul class="simple">
<li><p>First identify the subset of hidden dimensions that have at least one outliers based on certain magnitude criterion. We denote these dimensions by <span class="math notranslate nohighlight">\(O=\{i \mid i \in \mathbb{Z}, 0 \leq i \leq h\}\)</span>.</p></li>
<li><p>For columns of <span class="math notranslate nohighlight">\(X\)</span> and rows in <span class="math notranslate nohighlight">\(W\)</span> that reside in <span class="math notranslate nohighlight">\(O\)</span>, we preserve its 16-bit precision; for the remaining columns and rows, we quantize into 8-bit precision.</p></li>
<li><p>The final resulting matrix can be represented by the adding inner products together, that is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbf{C}_{f 16} \approx \sum_{h \in O} \mathbf{X}_{f 16}^h \mathbf{W}_{f 16}^h+\mathbf{S}_{f 16} \cdot \sum_{h \notin O} \mathbf{X}_{i 8}^h \mathbf{W}_{i 8}^h
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_{f 16}\)</span> is the denormalization term for the Int8 inputs and weight matrices <span class="math notranslate nohighlight">\(\mathbf{X}_{i 8}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{W}_{i 8}\)</span>.</p>
<p>It is found that 99.9% values can be represented by 8-bit int.</p>
<figure class="align-default" id="chapter-inference-quantization-fig-llm-int8-illustration-plot">
<a class="reference internal image-reference" href="../../_images/LLM_int8_illustration.png"><img alt="../../_images/LLM_int8_illustration.png" src="../../_images/LLM_int8_illustration.png" style="width: 817.5px; height: 320.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text">Figure 2: Schematic of LLM.int8(). Given 16-bit floating-point inputs <span class="math notranslate nohighlight">\(\mathbf{X}_{f 16}\)</span> and weights <span class="math notranslate nohighlight">\(\mathbf{W}_{f 16}\)</span>, the features and weights are decomposed into sub-matrices of large magnitude features and other values. The outlier feature matrices are multiplied in 16 -bit. All other values are multiplied in 8 -bit. We perform 8 -bit vector-wise multiplication by scaling by row and column-wise absolute maximum of <span class="math notranslate nohighlight">\(\mathbf{C}_x\)</span> and <span class="math notranslate nohighlight">\(\mathbf{C}_w\)</span> and then quantizing the outputs to Int8. The Int32 matrix multiplication outputs <span class="math notranslate nohighlight">\(\mathrm{Out}_{i 32}\)</span> are dequantization by the outer product of the normalization constants <span class="math notranslate nohighlight">\(\mathbf{C}_x \otimes \mathbf{C}_w\)</span>. Finally, both outlier and regular outputs are accumulated in 16-bit floating point outputs. Image from <span id="id6">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1494" title="Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022. URL: https://arxiv.org/abs/2208.07339, arXiv:2208.07339.">DLBZ22</a>]</span>.</span><a class="headerlink" href="#chapter-inference-quantization-fig-llm-int8-illustration-plot" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-inference-quantization-fig-llm-int8-performance-plot">
<a class="reference internal image-reference" href="../../_images/LLM_int8_performance_plot.png"><img alt="../../_images/LLM_int8_performance_plot.png" src="../../_images/LLM_int8_performance_plot.png" style="width: 593.4px; height: 489.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text">OPT model mean zeroshot benchmark accuracy at different quantization settings, including 16-bit baseline, regular 8-bit quantization method, and the LLM.int8() quantization method. Systematic outliers
emerge at a scale of 6.7B parameters, causing regular quantatization methods to have severe performance degradation. Image from <span id="id7">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1494" title="Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022. URL: https://arxiv.org/abs/2208.07339, arXiv:2208.07339.">DLBZ22</a>]</span>.</span><a class="headerlink" href="#chapter-inference-quantization-fig-llm-int8-performance-plot" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="smooth-quant">
<h3>Smooth Quant<a class="headerlink" href="#smooth-quant" title="Link to this heading">#</a></h3>
</section>
<section id="awq">
<h3>AWQ<a class="headerlink" href="#awq" title="Link to this heading">#</a></h3>
</section>
<section id="gptq">
<h3>GPTQ<a class="headerlink" href="#gptq" title="Link to this heading">#</a></h3>
</section>
<section id="fp8">
<h3>FP8<a class="headerlink" href="#fp8" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="references-and-software">
<h2>References and software<a class="headerlink" href="#references-and-software" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></p>
<p>Quantization:
<a class="reference external" href="https://leimao.github.io/article/Neural-Networks-Quantization/">https://leimao.github.io/article/Neural-Networks-Quantization/</a></p>
<p>:bibliography:<code class="docutils literal notranslate"><span class="pre">../llm_book.bib</span></code></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_inference"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="inference_fundamentals.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LLM Inference</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_prompt/basic_prompt.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Basic prompt</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Inference acceleration: Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-fundamental-challenge-of-llm-inference">The fundamental challenge of LLM inference</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-acceleration-quantization">Inference acceleration: Quantization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts">Basic Concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-quantization-techniques">Standard quantization techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-matrix-multiplication">Quantized matrix multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-granularities">Quantization granularities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-tensor-quantization">Per-tensor quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-channel-quantization">Per-channel quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-token-quantization">Per-token quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#groupwise-quantization">Groupwise quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-performance-trade-off-in-language-models">Quantization-performance trade-off in language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-quantization-techniques">Advanced quantization techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-int8">LLM.int8()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-quant">Smooth Quant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#awq">AWQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gptq">GPTQ</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fp8">FP8</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-software">References and software</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>