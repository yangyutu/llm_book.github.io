
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>19. Decoding &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_inference/inference_fundamentals';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="20. Inference Acceleration (WIP)" href="inference_acceleration.html" />
    <link rel="prev" title="18. DeepSeek Series (WIP)" href="../chapter_LLM_case_study/deepseek_series.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reasoning.html">14. LLM Reasoning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/llama_series.html">17. Llama Series (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/deepseek_series.html">18. DeepSeek Series (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">23. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">24. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">25. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">26. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">27. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">28. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">29. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/conversational_IR.html">30. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">31. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">32. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Decoding</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-fundamentals">19.1. Decoding Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-decoding">19.1.1. Greedy decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-decoding">19.1.2. Beam search decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">19.1.2.1. Basics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">19.1.2.2. Computational complexity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#controling-beam-search-behavior">19.1.2.3. Controling beam search behavior</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-controlled-sampling">19.2. Temperature-controlled sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-basics">19.2.1. The basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#top-k-and-top-p-sampling">19.2.2. Top-k and top-p sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-decoding">19.3. Multi-step Decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding">19.3.1. Speculative Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multihead-decoding-medusa">19.3.2. Multihead Decoding (Medusa)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">19.4. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="decoding">
<h1><span class="section-number">19. </span>Decoding<a class="headerlink" href="#decoding" title="Link to this heading">#</a></h1>
<section id="decoding-fundamentals">
<h2><span class="section-number">19.1. </span>Decoding Fundamentals<a class="headerlink" href="#decoding-fundamentals" title="Link to this heading">#</a></h2>
<p>In typical language modeling frameworks, a language model takes preceding words as context and outputs a probabilistic distribution of the next word over a pre-defined vocabulary.
The overall goal of decoding is to convert the probabilistic outputs iteratively to generate a sequence of words that meet the requirement of intended applications.</p>
<p>Decoding introduces a few challenges that are vastly different from typical NLU (natural language understanding) tasks:</p>
<ul class="simple">
<li><p>The decoding involves iterative forword pass of a model taking updated inputs. As a result, decoding has significantly more computation cost than typical NLU tasks that simply involves one forward pass of a model.</p></li>
<li><p>The quality (i.e., fluency and cohesion) and diversity of the generated text depends on the choice of language model, decoding methods, and their associated hyper-parameters.</p></li>
</ul>
<p>At the heart of decoding is to maximize the probability of the generated sequence of tokens <span class="math notranslate nohighlight">\(y_1,...,y_t\)</span> given the input <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \underset{y}{\operatorname{argmax}} P(y_1,...,y_t|x).\]</div>
<p>Since it is difficult optimize <span class="math notranslate nohighlight">\(P(\mathbf{y} \mid \mathbf{x})\)</span> directly, it is common to use the chain rule of probability to factorize it as a product of conditional probabilities</p>
<div class="math notranslate nohighlight">
\[
P\left(y_1, \ldots, y_t \mid \mathbf{x}\right)=\prod_{t=1}^N P\left(y_t \mid y_{&lt;t}, \mathbf{x}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{&lt;t}\)</span> is a shorthand notation for the sequence <span class="math notranslate nohighlight">\(y_1, \ldots, y_{t-1}\)</span>. The problem of generating the most probable sequence now amounts to carefully selecting each word at each step given the preceding words in a sentence such that the probability of final sequence is maximized.</p>
<p>The model takes the sequence <span class="math notranslate nohighlight">\(y_{&lt;t}, \mathbf{x}\)</span> as the input, and is trained to output the conditional probabilties of <span class="math notranslate nohighlight">\(P\left(y_t \mid y_{&lt;t}, \mathbf{x}\right)\)</span>.</p>
<section id="greedy-decoding">
<h3><span class="section-number">19.1.1. </span>Greedy decoding<a class="headerlink" href="#greedy-decoding" title="Link to this heading">#</a></h3>
<p>The simplest decoding method is greedy decoding, in which we at each step greedily select the token with the highest model predicted probability:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_t=\underset{y_t}{\operatorname{argmax}} P\left(y_t \mid y_{&lt;t}, \mathbf{x}\right).
\]</div>
<p>For example, suppose the language model produces a conditional probability <span class="math notranslate nohighlight">\(p_{t, i}\)</span> for token <span class="math notranslate nohighlight">\(i\)</span> in the vocabulary at step <span class="math notranslate nohighlight">\(t\)</span> given its preceding words, we will just take the token <span class="math notranslate nohighlight">\(j\)</span> that has the maximum value <span class="math notranslate nohighlight">\(p_{t,j}\)</span>.</p>
<figure class="align-default" id="chapter-inference-fig-inference-fundamentals-greedy-decoding-demo">
<a class="reference internal image-reference" href="../../_images/greedy_decoding_demo.png"><img alt="../../_images/greedy_decoding_demo.png" src="../../_images/greedy_decoding_demo.png" style="width: 735.6px; height: 373.20000000000005px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19.1 </span><span class="caption-text">Greedy decoding demonstrations.</span><a class="headerlink" href="#chapter-inference-fig-inference-fundamentals-greedy-decoding-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>While being efficient, greedy search decoding often fails to produce sequences that have high probability. The reason is that the token at each step is chosen without considering its impact on the subsequent tokens. In practice, the model may produce repetitive output sequences. This happens when the model identifies that certain words following each other with high probability. The root cause could be similar patterns are seen frequently during the pretraining stage.</p>
<p>In the following, we will introduce beam search and sampling methods to mitigate the drawbacks of greedy decoding.</p>
</section>
<section id="beam-search-decoding">
<span id="chapter-inference-sec-deconding-beam-search"></span><h3><span class="section-number">19.1.2. </span>Beam search decoding<a class="headerlink" href="#beam-search-decoding" title="Link to this heading">#</a></h3>
<section id="basics">
<h4><span class="section-number">19.1.2.1. </span>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h4>
<p>Instead of decoding the token with the highest probability at each step, beam search keeps track
of the top <span class="math notranslate nohighlight">\(B\)</span> most probable candidate sequences or hypotheses when selecting next-tokens. Here <span class="math notranslate nohighlight">\(B\)</span> is referred to the beam width or the number of hypotheses. By selecting the <span class="math notranslate nohighlight">\(B\)</span> next tokens from the vocabulary for extensions, we form <span class="math notranslate nohighlight">\(B\)</span> most likely new sequences as the next set of beams. This process is repeated until each sequence reaches the maximum length or an EOS token.</p>
<figure class="align-default" id="chapter-inference-fig-inference-fundamentals-beam-decoding-demo">
<a class="reference internal image-reference" href="../../_images/beam_decoding_demo.png"><img alt="../../_images/beam_decoding_demo.png" src="../../_images/beam_decoding_demo.png" style="width: 596.4px; height: 397.20000000000005px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19.2 </span><span class="caption-text">Beam Search decoding demonstrations.</span><a class="headerlink" href="#chapter-inference-fig-inference-fundamentals-beam-decoding-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Formally, we denote the set of <span class="math notranslate nohighlight">\(B\)</span> hypotheses tracked by beam search at the start of step <span class="math notranslate nohighlight">\(n\)</span> as <span class="math notranslate nohighlight">\(Y_{n-1}=\left\{\mathbf{y}_{1:n-1}^{(1)}, \ldots, \mathbf{y}_{1:n-1}^{(B)}\right\}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{y}_{1:n-1}^{(j)}\)</span> is hypothesis <span class="math notranslate nohighlight">\(j\)</span> consisting of <span class="math notranslate nohighlight">\(n-1\)</span> tokens. At each step, all possible single token extensions of these <span class="math notranslate nohighlight">\(B\)</span> beams given by the set <span class="math notranslate nohighlight">\(\mathcal{Y}_n=Y_{n-1} \times \mathcal{V}\)</span> and selects the <span class="math notranslate nohighlight">\(B\)</span> most likely extensions. At each step, we are selecting top <span class="math notranslate nohighlight">\(B\)</span> scored candidates from all <span class="math notranslate nohighlight">\(B \times|\mathcal{V}|\)</span> members of <span class="math notranslate nohighlight">\(\mathcal{Y}_t\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[
Y_{n}=\underset{Y_{n} \in \mathcal{Y}_t}{\operatorname{argmax}} \operatorname{Score}\left(\mathbf{y}_{1:n}\right).\]</div>
<p>The most commonly used score function is the sum of negative log-likelihoods of the sequence, discounted by a length-related factor. For example,</p>
<div class="math notranslate nohighlight">
\[ \operatorname{Score}\left(\mathbf{y}_{1:L}\right) = -\frac{1}{L^\alpha} \log P\left(y_1, \ldots, y_n \right)=-\frac{1}{L^\alpha} \sum_{n=1}^L \log P\left(y_{n} \mid y_1, \ldots, y_{n-1}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is the length of candidate sequence and <span class="math notranslate nohighlight">\(\alpha\)</span> is a scalar (e.g., 0.75)tuning the length discount factor. Since a longer sequence has more terms in the summation and thus tend to get higher scores, the term <span class="math notranslate nohighlight">\(L^\alpha\)</span> in the denominator penalizes long sequences.</p>
</section>
<section id="computational-complexity">
<h4><span class="section-number">19.1.2.2. </span>Computational complexity<a class="headerlink" href="#computational-complexity" title="Link to this heading">#</a></h4>
<p>Beam search has a computational complexity given by <span class="math notranslate nohighlight">\(O(LB|V|)\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> is the length of the sequence</p></li>
<li><p><span class="math notranslate nohighlight">\(B\)</span> is the beam width</p></li>
<li><p><span class="math notranslate nohighlight">\(|V|\)</span> is the vocabulary size (i.e., the search space size on each step).</p></li>
</ul>
<p>In principle, beam search cannot guarantee the finding of optimal sequence, but it is much more effective than brute force apporach (which has time complexity of <span class="math notranslate nohighlight">\(O(|V|^L)\)</span>).</p>
<p>We can tune the diversity or length of generated text by incorporating other elements into the scoring function. For example, we can penalize long sequence by multiplying by log-likelihoods by a length-dependent factor.</p>
</section>
<section id="controling-beam-search-behavior">
<h4><span class="section-number">19.1.2.3. </span>Controling beam search behavior<a class="headerlink" href="#controling-beam-search-behavior" title="Link to this heading">#</a></h4>
<p>Since beam-decoding is optimizing a user-defined score function, we can incorpoate various penalties or rewards into the score function to control the diversity, brevity, smoothness, etc. of the generated sequence<span id="id1">[<a class="reference internal" href="#id1498" title="Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.">VCS+16</a>]</span>.
For example,</p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(s\)</span> be the original score, we can add length penalty by using score <span class="math notranslate nohighlight">\(s\exp(-\alpha l)\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is a scalar and <span class="math notranslate nohighlight">\(l\)</span> is the length of the generated sequence. <span class="math notranslate nohighlight">\(\alpha &gt; 0.0\)</span> means that the beam score is penalized by the sequence length; <span class="math notranslate nohighlight">\(\alpha  &lt; 0.0\)</span> is used to encourage the model to generate longer sequence.</p></li>
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">min_length</span></code> to force the model to not produce an EOS (end of sentence) token before <code class="docutils literal notranslate"><span class="pre">min_length</span></code> is reached.</p></li>
<li><p>One can also introduce n-grams repetition penalty. For example, one can specify a hard penalty to ensure that no n-gram appears twice. Alternatively, one can also introduce soft penalty\cite{keskar2019ctrl}. For instance, given a list of generated tokens <span class="math notranslate nohighlight">\(G\)</span>, the probability distribution for the next token <span class="math notranslate nohighlight">\(p_i\)</span> is defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_i=\frac{\exp \left(z_i /(T \cdot I(i \in g))\right.}{\sum_j \exp \left(z_j /(T \cdot I(j \in g))\right.} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(I(c)=\theta\)</span> if <span class="math notranslate nohighlight">\(c\)</span> is True else 1.0.</p>
<p>Beam search aims to generate the most probable sequence, which however is not necessary of high quality from human language perspective. Human language does not necessarily follow a distribution of high probability next words, in particularly in creative writing or dialog generation. In other words, as humans, we want generated text to surprise us and not to be boring/predictable.</p>
<p>For applications like machine translation or summarization, we can specify beam search parameters to generate text of desired length. However, for tasks like story generation, the detailed length is often difficult to predict, which imposes a challenge to beam search.</p>
</section>
</section>
</section>
<section id="temperature-controlled-sampling">
<h2><span class="section-number">19.2. </span>Temperature-controlled sampling<a class="headerlink" href="#temperature-controlled-sampling" title="Link to this heading">#</a></h2>
<section id="the-basics">
<h3><span class="section-number">19.2.1. </span>The basics<a class="headerlink" href="#the-basics" title="Link to this heading">#</a></h3>
<p>Alternatively, we can look into stochastic approaches to avoid the response being generic.
Instead of taking the tokens that deterministic maximizes the probability from the Softmax function, we can perform randomly sample the next token from the vocabulary according to probability of each token from the Softmax function.
The temperature-controlled sampling randomly samples from the model output’s probability distribution over the full vocabulary at each step:</p>
<div class="math notranslate nohighlight">
\[P(y_t=w_i | y_{&lt;t}) = exp(z_{t,i} / T) / sum_{j=1}^{|V|} exp(z_{t,j} / T)\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{t_i}\)</span> is the logit for token <span class="math notranslate nohighlight">\(i\)</span> at step <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(|V|\)</span> denotes the cardinality of the vocabulary. We can easily control the diversity of the output by adding a temperature parameter T that rescales the logits before taking the Softmax. There are two special cases:</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(T \to 0\)</span>, the temperature-controlled sampling approaches a greedy decoding.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(T \to \inf\)</span>, the temperature-controlled sampling approaches the uniformly sampling.</p></li>
</ul>
<p>Setting the right temperature is critically to produce rich yet meaningful sentences. For example, a very high temperature would produce has produced mostly gibberish, including the appearance of many rare or even made-up words and uncommon grammar patterns.</p>
</section>
<section id="top-k-and-top-p-sampling">
<h3><span class="section-number">19.2.2. </span>Top-k and top-p sampling<a class="headerlink" href="#top-k-and-top-p-sampling" title="Link to this heading">#</a></h3>
<p>Top-k and top-p sampling are two common extensions to the temperature-controlled sampling. The basic idea of both approaches is to impose additional constraints on the number of possible tokens we can sample from at each step.</p>
<p>The idea behind top-<span class="math notranslate nohighlight">\(k\)</span> sampling is used to ensure that the less probable words will not be considered at all and only top <span class="math notranslate nohighlight">\(k\)</span> probable tokens will be sampled. This imposes a fixed cut on the long tail of the word distribution and prevent the generation of rare words and going off-topic. One disadvantage of top-<span class="math notranslate nohighlight">\(k\)</span> sampling is that the number <span class="math notranslate nohighlight">\(K\)</span> need to be defined in the beginning. Selecting an universal <span class="math notranslate nohighlight">\(K\)</span> is non-trivial, as shown in the two following cases:</p>
<ul class="simple">
<li><p>When the next word has a broad distribution, having a small <span class="math notranslate nohighlight">\(K\)</span> will discard many reasonable options.</p></li>
<li><p>When the next word has a narrow distribution, having a large <span class="math notranslate nohighlight">\(K\)</span> will cause the generation of rare words.</p></li>
</ul>
<p>The top-<span class="math notranslate nohighlight">\(p\)</span> approach aims to adapt <span class="math notranslate nohighlight">\(k\)</span> heuristically based on the distribution shape. Let token <span class="math notranslate nohighlight">\(i=1,...,|V|\)</span> be sorted descendingly according to its probability <span class="math notranslate nohighlight">\(p_i\)</span>. The top-p sampling approach chooses a probability threshold <span class="math notranslate nohighlight">\(p_0\)</span> and sets k to be the lowest value such that <span class="math notranslate nohighlight">\(\sum_i (p_i) &gt; p_0\)</span>. If the next word distribution is narrow (i.e., the model is confident in its next-word prediction), then k will be lower and vice versa.</p>
</section>
</section>
<section id="multi-step-decoding">
<h2><span class="section-number">19.3. </span>Multi-step Decoding<a class="headerlink" href="#multi-step-decoding" title="Link to this heading">#</a></h2>
<section id="speculative-decoding">
<h3><span class="section-number">19.3.1. </span>Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="Link to this heading">#</a></h3>
<p><span id="id2">[<a class="reference internal" href="#id1571" title="Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.">CBI+23</a>, <a class="reference internal" href="#id1570" title="Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, 19274–19286. PMLR, 2023.">LKM23</a>, <a class="reference internal" href="#id1572" title="Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, and others. Specinfer: accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023.">MOZ+23</a>]</span></p>
</section>
<section id="multihead-decoding-medusa">
<h3><span class="section-number">19.3.2. </span>Multihead Decoding (Medusa)<a class="headerlink" href="#multihead-decoding-medusa" title="Link to this heading">#</a></h3>
<p><span id="id3">[<a class="reference internal" href="#id1569" title="Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.">CLG+24</a>]</span></p>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">19.4. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id4">
<div role="list" class="citation-list">
<div class="citation" id="id1569" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">CLG+24</a><span class="fn-bracket">]</span></span>
<p>Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: simple llm inference acceleration framework with multiple decoding heads. <em>arXiv preprint arXiv:2401.10774</em>, 2024.</p>
</div>
<div class="citation" id="id1571" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">CBI+23</a><span class="fn-bracket">]</span></span>
<p>Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. <em>arXiv preprint arXiv:2302.01318</em>, 2023.</p>
</div>
<div class="citation" id="id1570" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">LKM23</a><span class="fn-bracket">]</span></span>
<p>Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In <em>International Conference on Machine Learning</em>, 19274–19286. PMLR, 2023.</p>
</div>
<div class="citation" id="id1572" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">MOZ+23</a><span class="fn-bracket">]</span></span>
<p>Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, and others. Specinfer: accelerating generative large language model serving with tree-based speculative inference and verification. <em>arXiv preprint arXiv:2305.09781</em>, 2023.</p>
</div>
<div class="citation" id="id1498" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">VCS+16</a><span class="fn-bracket">]</span></span>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: decoding diverse solutions from neural sequence models. <em>arXiv preprint arXiv:1610.02424</em>, 2016.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_inference"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_LLM_case_study/deepseek_series.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">18. </span>DeepSeek Series (WIP)</p>
      </div>
    </a>
    <a class="right-next"
       href="inference_acceleration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">20. </span>Inference Acceleration (WIP)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoding-fundamentals">19.1. Decoding Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-decoding">19.1.1. Greedy decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search-decoding">19.1.2. Beam search decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">19.1.2.1. Basics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">19.1.2.2. Computational complexity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#controling-beam-search-behavior">19.1.2.3. Controling beam search behavior</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-controlled-sampling">19.2. Temperature-controlled sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-basics">19.2.1. The basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#top-k-and-top-p-sampling">19.2.2. Top-k and top-p sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-decoding">19.3. Multi-step Decoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding">19.3.1. Speculative Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multihead-decoding-medusa">19.3.2. Multihead Decoding (Medusa)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">19.4. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>