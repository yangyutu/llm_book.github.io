
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9. LLM Architectures Fundamentals &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_LLM_arch/LLM_dense_architectures';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="10. *Annotated LLama" href="../notebooks/chapter_LLM_arch/annotated_llama.html" />
    <link rel="prev" title="8. GPT Series" href="../chapter_foundation/GPT_series.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_arch/annotated_llama.html">10. *Annotated LLama</a></li>
<li class="toctree-l1"><a class="reference internal" href="LLM_moe_sparse_architectures.html">11. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">12. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">13. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">14. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">15. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">16. LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">17. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">18. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">19. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">20. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">21. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">22. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">23. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Architectures Fundamentals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">9.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">9.2. Layer Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-basics">9.2.1. Layer normalization basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-norm-root-mean-square-norm">9.2.2. RMS Norm (Root Mean Square Norm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-position">9.2.3. Layer normalization position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-example-choices">9.2.4. Layer normalization example choices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinearity-in-ffn">9.3. Nonlinearity in FFN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-variants">9.4. Self-attention Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-mha">9.4.1. Multi-Head Attention (MHA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-query-attention-mqa">9.4.2. Multi Query Attention (MQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention-gqa">9.4.3. Grouped Query Attention (GQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sliding-window-attention">9.4.4. Sliding Window Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-encoding-and-long-context">9.5. Position Encoding and Long Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-position-encoding">9.5.1. Absolute Position Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alibi">9.5.2. ALiBi</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotary-postion-embedding">9.5.3. Rotary Postion Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mechanism">9.5.3.1. The mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-rope">9.5.3.2. Properties of RoPE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-context-windows">9.6. Extending Context Windows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-interpolation-for-rope">9.6.1. Position Interpolation for RoPE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ntk-aware-rope">9.6.2. NTK-Aware RoPE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-chunk-attention">9.6.3. Dual Chunk Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenziation-vocabulary-and-weight-tying">9.7. Tokenziation, vocabulary, and weight tying</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">9.7.1. BPE Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-bpe-to-bbpe">9.7.2. From BPE to BBPE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-composition-in-transformer-models">9.8. Parameter composition in Transformer models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-computation-breadown">9.9. Forward Pass Computation Breadown</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-architecture-examples">9.10. Dense Architecture Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">9.11. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-architectures-fundamentals">
<span id="chapter-llm-arch-sec-llm-arch-fundamentals"></span><h1><span class="section-number">9. </span>LLM Architectures Fundamentals<a class="headerlink" href="#llm-architectures-fundamentals" title="Link to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">9.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>LLM have revolutionized natural language processing and artificial intelligence, demonstrating remarkable capabilities in understanding and generating human-like text. Tech companies like Google, Microsoft, Meta, OpenAI are producing LLMs with increasing sizes and capabilities just over the past few years [<a class="reference internal" href="#chapter-foundation-fig-pretrained-llm-timeline"><span class="std std-numref">Fig. 9.1</span></a>].</p>
<p>From a high level, LLMs share the following characteristics:</p>
<ul class="simple">
<li><p><strong>Scale</strong>: LLMs are trained on enormous datasets, often containing hundreds to thousands of billions of words or tokens. This massive scale allows them to capture intricate patterns and nuances in language. For example, GPT-3 <span id="id1">[<a class="reference internal" href="#id408" title="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.">BMR+20</a>]</span> was trained on about 500 billion tokens, and recent Llama3 405B <span id="id2">[<a class="reference internal" href="#id1542" title="Abhimanyu Dubey and et al Abhinav Jauhri. The llama 3 herd of models. 2024. URL: https://arxiv.org/abs/2407.21783, arXiv:2407.21783.">DAJ24</a>]</span> was trained on 15.6T tokens.</p></li>
<li><p><strong>Transformer architecture</strong>: Most modern LLMs use transformer architectures, which were introduced in the “Attention is All You Need” paper. These models can have billions of parameters - GPT-3 has 175 billion, for instance. The transformer architecture allows for efficient parallel processing and captures long-range dependencies in text.</p></li>
<li><p><strong>Self-supervised learning</strong>: LLMs are typically pre-trained using self-supervised learning techniques. The most common approach is next-token prediction, where the model learns to predict the next word in a sequence given the previous words. This allows the model to learn from vast amounts of unlabeled text data.</p></li>
<li><p><strong>Multi-task capability</strong>: A single LLM can perform various language tasks such as translation, summarization, question-answering, reasongin, and text generation without needing separate models for each task. This versatility makes them powerful tools for a wide range of applications.</p></li>
<li><p><strong>Few-shot learning via prompting</strong>: The multi-task ability of LLMs can often be simply invoked by prompting with a few examples provided in the prompt. This “in-context learning” allows them to adapt to new tasks without model weight update.</p></li>
<li><p><strong>Emergent reasoning abilities</strong>: As LLMs grow in size and complexity, they often develop capabilities that were hard to acquire among small models, for example, arithmetic reasoning and logical reasoning: Models may show ability to follow simple logical arguments or solve puzzles.</p></li>
<li><p><strong>Hallucination</strong>: LLMs can sometimes generate text that sounds plausible but is factually incorrect. This hallucination behavior is a significant challenge in deploying LLMs for applications requiring high reliability.</p></li>
</ul>
<figure class="align-default" id="chapter-foundation-fig-pretrained-llm-timeline">
<a class="reference internal image-reference" href="../../_images/large_langage_models_release_timeline.png"><img alt="../../_images/large_langage_models_release_timeline.png" src="../../_images/large_langage_models_release_timeline.png" style="width: 673.0px; height: 334.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.1 </span><span class="caption-text">A timeline of existing large language models (having a size larger than 10B) in recent years. We mark the open-source LLMs in yellow color. Image from <span id="id3">[<a class="reference internal" href="#id1518" title="Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.">ZZL+23</a>]</span>.</span><a class="headerlink" href="#chapter-foundation-fig-pretrained-llm-timeline" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This chapter aims to go over the core components and architectural considerations that form the foundation of modern LLMs.</p>
<p>We begin by examining <strong>layer normalization</strong>, a crucial technique that stabilizes the learning process and allows for training of very deep networks, enhancing the overall performance of LLMs.</p>
<p>Next, we review the <strong>activation</strong> functions commonly used in LLM age, discussing their properties and impact on model performance and training dynamics.</p>
<p>The <strong>self-attention mechanism</strong>, a cornerstone of transformer models, is explored in depth, along with its variants that have emerged to address specific challenges or improve efficiency in processing and understanding context.</p>
<p>Next we cover <strong>position encoding</strong> techniques that allow transformer models to understand the sequential nature of language, with a focus on methods for handling <strong>long context</strong> – a critical challenge in scaling LLMs to process extensive inputs.</p>
<p>Finally, we examining the intricate details of LLM structure and function. We break down the <strong>distribution of parameters</strong> across different model components, provide a detailed explanation of the <strong>forward pass computation</strong>, and present <strong>examples of dense transformer architectures</strong>.</p>
</section>
<section id="layer-normalization">
<h2><span class="section-number">9.2. </span>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<section id="layer-normalization-basics">
<h3><span class="section-number">9.2.1. </span>Layer normalization basics<a class="headerlink" href="#layer-normalization-basics" title="Link to this heading">#</a></h3>
<p>The LayerNorm was originally proposed to overcome the  in combating the internal covariate shift issue <span id="id4">[<a class="reference internal" href="#id1528" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. 2015. URL: https://arxiv.org/abs/1502.03167, arXiv:1502.03167.">IS15</a>]</span>, where a layer’s input distribution changes as previous layers are updated, causing the difficulty of traning deep models.</p>
<p>The key idea in LayerNorm is to normalize the input to the neural network layer via</p>
<ul class="simple">
<li><p>re-centering by subtracting the mean</p></li>
<li><p>re-scaling by dividing the standard deviation.</p></li>
</ul>
<p>The calculation formula for an input vector <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(H\)</span> feature dimension is given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-layer-nomalization-formula">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-chapter-llm-arch-layer-nomalization-formula" title="Link to this equation">#</a></span>\[
\operatorname{LayerNorm}(x) =\frac{x-\mu}{\sqrt{\sigma+\epsilon}} \cdot \gamma+\beta
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the mean across feature dimensions, i.e., <span class="math notranslate nohighlight">\(\mu = \frac{1}{H} \sum_{i=1}^H x_i \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation across feature dimensions, i.e.,
<span class="math notranslate nohighlight">\(\sigma =\sqrt{\frac{1}{H} \sum_{i=1}^H\left(x_i-\mu\right)^2+\epsilon}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a small number acting as regularizer for division.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable scaling and shifting parameters</p></li>
</ul>
</section>
<section id="rms-norm-root-mean-square-norm">
<h3><span class="section-number">9.2.2. </span>RMS Norm (Root Mean Square Norm)<a class="headerlink" href="#rms-norm-root-mean-square-norm" title="Link to this heading">#</a></h3>
<p>A common hypothesis on why layer normalization can help stalize training and boost model convergence is the capability in  handling re-centering and re-scaling of both inputs and weight matrix. RMSNorm <span id="id5">[<a class="reference internal" href="#id1527" title="Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: https://arxiv.org/abs/1910.07467, arXiv:1910.07467.">ZS19</a>]</span> is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-rms-nomalization-formula">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-chapter-llm-arch-rms-nomalization-formula" title="Link to this equation">#</a></span>\[
\operatorname{RMSNorm}(x)=\frac{x}{\sqrt{\frac{1}{H} \sum_{i=1}^H x_i^2}} \cdot \gamma
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost.</p>
<p>In the following, we summarize the differences between RMSNorm and LayerNorm</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Computational complexity</strong></span></p></div>
<p class="sd-card-text"><strong>LayerNorm</strong> involves both mean and variance calculation for each normalization layer, which brings sizable computational cost for high-dimensional inputs in LLM (e.g., GPT-3 <span class="math notranslate nohighlight">\(d_model = 12288\)</span>).
<strong>RMSNorm</strong>, on the other hand, only keeps the variance calculation, reducing the normalization cost by half and increses efficiency</p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Gradient propogation</strong></span></p></div>
<p class="sd-card-text"><strong>LayerNorm</strong> stablizes the input distribution between layers through normalization and benefits deep networks training by alleviating the problem of vanishing or exploding gradients. However, LayerNorm can also be affected by noise and input shifts when calculating the mean, potentially leading to unstable gradient propagation.
<strong>RMSNorm</strong>, by using only RMS for normalization, can provide a more robust, smoother gradient flow, especially in deeper networks. It reduces the impact of mean on gradient fluctuations, thereby improving the stability and speed of training.</p>
</div>
<div class="sd-card-footer docutils">
<p class="sd-card-text">See <span id="id6">[<a class="reference internal" href="#id1527" title="Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: https://arxiv.org/abs/1910.07467, arXiv:1910.07467.">ZS19</a>]</span> for math derivation</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="layer-normalization-position">
<h3><span class="section-number">9.2.3. </span>Layer normalization position<a class="headerlink" href="#layer-normalization-position" title="Link to this heading">#</a></h3>
<p>It has been shown that <span id="id7">[<a class="reference internal" href="#id410" title="Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, 10524–10533. PMLR, 2020.">XYH+20</a>]</span> where to replace the normalization layer has an impact on model training, covergence, and final performance.</p>
<p>The Post-Norm (as in the vanilla Transformer architecture) has can stablize the variance of the output by applying the LayerNorm after the residual connection, which is given by</p>
<div class="math notranslate nohighlight">
\[\operatorname{PostNorm Output} = \operatorname{LayerNorm}(X + \operatorname{SubLayer}(X))\]</div>
<p>Here the SubLayer could be the FeedForward Layer or the Attention Layer.</p>
<p>The Pre-Norm normalize the input to SubLayers, which is given by</p>
<div class="math notranslate nohighlight">
\[\operatorname{PreNorm Output} = X + \operatorname{SubLayer}(\operatorname{LayerNorm}(X)).\]</div>
<p>It is shown in <span id="id8">[<a class="reference internal" href="#id410" title="Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, 10524–10533. PMLR, 2020.">XYH+20</a>]</span> that the gradients at last layer <span class="math notranslate nohighlight">\(L\)</span> satisfy the following condition:</p>
<div class="math notranslate nohighlight">
\[||G_{PostNorm,L}||_F \leq \mathcal{O}(d \sqrt{\ln d}), ||G_{PreNorm,L}||_F \leq \mathcal{O}\left(d \sqrt{\frac{\ln d}{L}}\right)
.\]</div>
<p>which intuitively implies the following</p>
<ul class="simple">
<li><p>The gradient norm magnitude in the Pre-Norm Transformer will be likely to stay the same for any layer index <span class="math notranslate nohighlight">\(l\)</span></p></li>
<li><p>Gradient norm in the Post-LN Transformer will likely increase as layer index <span class="math notranslate nohighlight">\(l\)</span> and be very large at the last layer <span class="math notranslate nohighlight">\(L\)</span>.</p></li>
</ul>
<p>Such gradient norm behavior has implication on training stability.</p>
<ul class="simple">
<li><p>For Post-norm model, it often requires learning rate scheduling and warm up (initializing from a small vaue) to stablize training.</p></li>
<li><p>When it comes to training very deep models, Post-norm can lead to more unstable gradients during training, especially in very deep networks. This can lead to slower convergence and increased likelihood of training failure.</p></li>
<li><p>On the other hand, Pre-Norm Transformers without the warm-up stage can reach comparable results with Post-Norm, simplifying the hyper-parameter tuning;</p></li>
<li><p>Pre-Norm, thanks to its stable gradient, is suitable for LLM architecture, which are very deep transformers.</p></li>
</ul>
<!-- l decreases

The residual connection $x+F(x)$ in the Transformer layer will modify the variance of input $x$. To see this, let the variance of $x$ be $\sigma_1^2$ and the variance of $F(x)$ be $\sigma_2^2$. Then the variance of $x + F(x)$ will be given by 

$$ 
Var[x + F(x)] = \sigma_1^2 + \sigma_2^2 + \rho \sigma_1\sigma_2
$$


Clearly, the normalization will reduce the effect of identity mapping $I(x) = x$ and therefore the gradient flow via the residual connection (aka high-way connection). 
As a result, using Post-Norm in general will require carefully designed learning rate warm-up stage (the optimization starts with
a small/tiny learning rate, and then gradually increases it to a pre-defined maximum value within certain number of steps.) to speed up model training and covergence, including learning rate warm-up and other hyper-parameter tuning. -->
<figure class="align-default" id="chapter-llm-arch-fig-pretrained-lm-transformer-layernormalizationposition">
<a class="reference internal image-reference" href="../../_images/layer_normalization_position.png"><img alt="../../_images/layer_normalization_position.png" src="../../_images/layer_normalization_position.png" style="width: 471.59999999999997px; height: 354.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.2 </span><span class="caption-text">Post-layer normalization and pre-layer normalization in an encoder layer.</span><a class="headerlink" href="#chapter-llm-arch-fig-pretrained-lm-transformer-layernormalizationposition" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Pre-Norm</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">In the Pre-Norm architecture, the normalization operation (RMS Norm or Layer Norm) is performed before the self-attention or feed-forward neural network (FFN) calculations. In other words, the input to each layer is first normalized before being passed to the attention or feed-forward layers.</p></li>
<li><p class="sd-card-text">Pre-Norm ensures that the magnitude of inputs remains within a stable range in deep networks, which is particularly beneficial for models with long-range dependencies. By performing normalization operations early, the model can learn from more stable inputs, thus helping to address the problem unstable gradients in deep models.</p></li>
<li><p class="sd-card-text">LLMs like GPT, LLama are using Pre-Norm design</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Post-Norm</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">In the Post-Norm architecture, the normalization operation is performed after the self-attention or FFN calculations. The model first goes through unnormalized operations, and finally, the results are normalized to ensure balanced model outputs.</p></li>
<li><p class="sd-card-text">Post-Norm can achieve good convergence effects in the early stages of training, performing particularly well in shallow models. However, in deep networks, the drawback of Post-Norm is that it may lead to gradient instability during the training process, especially as the network depth increases, gradients may become increasingly unstable during propagation.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Another modification of Post-Norm to enable training of very deep Post-Norm Transformer model (up to 1000 layers) is <strong>Deep-Norm</strong> <span id="id9">[]</span>, which gives</p>
<div class="math notranslate nohighlight">
\[\operatorname{DeepNorm Output} = \operatorname{LayerNorm}(\alphaX + \operatorname{SubLayer}(X))\]</div>
<p>Here <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span> is a constant, which up scales the residual connection (to help gradient vanishing issue for deep models). Besides, the weights in the SubLayers are scaled by <span class="math notranslate nohighlight">\(\beta &lt; 1\)</span> (i.e., make it smaller) during initalization.</p>
</section>
<section id="layer-normalization-example-choices">
<h3><span class="section-number">9.2.4. </span>Layer normalization example choices<a class="headerlink" href="#layer-normalization-example-choices" title="Link to this heading">#</a></h3>
<p>The core advantages of RMS Pre-Norm lie in its computational simplicity and gradient stability, making it an effective normalization choice for deep neural networks, especially large language models. This is exampified by the fact that LLaMa series started to use Pre-RMSNorm whereas GPT-3 model used Pre-LayerNorm.</p>
<ul class="simple">
<li><p>Improved computational efficiency: As RMS Norm omits mean calculation, it reduces the computational load for each layer, which is particularly important in deep networks. Compared to traditional Layer Norm, RMS Norm can process high-dimensional inputs more efficiently.</p></li>
<li><p>Enhanced gradient stability: RMS Pre-Norm can reduce instances of vanishing gradients, especially in deep networks. This normalization method improves training efficiency by smoothing gradient flow.</p></li>
<li><p>Suitable for large-scale models: For models like LLaMA, RMS Pre-Norm supports maintaining a relatively small model size while ensuring powerful performance. This allows the model to maintain good generalization capabilities without increasing complexity.</p></li>
</ul>
</section>
</section>
<section id="nonlinearity-in-ffn">
<h2><span class="section-number">9.3. </span>Nonlinearity in FFN<a class="headerlink" href="#nonlinearity-in-ffn" title="Link to this heading">#</a></h2>
<p>As introduced in <a class="reference internal" href="../chapter_foundation/transformers.html#chapter-foundation-sec-pretrained-lm-transformer-arch-ffn"><span class="std std-ref">Pointwise FeedForward Layer</span></a>, FFN block plays a critical role in improving model capacity via nonlinear activations.</p>
<p>With <span class="math notranslate nohighlight">\(x\)</span> is the input vector, <span class="math notranslate nohighlight">\(W_1, b_1\)</span> and <span class="math notranslate nohighlight">\(W_2, b_2\)</span> are weight matrices and biases for the two layers, the FFN block is given by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{FFN}(x)=f(xW_1+b_1)W_2+b_2
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the activation function.</p>
<p>While in the original Transformer paper ReLU is used, many other different activations are explored. In the latest LLMs, GLU activations <span id="id10">[<a class="reference internal" href="#id1543" title="Noam Shazeer. Glu variants improve transformer. 2020. URL: https://arxiv.org/abs/2002.05202, arXiv:2002.05202.">Sha20</a>]</span> are widely adopted and its variations SwiGLU are also widely used to achieve better performance in practice.</p>
<p><strong>Gated Linear Units (GLU)</strong> is a neural network layer defined as the componentwise product of two linear transformations of the input, one of which is sigmoid-activated.</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-eq-glu">
<span class="eqno">(9.3)<a class="headerlink" href="#equation-chapter-llm-arch-eq-glu" title="Link to this equation">#</a></span>\[\operatorname{GLU}(x; W, V, b)=\sigma(xW+b) \otimes xV
\]</div>
<p>where <span class="math notranslate nohighlight">\(W, V\)</span> are weight matrices and <span class="math notranslate nohighlight">\(b\)</span> is the bias. Note that intuitively GLU introduces a gating mechanism on the product <span class="math notranslate nohighlight">\(xV\)</span> via the sigmoid function <span class="math notranslate nohighlight">\(\sigma(xW+b)\)</span>. Such gating mechanism allows the model to learn when to emphasize or de-emphasize certain features.</p>
<p>Apply GLU in the FFN block, we yield</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-eq-ffn-glu">
<span class="eqno">(9.4)<a class="headerlink" href="#equation-chapter-llm-arch-eq-ffn-glu" title="Link to this equation">#</a></span>\[
\operatorname{FFN}_{GLU}(x; W_1,W_2,V, b_1, b_2)=(\sigma(xW_1 + b) \otimes xV)W_2 + b_2
\]</div>
<p>where <span class="math notranslate nohighlight">\(W_1, W_2, V\)</span> are weight matrices. Note that the FFN layer with GLU have three weight matrices, as opposed to two for the original FFN.</p>
<p>One variant of GLU is Swish <span id="id11">[<a class="reference internal" href="#id1544" title="Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. 2017. URL: https://arxiv.org/abs/1710.05941, arXiv:1710.05941.">RZL17</a>]</span>, which is given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-eq-swish">
<span class="eqno">(9.5)<a class="headerlink" href="#equation-chapter-llm-arch-eq-swish" title="Link to this equation">#</a></span>\[
\operatorname{Swish}_\beta(x)=x \cdot \sigma(\beta x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a hyperparameter for Swish. Compared to GLU, Swish is a self-gated activation function. As showed in <a class="reference internal" href="#chapter-foundation-fig-pretrained-llm-activation-swish"><span class="std std-numref">Fig. 9.3</span></a>, Swish has the following appealing properites:</p>
<ul class="simple">
<li><p>Smooth derivative leading to better gradient flow, while ReLU is nonsmooth at <span class="math notranslate nohighlight">\(x=0\)</span></p></li>
<li><p>Non-monotonicity: The non-monotonic nature of Swish allows it to capture more complex relationships in the data</p></li>
<li><p>Unbounded above and bounded below, where as GLU is bounded above and below</p></li>
<li><p>Non-zero gradient for negative inputs: For very negative inputs, Swish has a small but non-zero gradient, unlike ReLU which has a zero gradient. This can help mitigate the “dying ReLU” problem.</p></li>
<li><p>Self-gating property allows the network to learn when to emphasize or de-emphasize certain features.</p></li>
</ul>
<figure class="align-default" id="chapter-foundation-fig-pretrained-llm-activation-swish">
<a class="reference internal image-reference" href="../../_images/swish.png"><img alt="../../_images/swish.png" src="../../_images/swish.png" style="width: 767.5px; height: 270.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.3 </span><span class="caption-text">(Left) The Swish activation function. (Right)  First derivatives of Swish. Image from <span id="id12">[<a class="reference internal" href="#id1544" title="Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. 2017. URL: https://arxiv.org/abs/1710.05941, arXiv:1710.05941.">RZL17</a>]</span>.</span><a class="headerlink" href="#chapter-foundation-fig-pretrained-llm-activation-swish" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>If we use Swish function in the GLU, we can obtain the following variations:</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-eq-ffn-swiglu">
<span class="eqno">(9.6)<a class="headerlink" href="#equation-chapter-llm-arch-eq-ffn-swiglu" title="Link to this equation">#</a></span>\[\begin{split}
SwiGLU=\text{Swish}_1(xW) \otimes xV \\
\operatorname{FFN}_{SwiGLU} = (\underbrace{\text{Swish}_1(xW_1)}_{\text{Gate Projection}}\otimes \underbrace{xV}_{\text{Up Projection}} ) \underbrace{W_2}_{\text{Down Projection}}
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\operatorname{Swish}_1(x)=x \cdot \sigma(x)\)</span>.</p>
<p>Example activation in recent LLMs:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>LLM</p></th>
<th class="head text-center"><p>Activation Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Mistral</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>LLaMA</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Qwen</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="self-attention-variants">
<h2><span class="section-number">9.4. </span>Self-attention Variants<a class="headerlink" href="#self-attention-variants" title="Link to this heading">#</a></h2>
<section id="multi-head-attention-mha">
<h3><span class="section-number">9.4.1. </span>Multi-Head Attention (MHA)<a class="headerlink" href="#multi-head-attention-mha" title="Link to this heading">#</a></h3>
<p>Multi-Head Attention [detailed in <a class="reference internal" href="../chapter_foundation/transformers.html#chapter-foundation-sec-pretrained-lm-transformer-arch-mha"><span class="std std-ref">Multihead Attention with Masks</span></a>] is the foundation of many transformer-based models, including the original transformer architecture.</p>
<p>The computation of an <span class="math notranslate nohighlight">\(H\)</span>-headed MHA given input <span class="math notranslate nohighlight">\(X\in \mathbb{R}^{n\times d_{model}}\)</span> matrix and <span class="math notranslate nohighlight">\(H\)</span> projection matrices <span class="math notranslate nohighlight">\(W^Q_i, W^K_i, W^V_i \in\mathbb{R}^{d_{model}}\times d_{head}\)</span>, <span class="math notranslate nohighlight">\(i\in \{1,...,H\}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_H)W^O
\]</div>
<p>where each head is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{head}_i = \text{Attention}(XW^Q_i, XW^K_i, XW^V_i)
\]</div>
<p>with the attention given by</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V.
\]</div>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-mha">
<a class="reference internal image-reference" href="../../_images/MHA.png"><img alt="../../_images/MHA.png" src="../../_images/MHA.png" style="width: 214.0px; height: 209.60000000000002px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.4 </span><span class="caption-text">Multi-head attention has <span class="math notranslate nohighlight">\(H\)</span> query, key, and value heads for each token.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-mha" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In the following, we summarize the advantages and drawbacks of MHA.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Advantages</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Improves the model’s overall learning capacity</p></li>
<li><p class="sd-card-text">Different heads allow the model to jointly attend to information from different representation subspaces</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Drawbacks</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Computational complexity scales quadratically with sequence length (i.e., huge cost for long context applications)</p></li>
<li><p class="sd-card-text">During inference stage, each head has its own key and value to cache, bring additional memory burden to inference process.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="multi-query-attention-mqa">
<h3><span class="section-number">9.4.2. </span>Multi Query Attention (MQA)<a class="headerlink" href="#multi-query-attention-mqa" title="Link to this heading">#</a></h3>
<p>To reduce the inference cost from MHA, <span id="id13">[<a class="reference internal" href="#id1538" title="Noam Shazeer. Fast transformer decoding: one write-head is all you need. 2019. URL: https://arxiv.org/abs/1911.02150, arXiv:1911.02150.">Sha19</a>]</span> proposed <strong>MQA</strong>, which reduces <span class="math notranslate nohighlight">\(H\)</span> key and value heads in MHA to a single key and value head.
During inference, MQA reducing the size of the key-value cache by a factor of <span class="math notranslate nohighlight">\(H\)</span> (see <a class="reference internal" href="../chapter_inference/inference_acceleration.html#chapter-inference-sec-inference-acceleration-kv-cache"><span class="std std-ref">KV Cache</span></a>). However, larger models generally scale the number of heads (e.g., GPT-2 has 12 heads; GPT-3 has 96 heads), such that multi-query attention represents a more
aggressive cut in both memory bandwidth and capacity footprint.</p>
<p>In MQA, the single head attnetion is computed as</p>
<div class="math notranslate nohighlight">
\[
\text{head}_i = \text{Attention}(XW^Q_i, XW^K, XW^V).
\]</div>
<p>Note that we only have one group of <span class="math notranslate nohighlight">\(W^K, W^V\)</span> matrices.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-mqa">
<a class="reference internal image-reference" href="../../_images/MHA.png"><img alt="../../_images/MHA.png" src="../../_images/MHA.png" style="width: 214.0px; height: 209.60000000000002px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.5 </span><span class="caption-text">Multi-head attention has <span class="math notranslate nohighlight">\(H\)</span> query, and one shared single key head and single value head for each token.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-mqa" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>MQA often comes at the cost of quality degradation. In the following, we summarize the MHA advantages and drawbacks.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Advantages</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">During inference stage, each head has its own key and value to cache, bring additional memory burden to inference process.</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Drawbacks</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Computational complexity scales quadratically with sequence length (i.e., huge cost for long context applications)</p></li>
<li><p class="sd-card-text">Modeling capacity is largely compromised due to the reduction of multiple heads to single head, leading to quality degradation.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="grouped-query-attention-gqa">
<span id="chapter-llm-arch-sec-self-attention-variant-gqa"></span><h3><span class="section-number">9.4.3. </span>Grouped Query Attention (GQA)<a class="headerlink" href="#grouped-query-attention-gqa" title="Link to this heading">#</a></h3>
<p>GQA <span id="id14">[<a class="reference internal" href="#id1537" title="Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: training generalized multi-query transformer models from multi-head checkpoints. 2023. URL: https://arxiv.org/abs/2305.13245, arXiv:2305.13245.">ALTdJ+23</a>]</span> is an optimization of MHA and MQA that reduces computational complexity while maintaining performance.</p>
<p>Unlike MQA, GQA uses an intermediate (more than one, less than number of query heads) number of key-value heads.
GQA is shown to achieve quality close to MHA with comparable speed to MQA</p>
<p>In GQA, the single head attnetion is computed as</p>
<div class="math notranslate nohighlight">
\[
\text{head}_i = \text{Attention}(QW^Q_i, KW^K_{g(i)}, VW^V_{g(i)})
\]</div>
<p>Here <span class="math notranslate nohighlight">\(g(i)\)</span> is a function that maps head index to group index (e.g., <span class="math notranslate nohighlight">\(g(1): \{1, 2\} \to \{1\}\)</span>) and we have <span class="math notranslate nohighlight">\(G\)</span> groups of <span class="math notranslate nohighlight">\(W^K,W^V\)</span> matrices.</p>
<p>Studies [<a class="reference internal" href="#chapter-llm-arch-fig-fundamentals-attention-gqa"><span class="std std-numref">Fig. 9.6</span></a>] show that GQA (with <span class="math notranslate nohighlight">\(G &lt;=  8\)</span>) can improve latency by reduces parameters and computation compared to MHA and at the same time maintain most of the performance of MHA.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-gqa">
<a class="reference internal image-reference" href="../../_images/GQA_performance.png"><img alt="../../_images/GQA_performance.png" src="../../_images/GQA_performance.png" style="width: 801.15px; height: 515.55px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.6 </span><span class="caption-text">(Top) GQA divides the key and value heads into multiple groups. Within each group, a single shared
key and value heads are attended to by query heads. GQA interpolats between MHA and MQA. (Bottom) GQA-8 performance and latency compared with MHA and MQA. Image from <span id="id15">[<a class="reference internal" href="#id1537" title="Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: training generalized multi-query transformer models from multi-head checkpoints. 2023. URL: https://arxiv.org/abs/2305.13245, arXiv:2305.13245.">ALTdJ+23</a>]</span></span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-gqa" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>GQA is widely adopted in the latest LLM. Following shows example configurations of Qwen2 and Mistral LLM series <span id="id16">[<a class="reference internal" href="#id1541" title="An Yang, Baosong Yang, Binyuan Hui, and et al. Bo Zheng. Qwen2 technical report. 2024. URL: https://arxiv.org/abs/2407.10671, arXiv:2407.10671.">YYHBZ24</a>, <a class="reference internal" href="#id1545" title="Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. 2023. URL: https://arxiv.org/abs/2310.06825, arXiv:2310.06825.">JSM+23</a>]</span>.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1549">
<caption><span class="caption-number">Table 9.1 </span><span class="caption-text">Model configuration of Qwen2 and Mistral, which uses GQA (# KV heads is number of groups )</span><a class="headerlink" href="#id1549" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Configuration</p></th>
<th class="head text-center"><p>Hidden Size</p></th>
<th class="head text-center"><p># Layers</p></th>
<th class="head text-center"><p># Query Heads</p></th>
<th class="head text-center"><p># KV Heads</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Qwen2 0.5B</p></td>
<td class="text-center"><p>896</p></td>
<td class="text-center"><p>24</p></td>
<td class="text-center"><p>14</p></td>
<td class="text-center"><p>2</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Qwen2 1.5B</p></td>
<td class="text-center"><p>1,536</p></td>
<td class="text-center"><p>28</p></td>
<td class="text-center"><p>12</p></td>
<td class="text-center"><p>2</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Qwen2 7B</p></td>
<td class="text-center"><p>3,584</p></td>
<td class="text-center"><p>28</p></td>
<td class="text-center"><p>28</p></td>
<td class="text-center"><p>4</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Qwen2 72B</p></td>
<td class="text-center"><p>8,192</p></td>
<td class="text-center"><p>80</p></td>
<td class="text-center"><p>64</p></td>
<td class="text-center"><p>8</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Mistral 7B</p></td>
<td class="text-center"><p>4096</p></td>
<td class="text-center"><p>32</p></td>
<td class="text-center"><p>32</p></td>
<td class="text-center"><p>8</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sliding-window-attention">
<h3><span class="section-number">9.4.4. </span>Sliding Window Attention<a class="headerlink" href="#sliding-window-attention" title="Link to this heading">#</a></h3>
<p>The computational complexity for MHA, MQA, GQA are scaling quadratically with the sequence length. This constrains the context length that LLM can effectively process, impacting their ability to handle long documents or maintain coherence over extended generations.</p>
<p>To address this challenge, recent LLMs (e.g., Mistral <span id="id17">[<a class="reference internal" href="#id1545" title="Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. 2023. URL: https://arxiv.org/abs/2310.06825, arXiv:2310.06825.">JSM+23</a>]</span>) adopts <strong>sliding window attention</strong>, which reduces the computational complexity by restricting each token’s attention to a fixed-size window <span class="math notranslate nohighlight">\(W\)</span> of preceding tokens, rather than attending to the entire sequence. The computational complexity is reduced from quadratic <span class="math notranslate nohighlight">\(O(s^2)\)</span> to linear <span class="math notranslate nohighlight">\(O(\min(W, s)\times s)\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is the sequence length. Although the token can only capture local context within its fixed window, with multiple layers stacked upon each other, a token at layer <span class="math notranslate nohighlight">\(L\)</span> can effectively attend to previous <span class="math notranslate nohighlight">\(L\times W\)</span> tokens.</p>
<p>In Mistral 7B with <span class="math notranslate nohighlight">\(L = 32\)</span>, and <span class="math notranslate nohighlight">\(W\)</span> set to 4096, the effective attention length is about <span class="math notranslate nohighlight">\(131K\)</span> tokens.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-sliding-window-attention">
<a class="reference internal image-reference" href="../../_images/sliding_window_attention.png"><img alt="../../_images/sliding_window_attention.png" src="../../_images/sliding_window_attention.png" style="width: 800.0999999999999px; height: 261.09999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.7 </span><span class="caption-text">Illustration of sliding window attention (Middle), which restrict each token to attend at most <span class="math notranslate nohighlight">\(W\)</span> preceding tokens. As a comparison, MHA (Left) attends to all the preceding tokens. While at each layer the information flow is limited by window size, after <span class="math notranslate nohighlight">\(L\)</span> layers, information can flow forward by up to <span class="math notranslate nohighlight">\(L\times W\)</span> tokens.  Image from <span id="id18">[<a class="reference internal" href="#id1545" title="Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. 2023. URL: https://arxiv.org/abs/2310.06825, arXiv:2310.06825.">JSM+23</a>]</span></span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-sliding-window-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- ### Sparse Attention

Sparse Attention introduces patterns of sparsity in the attention matrix.

Key idea:
- Predefined or learned patterns determine which tokens can attend to which other tokens

Advantages:
- Can capture both local and global context
- Reduces computational complexity

Examples:
- Longformer: combines sliding window attention with global attention
- Big Bird: uses random, window, and global attention patterns -->
</section>
</section>
<section id="position-encoding-and-long-context">
<h2><span class="section-number">9.5. </span>Position Encoding and Long Context<a class="headerlink" href="#position-encoding-and-long-context" title="Link to this heading">#</a></h2>
<section id="absolute-position-encoding">
<h3><span class="section-number">9.5.1. </span>Absolute Position Encoding<a class="headerlink" href="#absolute-position-encoding" title="Link to this heading">#</a></h3>
<p>In <a class="reference internal" href="../chapter_foundation/transformers.html#chapter-foundation-sec-pretrained-lm-transformer-arch-absolute-pe"><span class="std std-ref">Position Encodings</span></a>, we discuss <strong>absolute position encoding</strong>, which maps an integer <span class="math notranslate nohighlight">\(i\)</span> (used to represent the position of the token) to a <span class="math notranslate nohighlight">\(d_{model}\)</span> sinusoidal vector. Specifically, let <span class="math notranslate nohighlight">\(PE(i)_j\)</span> represent the <span class="math notranslate nohighlight">\(j\)</span>th dimention position encoding, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{PE}(i)_j = \left\{\begin{array}{l}
\sin \left(w_j i\right), \quad \text { if } j \text{ is even} \\
\cos \left(w_j i\right), \quad \text { if } j \text{ is odd}
\end{array}\right.
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_j=1/10000^{j / d_{model}}\)</span> if <span class="math notranslate nohighlight">\(j\)</span> is even and <span class="math notranslate nohighlight">\(w_j=1/10000^{j-1 / d_{model}}\)</span> if <span class="math notranslate nohighlight">\(j\)</span> is odd and <span class="math notranslate nohighlight">\(j=0,...,d_{model} - 1\)</span>.</p>
<p>While absolute position encoding has achieved success in BERT, it has several key issues when is applied in LLMs:</p>
<ul class="simple">
<li><p>Lack of extrapolation due to limited sequence length: Models are restricted to a maximum sequence length during training (e.g., BERT 512), limiting their ability to  generalize to positions beyond the maximum length at inference time.</p></li>
<li><p>Position insensitivity: The position encoding is added on top of token embedding and go through linear projection before interacting with other tokens, instead of directly interacting with other tokens during attention score computation.</p></li>
<li><p>Lack of invariance to shift: For two tokens with fixed relative position disance, their interaction at attention score computation layer is dependent on their absolute position. For relative position encodings, this property is by construction.</p></li>
</ul>
</section>
<section id="alibi">
<h3><span class="section-number">9.5.2. </span>ALiBi<a class="headerlink" href="#alibi" title="Link to this heading">#</a></h3>
<p><strong>ALiBi</strong> (Attention with Linear Biases) <span id="id19">[<a class="reference internal" href="#id1547" title="Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: attention with linear biases enables input length extrapolation. 2022. URL: https://arxiv.org/abs/2108.12409, arXiv:2108.12409.">PSL22</a>]</span> is a simple approach that suprisinly addresses all drawbacks in the sinusoidal abolute position encoding above. The key idea is to  <strong>simply add a static, relative position dependent bias into the Softmax computation step</strong> [<a class="reference internal" href="#chapter-llm-arch-fig-fundamentals-position-encoding-alibi"><span class="std std-numref">Fig. 9.8</span></a>]. Specifically, for the attention weight between query token <span class="math notranslate nohighlight">\(i\)</span> to all the key vectors, we have</p>
<div class="math notranslate nohighlight">
\[\operatorname{AttentionWeight} = \operatorname{Softmax}(\underbrace{Q_iK^T/d_{Head}}_{\text{scaled query-key doc product}} + \underbrace{m \cdot[−(i − 1), ..., −2, −1, 0]}_{\text{ALiBi bias vector}})
\]</div>
<p>where scalar <span class="math notranslate nohighlight">\(m\)</span> is a head-specific slope hyperparameter fixed before training (e.g., for a model with 8 heads, <span class="math notranslate nohighlight">\(1/2, 1/2^2,...,1/2^8\)</span>).</p>
<p>Note that AliBi has the following nice property by construction:</p>
<ul class="simple">
<li><p>It is a relative position encoding</p></li>
<li><p>Long distance decay, tokens with larger distance have smaller impact.</p></li>
</ul>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-position-encoding-alibi">
<a class="reference internal image-reference" href="../../_images/Alibi.png"><img alt="../../_images/Alibi.png" src="../../_images/Alibi.png" style="width: 492.8px; height: 204.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.8 </span><span class="caption-text">When computing attention weights for each head, ALiBi adds a constant bias (Right) to each attention score ($Q_iK^T), with scaling factor omitted (Left).  Image from <span id="id20">[<a class="reference internal" href="#id1547" title="Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: attention with linear biases enables input length extrapolation. 2022. URL: https://arxiv.org/abs/2108.12409, arXiv:2108.12409.">PSL22</a>]</span>.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-position-encoding-alibi" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Compare with sinusoidal absolute position encoding baseline [<a class="reference internal" href="#chapter-llm-arch-fig-fundamentals-position-encoding-alibi-comparison"><span class="std std-numref">Fig. 9.9</span></a>], there are several advantages of Alibi:</p>
<ul class="simple">
<li><p>When train and validate on the same input token length <span class="math notranslate nohighlight">\(L\)</span>, Alibi shows advantages over baseline.</p></li>
<li><p>When train on shorter length (e.g., 512), but validate on longer (e.g., 1024,…,3072), Alibi method extropolates well.</p></li>
</ul>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-position-encoding-alibi-comparison">
<a class="reference internal image-reference" href="../../_images/Alibi_vs_absolution_PE_performance.png"><img alt="../../_images/Alibi_vs_absolution_PE_performance.png" src="../../_images/Alibi_vs_absolution_PE_performance.png" style="width: 742.0px; height: 341.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.9 </span><span class="caption-text">Comparision between the ALiBi models trained and evaluated on varying sequence lengths on the WikiText-103
validation set and the sinusoidal absolute position encoding baseline. Image from <span id="id21">[<a class="reference internal" href="#id1547" title="Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: attention with linear biases enables input length extrapolation. 2022. URL: https://arxiv.org/abs/2108.12409, arXiv:2108.12409.">PSL22</a>]</span>.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-position-encoding-alibi-comparison" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="rotary-postion-embedding">
<h3><span class="section-number">9.5.3. </span>Rotary Postion Embedding<a class="headerlink" href="#rotary-postion-embedding" title="Link to this heading">#</a></h3>
<section id="the-mechanism">
<h4><span class="section-number">9.5.3.1. </span>The mechanism<a class="headerlink" href="#the-mechanism" title="Link to this heading">#</a></h4>
<p>Rotary Position Encoding (RoPE) <span id="id22">[<a class="reference internal" href="#id1548" title="Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. 2023. URL: https://arxiv.org/abs/2104.09864, arXiv:2104.09864.">SLP+23</a>]</span> is a widely adopted and proved-effective position encoding method in latest LLM (e.g., Llama, Qwen, etc.). RoPE has ideas similar to ALiBi and sinusoid position encoding:</p>
<ul class="simple">
<li><p>Like ALiBi, relative positional information is directly used in attention score computation.</p></li>
<li><p>Sinusoid functions are used in construction for their nice mathematical properties.</p></li>
</ul>
<p>Specifically, the key idea of RoPE is to multiply query vector <span class="math notranslate nohighlight">\(Q_m\)</span> (of a token at position <span class="math notranslate nohighlight">\(m\)</span>) and key vector <span class="math notranslate nohighlight">\(K_n\)</span> (of another token at position <span class="math notranslate nohighlight">\(n\)</span>) by a rotational matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}(m; \Theta)\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{R}(n; \Theta)\)</span> before taking the scaled doc product. Here rotational matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}(\cdot; \Theta)\)</span> is constructed a group of 2D rotational matrices, whose wave-length are specified by <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(d_{model}\times d_{model}\)</span> rotational matrix for position <span class="math notranslate nohighlight">\(m\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R}_{\Theta, m}^d=\left(\begin{array}{ccccccc}
\cos m \theta_1 &amp; -\sin m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin m \theta_1 &amp; \cos m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos m \theta_2 &amp; -\sin m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin m \theta_2 &amp; \cos m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m \theta_{d / 2} &amp; -\sin m \theta_{d / 2} \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m \theta_{d / 2} &amp; \cos m \theta_{d / 2}
\end{array}\right)
\end{split}\]</div>
<p>Here the rotary matrix has pre-defined parameters <span class="math notranslate nohighlight">\(\Theta=\left\{\theta_i=10000^{-2(i-1) / d}, i \in[1,2, \ldots, d_{model} / 2]\right\}\)</span>, which can be interpreted as wave length from <span class="math notranslate nohighlight">\(2\pi\)</span> (when <span class="math notranslate nohighlight">\(i = 1\)</span>) to <span class="math notranslate nohighlight">\(10000 \cdot 2\pi\)</span> (when <span class="math notranslate nohighlight">\(i = d_{model}/2\)</span>). <strong>Short wave length is used capture the high-frequency, short-ranged information in positions and long wave length is used to capture low-frequency, long-range information in position.</strong></p>
<p>Pre-SoftMax input (omitting scaling) for query token at position <span class="math notranslate nohighlight">\(m\)</span> and key token at position <span class="math notranslate nohighlight">\(n\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{PreSoftmax}(Q_m, K_n) =\left(\boldsymbol{R}_{\Theta, m} Q_m\right) \cdot \left(\boldsymbol{R}{\Theta,n} K_n \right)
\]</div>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 9.1 </span></p>
<section class="example-content" id="proof-content">
<p>For <span class="math notranslate nohighlight">\(d_{model} == 2\)</span>, the rotation matrix for position <span class="math notranslate nohighlight">\(m \)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R}(m; \Theta)=\left[\begin{array}{cc}
\cos (m\theta_1) &amp; -\sin (m\theta_1) \\
\sin (m\theta_1) &amp; \cos (m\theta_1)
\end{array}\right]
\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\theta_1 = 1\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(d_{model} == 4\)</span>, the rotation matrix for position <span class="math notranslate nohighlight">\(m \)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R}(m; \Theta)=\left[\begin{array}{cc}
\cos (m\theta_1) &amp; -\sin (m\theta_1) &amp; 0 &amp; 0 \\
\sin (m\theta_1) &amp; \cos (m\theta_1) &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos (m\theta_2) &amp; -\sin (m\theta_2) \\
0 &amp; 0 &amp; \sin (m\theta_2) &amp; \cos (m\theta_2)  
\end{array}\right]
\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\theta_1 = 1, \theta_2 = 10000^{-2/4}\)</span>.</p>
</section>
</div></section>
<section id="properties-of-rope">
<h4><span class="section-number">9.5.3.2. </span>Properties of RoPE<a class="headerlink" href="#properties-of-rope" title="Link to this heading">#</a></h4>
<p><strong>Relative position encoding</strong>:
Now we are showing that the rotated query-key inner product is a function of the relative position in 2D cases (the conclusion can be generalized to high-dimensional rotational matrix). Specifically, let <span class="math notranslate nohighlight">\(\theta_q = m\theta\)</span> and <span class="math notranslate nohighlight">\(\theta_k = n\theta\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span> are integer positions of query vector token and key vector token.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\left\langle R\left(\theta_q\right) Q_m, R\left(\theta_k\right) K_n\right\rangle &amp; =Q_m^{\top} R\left(\theta_q\right)^{\top} R\left(\theta_k\right) K_n \\
&amp; =Q_m^{\top} R\left(\theta_k-\theta_q\right) K_n \\
&amp; =\left\langle R\left(\theta_q-\theta_k\right) Q_m, K_n\right\rangle \\
&amp; =\left\langle R\left(\theta (m-n)\right) Q_m, K_n\right\rangle 
\end{aligned}
\end{split}\]</div>
<p>That is, the Pre-Softmax input of <span class="math notranslate nohighlight">\(Q_m, K_n\)</span> is a funciton of <span class="math notranslate nohighlight">\(m - n\)</span>.</p>
<p>We have used the following important properties of rotational matrix:</p>
<ol class="arabic simple">
<li><p>The transpose of a rotation matrix is equal to its inverse: <span class="math notranslate nohighlight">\(R(\theta)^{\top}=R(-\theta)\)</span>.</p></li>
<li><p>The matrix multiplication of rotational matrices satisfies: <span class="math notranslate nohighlight">\(R(\theta_x)\cdot R(\theta_y) = R(\theta_x + \theta_y)\)</span></p></li>
</ol>
<p>In other words, the inner product of two rotated vectors is equal to the inner product of one vector rotated by their angle difference and the other original vector.</p>
<p><strong>Long-term decay</strong>: In <span id="id23">[<a class="reference internal" href="#id1548" title="Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. 2023. URL: https://arxiv.org/abs/2104.09864, arXiv:2104.09864.">SLP+23</a>]</span>, it is shown that the inner-product will decay when the relative position increase. This property aligns with desired property that a pair of tokens will have gradually descreasing semantic impact on each other when they are far apart.</p>
<!-- ### Understanding RoPE with Visualization

```{figure} ../img/chapter_LLM_arch/position_encoding/RoPE/RoPE_visualization.png
---
scale: 70%
name: chapter_LLM_arch_fig_fundamentals_position_encoding_Alibi_comparison
---
Visualization of 2D RoPE and its mechanism in encoding context. Image from [Blog](https://mp.weixin.qq.com/s/dn8Pb80iRF9UkRn4vPOhHA).
``` -->
</section>
</section>
</section>
<section id="extending-context-windows">
<h2><span class="section-number">9.6. </span>Extending Context Windows<a class="headerlink" href="#extending-context-windows" title="Link to this heading">#</a></h2>
<section id="position-interpolation-for-rope">
<h3><span class="section-number">9.6.1. </span>Position Interpolation for RoPE<a class="headerlink" href="#position-interpolation-for-rope" title="Link to this heading">#</a></h3>
<p><strong>Position Interpolation</strong><span id="id24">[]</span> is a cheap method to extend the context window of an existing LLM, which allows LLM to have longer context window during inference time than the context window size during training.</p>
<p>The idea is to linearly down-scales the input position indices to match the original context window size [<a class="reference internal" href="#chapter-llm-arch-fig-fundamentals-rope-position-interpolation"><span class="std std-numref">Fig. 9.10</span></a>].Specifically, given that the rotation matrix in RoPE is a continuous function, one can adjust the rotation matrix for large position <span class="math notranslate nohighlight">\(m\)</span> to <span class="math notranslate nohighlight">\(m'\)</span> as</p>
<div class="math notranslate nohighlight">
\[R_{\Theta, m} = R_{\Theta, m'}, m' = \frac{L}{L'}\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> maximum length of context window during the training and <span class="math notranslate nohighlight">\(L' &gt; L\)</span> is larger context window we would like to apply during the inference stage.
Intuitively, we reduce position indices from <span class="math notranslate nohighlight">\(\left[0, L^{\prime}\right)\)</span> to <span class="math notranslate nohighlight">\([0, L)\)</span> to match the original range of indices before computing RoPE.</p>
<p>It is found that Position Interpolation is highly effective and efficient, requiring only a
very short period of fine-tuning for the model to fully adapt to greatly extended context windows. For example, extending the initial context window of 2048 to 32768 only requires fine-tuning for 1000 steps on the Pile.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-rope-position-interpolation">
<a class="reference internal image-reference" href="../../_images/Rope_position_interpolation.png"><img alt="../../_images/Rope_position_interpolation.png" src="../../_images/Rope_position_interpolation.png" style="width: 906.4999999999999px; height: 457.09999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.10 </span><span class="caption-text">An illustration of the Position Interpolation method, which is used to extend an initial context window from 2048 to 4096. Image from <span id="id25">[]</span>.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-rope-position-interpolation" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="ntk-aware-rope">
<h3><span class="section-number">9.6.2. </span>NTK-Aware RoPE<a class="headerlink" href="#ntk-aware-rope" title="Link to this heading">#</a></h3>
<p>From the information encoding (i.e., Neural Tangent Kernel - NTK theory) perspective, the scaling by Position Interpolation uniformly scales wave length by a factor of <span class="math notranslate nohighlight">\(L'/L\)</span>, which can hurt the model’s ability in capture high-frequency, short-ranged position information after rescaling.</p>
<p>The <strong>NTK-aware interpolation</strong> was proposed in public as a <a class="reference external" href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">reddit post</a>. Instead of scaling the wave length of every dimension uniformly, we scale up short wavelength less and long wavelength more.</p>
<p>More precisely, let the scaling factor be <span class="math notranslate nohighlight">\(s=L^{\prime} / L\)</span> and <span class="math notranslate nohighlight">\(b\)</span> be the original base. We perform a base change as follows:</p>
<div class="math notranslate nohighlight">
\[
b^{\prime}=b \cdot s^{\frac{|d_{model}|}{|d_{model}|-2}} .
\]</div>
<p>Note that the wave length at dimention <span class="math notranslate nohighlight">\(i\)</span> is given <span class="math notranslate nohighlight">\(\lambda = 2\pi b^{2i/d_{model}}\)</span>. To see the <strong>scaling-up effect is larger on dimensions with large wavelength</strong> (i.e., large <span class="math notranslate nohighlight">\(i\)</span>), we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log \frac{\lambda'}{\lambda} &amp;= 2 i d_{model} (\log b' - \log b) \\
                              &amp;= 2 i d_{model} (\frac{|d_{model}|}{|d_{model}|-2} \log s)
\end{align*}
\end{split}\]</div>
<p>which is a monontically increasing function on <span class="math notranslate nohighlight">\(i\)</span> given that <span class="math notranslate nohighlight">\(d_{model}\)</span> and <span class="math notranslate nohighlight">\(s\)</span> are constants.</p>
</section>
<section id="dual-chunk-attention">
<h3><span class="section-number">9.6.3. </span>Dual Chunk Attention<a class="headerlink" href="#dual-chunk-attention" title="Link to this heading">#</a></h3>
<p><strong>Dual Chunk Attention</strong> <span id="id26">[]</span> applies the chunking idea to map the position distance <span class="math notranslate nohighlight">\((i - j)\)</span> between a query state at position <span class="math notranslate nohighlight">\(i\)</span> and and a key state at position <span class="math notranslate nohighlight">\(j\)</span> to a value within the <strong>training stage context window size</strong> <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>More specifically, let the <span class="math notranslate nohighlight">\(M(i, j)\)</span> be the mapping function of dual chunk attention. <span class="math notranslate nohighlight">\(M\)</span> has hyperparameter of <strong>chunk size</strong> <span class="math notranslate nohighlight">\(s &lt; L\)</span> and <strong>local context window size</strong> <span class="math notranslate nohighlight">\(w = L - s\)</span>, which is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
M(i, j)= \begin{cases}P_{\mathbf{q}}^{\text {Intra }}[i]-P_{\mathbf{k}}[j] &amp; \text { if }\lfloor i / s\rfloor-\lfloor j / s\rfloor=0 \\ P_{\mathbf{q}}^{\text {Succ }}[i]-P_{\mathbf{k}}[j] &amp; \text { if }\lfloor i / s\rfloor-\lfloor j / s\rfloor=1 \\ P_{\mathbf{q}}^{\text {Inter }}[i]-P_{\mathbf{k}}[j] &amp; \text { if }\lfloor i / s\rfloor-\lfloor j / s\rfloor&gt;1 .\end{cases}
\end{split}\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P_{\mathbf{q}}^{\text {Intra }}[i] = P_{\mathbf{k}}[i] = i \bmod s\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{\mathbf{q}}^{\text {Inter }}[i] = L - 1 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{\mathbf{q}}^{\text {Succ }} = (s, s+1,...,s + w - 1, L-1,...,L-1) \)</span></p></li>
</ul>
<p>Intutiviely,</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are within the same chunk, i.e., <span class="math notranslate nohighlight">\(|i - j| &lt;= s\)</span>, <span class="math notranslate nohighlight">\(M(i, j) = i - j\)</span>, which recovers the original position distance.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are have distances of at least two chunks, <span class="math notranslate nohighlight">\(i\)</span> is capped at the value of <span class="math notranslate nohighlight">\(L - 1\)</span> and <span class="math notranslate nohighlight">\(j = j \bmod s\)</span>.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are within two consecutive chunks separately, a smoothed mapping is used to preverse locality.</p></li>
</ul>
<p>To summarize, DCA consists of three components: (1) intra-chunk attention, which recover the same attention when two positions are within the same chunk; (2) inter-chunk attention for tokens between distinct chunks; and (3) successive chunk attention for processing tokens residing in two consecutive distinct chunks.</p>
</section>
</section>
<section id="tokenziation-vocabulary-and-weight-tying">
<h2><span class="section-number">9.7. </span>Tokenziation, vocabulary, and weight tying<a class="headerlink" href="#tokenziation-vocabulary-and-weight-tying" title="Link to this heading">#</a></h2>
<section id="bpe-tokenization">
<h3><span class="section-number">9.7.1. </span>BPE Tokenization<a class="headerlink" href="#bpe-tokenization" title="Link to this heading">#</a></h3>
<p>Byte Pair Encoding (BPE) is a commonly used subword tokenization algorithm in NLP. It starts with individual characters and iteratively merges the most frequent pairs to create new subword units, repeating this process N times to build the final subword vocabulary. The following is a summary of the algorithm.</p>
<div class="proof algorithm admonition" id="BPE-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (BPE)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Word list <span class="math notranslate nohighlight">\(W\)</span>, Number of desired merges <span class="math notranslate nohighlight">\(N\)</span></p>
<p><strong>Output</strong> Subword vocabulary <span class="math notranslate nohighlight">\(V = \emptyset\)</span></p>
<ol class="arabic simple">
<li><p>Represent each word as a sequence of characters</p></li>
<li><p>Initialize the subword vocabulary as a set of single characters</p></li>
<li><p>For i in 1 to N:
3.1 Calculate the frequency of each consecutive character pair
3.2 Find the character pair <span class="math notranslate nohighlight">\((x, y)\)</span> with the highest frequency
3.3 Merge the character pair <span class="math notranslate nohighlight">\(c = (x, y)\)</span>, update the subword vocabulary <span class="math notranslate nohighlight">\(V = V \cup c\)</span>.</p></li>
<li><p>Return the subword vocabulary <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
</ol>
</section>
</div><div class="proof example admonition" id="example-2">
<p class="admonition-title"><span class="caption-number">Example 9.2 </span></p>
<section class="example-content" id="proof-content">
<p>GPT-2’s vocabulary size is 50,257, corresponding to 256 basic byte tokens, a special end-of-text token, and 50,000 tokens obtained through merging process.</p>
</section>
</div></section>
<section id="from-bpe-to-bbpe">
<h3><span class="section-number">9.7.2. </span>From BPE to BBPE<a class="headerlink" href="#from-bpe-to-bbpe" title="Link to this heading">#</a></h3>
<p>BPE (Byte Pair Encoding) and BBPE (Byte-level BPE) are both subword tokenization following the same idea of merging algorithm <a class="reference internal" href="#BPE-algorithm">Algorithm 9.1</a> but operating on different granularities.</p>
<p>In short, BPE Works on character or unicode level whereas BBPE works on byte level of UTF-8 representation. Their comparison is summarized as the following.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<span style="background-color: #e4ac94"><strong>BPE</strong></span></div>
<p class="sd-card-text">In BPE, the generation of subwords is more consistent with linguistic rules (e.g., utilizing word roots). The subword choices often better align with common vocabulary.</p>
<p class="sd-card-text">However, it often requires different treatments for different languages (like English vs Chinese) and it cannot effectively represent emojis and unseen special tokens.</p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<span style="background-color: #b4c9da"><strong>BBPE</strong></span></div>
<p class="sd-card-text">BBPE has following advantages:</p>
<ul class="simple">
<li><p class="sd-card-text">It can process all character sets (including Unicode characters), making it suitable for multilingual scenarios.</p></li>
<li><p class="sd-card-text">It provides good support for unconventional symbols and emojis.</p></li>
</ul>
<p class="sd-card-text">However, as BBPE is working on smaller granularity level than characters, it might result in larger vocabulary size (i.e., larger embedding layers) and unnatural subword units.</p>
</div>
</div>
</div>
</div>
</div>
<p>Currently, many mainstream large language models (such as the GPT series, Mistral<a class="footnote-reference brackets" href="#footnote1" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, etc.) primarily use BBPE instead of BPE. The reasons for the widespread adoption of this method include:</p>
<ul class="simple">
<li><p>Ability to process multilingual text: Large models typically need to handle vast amounts of text in different languages. BBPE operates at the byte level, allowing it to process all Unicode characters, performing particularly well for languages with complex character sets (such as Chinese, Korean, Arabic).</p></li>
<li><p>Unified tokenization: The BBPE method does not rely on language-specific character structures. Therefore, it can be uniformly applied to multilingual tasks without adding extra complexity, simplifying the tokenization process in both pre-training and downstream tasks.</p></li>
<li><p>Compatibility with emojis and special characters: Modern large language models need to process large amounts of internet data, which contains many emojis, special characters, and non-standard symbols. BBPE can better support these types of symbols.</p></li>
</ul>
<div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 9.1 </span> (What does Byte-level mean?)</p>
<section class="remark-content" id="proof-content">
<p>“Byte-level” in the context of BBPE means that the algorithm operates on individual bytes of data rather than on characters or higher-level text units. Note that characters are typically encoded using schemes like UTF-8, where a single character might be represented by one or more bytes. In other words, BBPE treats the input as a sequence of raw bytes, without interpreting them as characters or considering character boundaries.</p>
<p>Below is more context about UTF-8 encoding.</p>
<ol class="arabic simple">
<li><p>ASCII encoding:</p>
<ul class="simple">
<li><p>In the original ASCII encoding, each character is represented by a single byte (8 bits).</p></li>
<li><p>This allows for 256 different characters (2^8 = 256).</p></li>
<li><p>ASCII mainly covers English letters, numbers, and some basic symbols.</p></li>
</ul>
</li>
<li><p>Unicode and UTF-8:</p>
<ul class="simple">
<li><p>Unicode was developed to represent characters from all writing systems in the world.</p></li>
<li><p>UTF-8 is a variable-width encoding scheme for Unicode.</p></li>
<li><p>In UTF-8, characters can be encoded using 1 to 4 bytes:</p>
<ul>
<li><p>ASCII characters still use 1 byte</p></li>
<li><p>Many other characters use 2 or 3 bytes</p></li>
<li><p>Some very rare characters use 4 bytes</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Examples:</p>
<ul class="simple">
<li><p>The letter ‘A’ (U+0041 in Unicode) is represented as a single byte: 01000001</p></li>
<li><p>The Euro symbol ‘€’ (U+20AC) is represented by three bytes: 11100010 10000010 10101100</p></li>
<li><p>The emoji ‘😊’ (U+1F60A) is represented by four bytes: 11110000 10011111 10011000 10001010</p></li>
</ul>
</li>
</ol>
<p>This multi-byte representation for single characters is why text processing algorithms that work at the character level can be more complex than those that work at the byte level, especially when dealing with multilingual text.</p>
</section>
</div></section>
</section>
<section id="parameter-composition-in-transformer-models">
<span id="chapter-llm-arch-sec-parameter-composition"></span><h2><span class="section-number">9.8. </span>Parameter composition in Transformer models<a class="headerlink" href="#parameter-composition-in-transformer-models" title="Link to this heading">#</a></h2>
<p>In this section, we do an accounting exercise by estimating the number of parameters in a Transformer model. This will give us some insight on</p>
<ul class="simple">
<li><p>which component makes up the majority of parameters and</p></li>
<li><p>how the total number of parameters scales when we scale up different components.</p></li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(V\)</span> be the vocabulary size, <span class="math notranslate nohighlight">\(d\)</span> be the model hidden dimensions, <span class="math notranslate nohighlight">\(L\)</span> be the number of layer,</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1550">
<caption><span class="caption-number">Table 9.2 </span><span class="caption-text">Parameters in a Transformer</span><a class="headerlink" href="#id1550" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Module</p></th>
<th class="head text-left"><p>Computation</p></th>
<th class="head text-left"><p>Parameter Name</p></th>
<th class="head text-left"><p>Shape</p></th>
<th class="head text-left"><p>Parameter Number</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Attention</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\({Q} / {K} / {V}\)</span> projection</p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, {d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(3 d^2+3 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Attention output projection</p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, {d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(d^2+d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Layernorm</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\gamma, \beta\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>FFN</p></td>
<td class="text-left"><p>First layer up-projection</p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, 4 {~d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 d^2+d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Second layer down-projection</p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([4 {~d}, {~d}] /[4 {~d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 d^2+4 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Layernorm</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\gamma, \beta\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Embedding (tied)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{V}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Total</strong></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V d+L\left(12 d^2+13 d\right)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>The <strong>key scaling properties</strong> from this table are:</p>
<ul class="simple">
<li><p>The total number of parameters scales linearly with number of layers <span class="math notranslate nohighlight">\(L\)</span></p></li>
<li><p>The total number of parameters scales quadratically with model hidden dimensionality <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ul>
<div class="proof remark admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 9.2 </span></p>
<section class="remark-content" id="proof-content">
<p>We have simplification in the above computation for MHA but the results are the same. Suppose we have <span class="math notranslate nohighlight">\(H\)</span> heads, head dimension <span class="math notranslate nohighlight">\(d_{head}\)</span> and <span class="math notranslate nohighlight">\(H \times d_{head} = d\)</span>. QKV transformation matrices have weight parameters <span class="math notranslate nohighlight">\(3 \times H \times d \times d_{head} = 3d^2\)</span>.</p>
<p>With GQA that has <span class="math notranslate nohighlight">\(G\)</span> key-value shared heads, the total parameters are <span class="math notranslate nohighlight">\(d^2 + 2Gd_{head}d\)</span>.</p>
</section>
</div><div class="proof example admonition" id="example-5">
<p class="admonition-title"><span class="caption-number">Example 9.3 </span></p>
<section class="example-content" id="proof-content">
<p>Take the following GPT-3 13B and 175B as an example, 175B model has approximate 2.4 times of <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(d_{model}\)</span>. Extrapolating from 13B model, we estimate the 175B model to have model parameters of <span class="math notranslate nohighlight">\(13\times 2.4^3 = 179B\)</span>, which is very close.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(n_{\text{params}}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(H\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d_{head}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GPT-3 13B</p></td>
<td><p>13.0B</p></td>
<td><p>40</p></td>
<td><p>5140</p></td>
<td><p>40</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 175B or “GPT-3”</p></td>
<td><p>175.0B</p></td>
<td><p>96</p></td>
<td><p>12288</p></td>
<td><p>96</p></td>
<td><p>128</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</div></section>
<section id="forward-pass-computation-breadown">
<span id="chapter-llm-arch-sec-llm-arch-fundamentals-forward-pass-computation"></span><h2><span class="section-number">9.9. </span>Forward Pass Computation Breadown<a class="headerlink" href="#forward-pass-computation-breadown" title="Link to this heading">#</a></h2>
<p>In this section, we estimate the computational cost (in term of FLOPS) for a forward pass.</p>
<div class="proof remark admonition" id="remark-6">
<p class="admonition-title"><span class="caption-number">Remark 9.3 </span> (FLOPs estimation)</p>
<section class="remark-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(A \in R^{m \times k}, B \in R^{k \times n}\)</span> then, to compute <span class="math notranslate nohighlight">\(A B\)</span> the number of floating-point arithmetic required is <span class="math notranslate nohighlight">\(2 m n k\)</span>.</p>
<p>For example, for</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22}\end{bmatrix}, B = \begin{bmatrix}b_{11} &amp; b_{12} \\ b_{21} &amp; b_{22}\end{bmatrix},\end{split}\]</div>
<p>The resulting <span class="math notranslate nohighlight">\(C = AB\)</span> has <span class="math notranslate nohighlight">\(k\)</span> terms, which are given by</p>
<div class="math notranslate nohighlight">
\[c_{ij} = \sum_{t=1}^k a_{ik}b_{kj}.\]</div>
<p>It is clear that for each <span class="math notranslate nohighlight">\(c_{ij}\)</span> there are <span class="math notranslate nohighlight">\(k\)</span> multiplications and <span class="math notranslate nohighlight">\(k\)</span> additions (technically <span class="math notranslate nohighlight">\(k-1\)</span> additions among <span class="math notranslate nohighlight">\(k\)</span> terms).</p>
</section>
</div><p>Let <span class="math notranslate nohighlight">\(V\)</span> be the vocabulary size, <span class="math notranslate nohighlight">\(b\)</span> be the batch size, <span class="math notranslate nohighlight">\(s\)</span> be sequence length, <span class="math notranslate nohighlight">\(d\)</span> be the model hidden dimensions, <span class="math notranslate nohighlight">\(L\)</span> be the number of layer, we have summarized the computation breakdown in the following.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1551">
<caption><span class="caption-number">Table 9.3 </span><span class="caption-text">Computation breakdown</span><a class="headerlink" href="#id1551" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Module</p></th>
<th class="head text-left"><p>Computation</p></th>
<th class="head text-left"><p>Matrix Shape Changes</p></th>
<th class="head text-left"><p>FLOPs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Attention</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\({Q} / {K} / {V}\)</span> Projection</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, {d}] \times [{~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(3\times 2 b s d^2\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(Q K^T\)</span> dot product</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~d}] \times [{~b}, {~d}, {~s}]\to[{b}, {s}, {s}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s^2 d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Score Matrix <span class="math notranslate nohighlight">\( \dot V\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~s}] \times [{~b}, {~s}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s^2 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Output (with <span class="math notranslate nohighlight">\(W_o\)</span>)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, {d}] \times[{~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s d^2\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>FFN</p></td>
<td class="text-left"><p>First layer up-projection</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~d}] \times[{~d}, 4 {~d}] \to [{b}, {s}, 4 {~d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(8 b s d^2\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Second layer down-projection</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, 4 {~d}] \times[4 {~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(8 b s d^2\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Embedding</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, 1] \times[{~V}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s d V\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>In total</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\left(24 b s d^2+4 b d s^2\right) \times L+2 b s d V\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>The <strong>key scaling properties</strong> from this table are:</p>
<ul class="simple">
<li><p>The total compute scales linearly with number of layers <span class="math notranslate nohighlight">\(L\)</span>, and number of batch size <span class="math notranslate nohighlight">\(b\)</span></p></li>
<li><p>The total compute scales quadratically with model hidden dimensionality <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ul>
</section>
<section id="dense-architecture-examples">
<h2><span class="section-number">9.10. </span>Dense Architecture Examples<a class="headerlink" href="#dense-architecture-examples" title="Link to this heading">#</a></h2>
<!-- 
```{table} Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and MCL denotes the maximum context length during training.
| Model | Size | Normalization | PE | Activation | Bias | #L | #H | $d_{\text {model }}$ | MCL |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| GPT3 [55] | 175B | Pre LayerNorm | Learned | GeLU | $\checkmark$ | 96 | 96 | 12288 | 2048 |
| Llama | 207B | Pre RMSNorm | Learned | SwiGLU | $\checkmark$ | 64 | 128 | 16384 | 1024 |
| Qwen 2 {cite:p}`yang2024qwen2technicalreport`| 72B | Pre RMSNorm | RoPe | SwiGLU | $\checkmark$ | 80 | 64 | 8192 | ... |
```

% from A Survey of Large Language Models  -->
</section>
<section id="bibliography">
<h2><span class="section-number">9.11. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<p>Good reviews <span id="id28">[<a class="reference internal" href="#id1532" title="Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. 2024. URL: https://arxiv.org/abs/2303.18223, arXiv:2303.18223.">ZZL+24</a>]</span></p>
<div class="docutils container" id="id29">
<div role="list" class="citation-list">
<div class="citation" id="id1537" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ALTdJ+23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id14">1</a>,<a role="doc-backlink" href="#id15">2</a>)</span>
<p>Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: training generalized multi-query transformer models from multi-head checkpoints. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a>, <a class="reference external" href="https://arxiv.org/abs/2305.13245">arXiv:2305.13245</a>.</p>
</div>
<div class="citation" id="id408" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">BMR+20</a><span class="fn-bracket">]</span></span>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, 33:1877–1901, 2020.</p>
</div>
<div class="citation" id="id1542" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">DAJ24</a><span class="fn-bracket">]</span></span>
<p>Abhimanyu Dubey and et al Abhinav Jauhri. The llama 3 herd of models. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a>, <a class="reference external" href="https://arxiv.org/abs/2407.21783">arXiv:2407.21783</a>.</p>
</div>
<div class="citation" id="id1528" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">IS15</a><span class="fn-bracket">]</span></span>
<p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. 2015. URL: <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>, <a class="reference external" href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>.</p>
</div>
<div class="citation" id="id1545" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JSM+23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id16">1</a>,<a role="doc-backlink" href="#id17">2</a>,<a role="doc-backlink" href="#id18">3</a>)</span>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2310.06825">https://arxiv.org/abs/2310.06825</a>, <a class="reference external" href="https://arxiv.org/abs/2310.06825">arXiv:2310.06825</a>.</p>
</div>
<div class="citation" id="id1547" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PSL22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id19">1</a>,<a role="doc-backlink" href="#id20">2</a>,<a role="doc-backlink" href="#id21">3</a>)</span>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: attention with linear biases enables input length extrapolation. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a>, <a class="reference external" href="https://arxiv.org/abs/2108.12409">arXiv:2108.12409</a>.</p>
</div>
<div class="citation" id="id1544" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RZL17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1710.05941">https://arxiv.org/abs/1710.05941</a>, <a class="reference external" href="https://arxiv.org/abs/1710.05941">arXiv:1710.05941</a>.</p>
</div>
<div class="citation" id="id1538" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">Sha19</a><span class="fn-bracket">]</span></span>
<p>Noam Shazeer. Fast transformer decoding: one write-head is all you need. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a>, <a class="reference external" href="https://arxiv.org/abs/1911.02150">arXiv:1911.02150</a>.</p>
</div>
<div class="citation" id="id1543" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Sha20</a><span class="fn-bracket">]</span></span>
<p>Noam Shazeer. Glu variants improve transformer. 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2002.05202">https://arxiv.org/abs/2002.05202</a>, <a class="reference external" href="https://arxiv.org/abs/2002.05202">arXiv:2002.05202</a>.</p>
</div>
<div class="citation" id="id1548" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SLP+23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id22">1</a>,<a role="doc-backlink" href="#id23">2</a>)</span>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2104.09864">https://arxiv.org/abs/2104.09864</a>, <a class="reference external" href="https://arxiv.org/abs/2104.09864">arXiv:2104.09864</a>.</p>
</div>
<div class="citation" id="id410" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XYH+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id8">2</a>)</span>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In <em>International Conference on Machine Learning</em>, 10524–10533. PMLR, 2020.</p>
</div>
<div class="citation" id="id1541" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">YYHBZ24</a><span class="fn-bracket">]</span></span>
<p>An Yang, Baosong Yang, Binyuan Hui, and et al. Bo Zheng. Qwen2 technical report. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.10671">https://arxiv.org/abs/2407.10671</a>, <a class="reference external" href="https://arxiv.org/abs/2407.10671">arXiv:2407.10671</a>.</p>
</div>
<div class="citation" id="id1527" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZS19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1910.07467">https://arxiv.org/abs/1910.07467</a>, <a class="reference external" href="https://arxiv.org/abs/1910.07467">arXiv:1910.07467</a>.</p>
</div>
<div class="citation" id="id1518" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">ZZL+23</a><span class="fn-bracket">]</span></span>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. <em>arXiv preprint arXiv:2303.18223</em>, 2023.</p>
</div>
<div class="citation" id="id1532" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">ZZL+24</a><span class="fn-bracket">]</span></span>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a>, <a class="reference external" href="https://arxiv.org/abs/2303.18223">arXiv:2303.18223</a>.</p>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnote1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">1</a><span class="fn-bracket">]</span></span>
<p>See <a class="reference external" href="https://docs.mistral.ai/guides/tokenization/">https://docs.mistral.ai/guides/tokenization/</a> for details for tokenizer construction and usage in Mistral LLMs.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_LLM_arch"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_foundation/GPT_series.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>GPT Series</p>
      </div>
    </a>
    <a class="right-next"
       href="../notebooks/chapter_LLM_arch/annotated_llama.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>*Annotated LLama</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">9.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">9.2. Layer Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-basics">9.2.1. Layer normalization basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-norm-root-mean-square-norm">9.2.2. RMS Norm (Root Mean Square Norm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-position">9.2.3. Layer normalization position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-example-choices">9.2.4. Layer normalization example choices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinearity-in-ffn">9.3. Nonlinearity in FFN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-variants">9.4. Self-attention Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-mha">9.4.1. Multi-Head Attention (MHA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-query-attention-mqa">9.4.2. Multi Query Attention (MQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention-gqa">9.4.3. Grouped Query Attention (GQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sliding-window-attention">9.4.4. Sliding Window Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-encoding-and-long-context">9.5. Position Encoding and Long Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-position-encoding">9.5.1. Absolute Position Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alibi">9.5.2. ALiBi</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotary-postion-embedding">9.5.3. Rotary Postion Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mechanism">9.5.3.1. The mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-rope">9.5.3.2. Properties of RoPE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-context-windows">9.6. Extending Context Windows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-interpolation-for-rope">9.6.1. Position Interpolation for RoPE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ntk-aware-rope">9.6.2. NTK-Aware RoPE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-chunk-attention">9.6.3. Dual Chunk Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenziation-vocabulary-and-weight-tying">9.7. Tokenziation, vocabulary, and weight tying</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">9.7.1. BPE Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-bpe-to-bbpe">9.7.2. From BPE to BBPE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-composition-in-transformer-models">9.8. Parameter composition in Transformer models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-computation-breadown">9.9. Forward Pass Computation Breadown</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-architecture-examples">9.10. Dense Architecture Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">9.11. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>