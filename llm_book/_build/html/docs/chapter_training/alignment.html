
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>13. LLM Alignement and Preference Learning &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/alignment';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="14. LLM Reasoning (WIP)" href="reasoning.html" />
    <link rel="prev" title="12. LLM Finetuning" href="finetuning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architecture Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="reasoning.html">14. LLM Reasoning (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/llama_series.html">17. Llama Series (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/deepseek_series.html">18. DeepSeek Series (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">23. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">24. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">25. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">26. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">27. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">28. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">29. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Alignement and Preference Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-overview">13.1. Motivation and Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-using-rlhf">13.2. Alignment Using RLHF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-methodology">13.2.1. Overall methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-and-reward-modeling">13.2.2. Preference Data and Reward Modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp-and-reinforcement-learning">13.2.3. Markov Decision Process (MDP) and Reinforcement learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ppo-algorithm">13.2.4. The PPO Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-deep-dive">13.2.5. PPO Deep Dive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-sft-vs-rlhf">13.2.6. Discussion: SFT vs RLHF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-reward-model-criticality">13.2.7. Discussion: Reward Model Criticality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-variants">13.3. RL Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#group-relative-policy-optimization-grpo">13.3.1. Group Relative Policy Optimization (GRPO)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">13.4. DPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">13.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-preference-modeling">13.4.2. Preliminary: Preference modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#driving-the-dpo">13.4.3. Driving the DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-dpo-vs-rl">13.4.4. Discussion: DPO vs RL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-dpo">13.4.5. Iterative DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-variants">13.5. DPO Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-preference-label">13.5.1. Smoothing preference label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-dpo">13.5.2. Simple DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-positive-and-regularized-dpo">13.5.3. DPO-Positive and Regularized DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cringe-loss">13.5.4. Cringe Loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">13.6. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-alignement-and-preference-learning">
<span id="chapter-training-sec-llm-alignment"></span><h1><span class="section-number">13. </span>LLM Alignement and Preference Learning<a class="headerlink" href="#llm-alignement-and-preference-learning" title="Link to this heading">#</a></h1>
<section id="motivation-and-overview">
<h2><span class="section-number">13.1. </span>Motivation and Overview<a class="headerlink" href="#motivation-and-overview" title="Link to this heading">#</a></h2>
<p>The objective function in LLM pretraining is predicting the next token in the training corpus. When the trained model is properly and carefully prompted with demonstrations (i.e., in-context learning as in <a class="reference internal" href="../chapter_foundation/GPT_series.html#content-chapter-foundation-gpt-series-gpt-3"><span class="std std-ref">GPT-3</span></a>), the model can largely accomplish useful tasks by following these demonstrations. However, these model can often generate un-desired outputs, including un-factual content, biased and harmful text, or simply do not follow the instructions in the prompt.</p>
<p>This is because the pretraining task of <em>predicting the next token</em> is inherently different from the objective of training an LLM to be an instruction-following assistant that avoids generating unintended text. Although <strong>instruction tuning data</strong> (<a class="reference internal" href="finetuning.html#chapter-training-sec-llm-finetuning"><span class="std std-ref">LLM Finetuning</span></a>), which are (prompt, completion) pairs, can expose the LLM to what humans like to see for given prompts, it is often not enough to prevent model from producing unintended texts. As shown <a class="reference internal" href="#chapter-training-fig-alignment-sft-drawback-empirical-data"><span class="std std-numref">Fig. 13.1</span></a>, when SFT an LLM on prefered harmless responses in the HH-RLHF dataset <span id="id1">[<a class="reference internal" href="#id547" title="Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, and et al. Constitutional AI: harmlessness from AI feedback. ArXiv:2212.08073, 2022.">BKK+22</a>]</span>, the log probability of preferred and unwanted responses both exhibited a simultaneous increase. This indiciates that despite the cross-entropy loss can effectively guide the model toward the intended domain (e.g., dialogue), the absence of a penalty also increases the probablity of generating unwanted responses.</p>
<figure class="align-default" id="chapter-training-fig-alignment-sft-drawback-empirical-data">
<a class="reference internal image-reference" href="../../_images/SFT_drawback_empirical_data.png"><img alt="../../_images/SFT_drawback_empirical_data.png" src="../../_images/SFT_drawback_empirical_data.png" style="width: 661.6500000000001px; height: 427.35px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">Log probabilities for chosen and rejected responses during model fine-tuning on HH-RLHF dataset. Despite only chosen responses being used for SFT, rejected responses show a comparable likelihood of generation. Image from <span id="id2">[<a class="reference internal" href="#id1585" title="Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv e-prints, pages arXiv–2403, 2024.">HLT24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignment-sft-drawback-empirical-data" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Instead, we need a training methodology to <strong>explicitly reward the model when it is well-behaved and penalize the model when it is mis-behaved</strong>. Training the model to learn the human preference using rewards and penalities are the core of LLM alignment and preference learning. The pioneering approach is using reinforcement learning via the PPO algorithm <span id="id3">[<a class="reference internal" href="#id37" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</p>
<p>As shown in the <a class="reference internal" href="#chapter-training-fig-alignment-model-alignment-motivation"><span class="std std-numref">Fig. 13.2</span></a>, while SFT on instruction tuning dataset can improve model helpfulness and instruction following abilities, reinforcement learning can help the model achieve much larger gains than SFT.</p>
<figure class="align-default" id="chapter-training-fig-alignment-model-alignment-motivation">
<a class="reference internal image-reference" href="../../_images/model_alignment_motivation.png"><img alt="../../_images/model_alignment_motivation.png" src="../../_images/model_alignment_motivation.png" style="width: 556.8000000000001px; height: 324.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.2 </span><span class="caption-text">Human evaluations of various models outputs show that how often outputs from each model were preferred to those from the 175B GPT-3 SFT model. The aligned models InstructGPT models (PPO-ptx) as well as variant (PPO) significantly outperform the GPT-3 baselines. Image from <span id="id4">[<a class="reference internal" href="#id37" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignment-model-alignment-motivation" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="alignment-using-rlhf">
<h2><span class="section-number">13.2. </span>Alignment Using RLHF<a class="headerlink" href="#alignment-using-rlhf" title="Link to this heading">#</a></h2>
<section id="overall-methodology">
<h3><span class="section-number">13.2.1. </span>Overall methodology<a class="headerlink" href="#overall-methodology" title="Link to this heading">#</a></h3>
<p>The Alignment methodology <span id="id5">[<a class="reference internal" href="#id37" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>, <a class="reference internal" href="#id1567" title="Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.">SOW+20</a>]</span> has the following three steps [<a class="reference internal" href="#chapter-training-fig-alignment-rlhf-demo"><span class="std std-numref">Fig. 13.3</span></a>].</p>
<p><strong>Step 1: SFT on demeonstration data</strong>: Collect demonstration data showing the target output given different prompt input, and SFT the model to mimic the target output.</p>
<p><strong>Step 2: Preference/comparison labeling and reward modeling</strong>: Collect preference data, and train a reward model. The preference data set consists of labeler’s preferences towards different model outputs. Such preference data will be used to train a reward model to predict if human would prefer the model output given a model input.</p>
<p><strong>Step 3 Optimize model generation policy with reward model</strong>: The reward model will be used to guide the model’s improvement on producing human preferred outputs. The optimization can be done using reinforcement learning, particularly the PPO algorithm <span id="id6">[<a class="reference internal" href="#id1561" title="John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. 2017. URL: https://arxiv.org/abs/1707.06347, arXiv:1707.06347.">SWD+17</a>]</span>.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 13.1 </span> (Iterative reward model and policy improvement)</p>
<section class="example-content" id="proof-content">
<p><strong>Steps 2 and 3 can be, and sometimes must be, iterated continuously</strong>; with model policy improved using reward model, the reward model might need be updated to further guide the improvement of the data. The reason is that</p>
<ul class="simple">
<li><p>The text generated from improved model will have a different distribution than what is used in reward model training.</p></li>
<li><p>The trained reward model <span class="math notranslate nohighlight">\(R_0\)</span> is an approximate to the groundtruth reward model <span class="math notranslate nohighlight">\(R_{GT}\)</span>. After one round of policy optimization, the model is likely overfitting to the <span class="math notranslate nohighlight">\(R_0\)</span>, and actually performs poorly under the evaluation of <span class="math notranslate nohighlight">\(R_{GT}\)</span> (also see <span id="id7">[<a class="reference internal" href="#id1567" title="Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.">SOW+20</a>]</span>).</p></li>
</ul>
<p>To iterate the reward model to adapte to the new input distribution determined by the policy, we can collect more preference data, and combine with the original preference data to train a new RM and then a new policy.</p>
</section>
</div><figure class="align-default" id="chapter-training-fig-alignment-rlhf-demo">
<a class="reference internal image-reference" href="../../_images/RLHF_demo.png"><img alt="../../_images/RLHF_demo.png" src="../../_images/RLHF_demo.png" style="width: 699.5px; height: 413.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.3 </span><span class="caption-text">A diagram illustrating the three steps of our method: (1) supervised fine-tuning (SFT), (2)
reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO)
on this reward model. Image from <span id="id8">[<a class="reference internal" href="#id37" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignment-rlhf-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13.1 </span> (The importance of accurate reward model)</p>
<section class="remark-content" id="proof-content">
<p>In classical RL, the reward model is usually deterministic and given. And RL algorithm optimizes the policy to maximize the reward. In LLM alignment, the reward model is not clearly defined and needs to be inferred from the preference data.</p>
<p>The size, quality, and distribution of the preference data affects how good we can train a reward model that approximates the ground-truth reward model. If there is a gap between the trained reward model and the ground-truth reward model, the gap will translate to the sub-optimality of the learned policy.</p>
</section>
</div></section>
<section id="preference-data-and-reward-modeling">
<h3><span class="section-number">13.2.2. </span>Preference Data and Reward Modeling<a class="headerlink" href="#preference-data-and-reward-modeling" title="Link to this heading">#</a></h3>
<p>After SFT process on positive example, the trained model has improved ability on producing human preferred output. <strong>But it often has the overfitting risk and does not generalize well to unseen input data distributions.</strong> Preference data collections aims to provide both positive and negative examples, which are then used to train a reward model to help guide the model to generalize.</p>
<p>In the preference data collection process, human labelers assess different model outputs given the same prompt input and rank the output based on the human preference. The following table show the scoring standard used in the preference ranking process.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1652">
<caption><span class="caption-number">Table 13.1 </span><span class="caption-text">How labeler evaluates the response quality</span><a class="headerlink" href="#id1652" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metadata</p></th>
<th class="head text-right"><p>Scale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Overall quality</p></td>
<td class="text-right"><p>Likert scale; 1-7</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Fails to follow the correct instruction / task</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Inappropriate for customer assistant</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Hallucination</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Satisifies constraint provided in the instruction</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Contains sexual content</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>…</p></td>
<td class="text-right"><p>…</p></td>
</tr>
</tbody>
</table>
</div>
<p>The objective of <strong>reward modeling</strong> is to train a model that take prompt <span class="math notranslate nohighlight">\(x\)</span> and one completion <span class="math notranslate nohighlight">\(y\)</span> as input and output a scalar score that align with human preference.
More specificlly, let <span class="math notranslate nohighlight">\(r(x, y)\)</span> be the model’s scalar output, we have</p>
<div class="math notranslate nohighlight">
\[r(x, y_w) &gt; r(x, y_l) ~\text{if} ~ y_w \succ w_l\]</div>
<p>where <span class="math notranslate nohighlight">\(y_w\)</span> and <span class="math notranslate nohighlight">\(y_l\)</span> are two completions of prompt <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(y_w\)</span> is the preferred completion compared to <span class="math notranslate nohighlight">\(y_l\)</span>.</p>
<p>Specifically, the loss function for the reward model (parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>) is given by:</p>
<div class="math notranslate nohighlight">
\[
L_\theta=-\frac{1}{\binom{K}{2}} E_{\left(x, y_w, y_l\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)\right]
\]</div>
<p>Here <span class="math notranslate nohighlight">\(K\)</span> (between 4 and 9) is the number reponses from the model for a given input, which forms <span class="math notranslate nohighlight">\(\binom{K}{2}\)</span> pairwise comparison for the labeler to rank. Usually, all completions associated with a model input are put into a single batch. This makes the training more efficient, as only one forward pass is needed, as well as helps model generation, as the model sees both positive and negative examples at the same time.</p>
<p>The interpretation of the loss function is that it encourages the reward model to give higher score to winning completions <span class="math notranslate nohighlight">\(y_w\)</span> then losing completions <span class="math notranslate nohighlight">\(y_l\)</span>.</p>
<p>The reward model can be initialized from the SFT model (e.g., a 6B model) with the final embedding layer removed and a predictor head added on top to the final token’s last layer hidden dimensions. The reward model is trained to take in a prompt and response, and output a scalar reward.</p>
<!-- Tends to overfitting to highly scored completions.

Instead, we train on all $\binom{K}{2}$ comparisons from each prompt as a single batch element. This is much more computationally efficient because it only requires a single forward pass of the RM for each completion (rather than $\binom{K}{2}$ forward passes for $K$ completions) and, because it no longer overfits, it achieves much improved validation accuracy and log loss.


每一个输入token最终都能够生成一个标量值。对于LLM来说，最后一个输入token的处理结果会采样变成next_token，现在变成了score，作为所有输入token的打分结果（其实也可以取所有token生成的score进行平均，通常是直接取最后一个score，训练的效果更好一些）。
预训练好的Reward模型可以参考：https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward
 -->
</section>
<section id="markov-decision-process-mdp-and-reinforcement-learning">
<h3><span class="section-number">13.2.3. </span>Markov Decision Process (MDP) and Reinforcement learning<a class="headerlink" href="#markov-decision-process-mdp-and-reinforcement-learning" title="Link to this heading">#</a></h3>
<p>To understand how we can use a reward model to guide the improvement of the LLM using reinforcement learning, we can use the <strong>MDP</strong> framework. In the MDP fraemwork, we view model text generation as an agent’s sequential decision process. In particular, the MDP agent provides a response to the given prompt, need to decide the action (which token to generate) at each step. The alignment of LLM is equivalent to optimizing the agent on decision making policy to complete a final human preferred sequence.</p>
<p>Mathematically, an MDP is charcterized by tuple <span class="math notranslate nohighlight">\(\langle\mathcal{S}, V, R, \pi, \gamma, T\rangle\)</span>.</p>
<ul class="simple">
<li><p>The initial state <span class="math notranslate nohighlight">\(\boldsymbol{s}_0 \in \mathcal{S}\)</span> is a task-specific prompt represented by <span class="math notranslate nohighlight">\(\boldsymbol{x}=\left(x_0, \cdots, x_m\right)\)</span>. That is, <span class="math notranslate nohighlight">\(\boldsymbol{s}_0 = \boldsymbol{x}\)</span>.</p></li>
<li><p>An action in the environment <span class="math notranslate nohighlight">\(a_t \in \mathcal{A}\)</span> consists of a token from our vocabulary <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p>The transition function or policy <span class="math notranslate nohighlight">\(\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}\)</span> decides an action <span class="math notranslate nohighlight">\(a_t\)</span> and updatethe state <span class="math notranslate nohighlight">\(s_{t-1}=\left(x_0, \cdots, x_m, a_0, \cdots, a_{t-1}\right)\)</span>.</p></li>
<li><p>At the end of an episode a reward <span class="math notranslate nohighlight">\(R: \mathcal{S} \rightarrow \mathbb{R}^1\)</span> is provided by the reward model. The episode ends when the current time step <span class="math notranslate nohighlight">\(t\)</span> exceeds the horizon <span class="math notranslate nohighlight">\(T\)</span> or an end of sentence (EOS) token is generated.</p></li>
</ul>
<p>Specifically, we will parameterize the <strong>step-wise policy</strong> by <span class="math notranslate nohighlight">\(\theta\)</span> such that a stochastic policy for each step is given by</p>
<div class="math notranslate nohighlight">
\[\pi_s(a|s, \theta) = Pr(a_t = a| s_t = s, \theta).\]</div>
<!-- 
Each environment is an NLP task: we are given a supervised dataset $\mathcal{D}=\left\{\left(\boldsymbol{x}^i, \boldsymbol{y}^i\right)\right\}_{i=1}^N$ of $N$ examples, where $\boldsymbol{x} \in \mathcal{X}$ is an language input and $\boldsymbol{y} \in \mathcal{Y}$ is the target string. 



Each episode in the MDP begins by sampling a datapoint $(\boldsymbol{x}, \boldsymbol{y})$ from our dataset and ends when the current time step $t$ exceeds the horizon $T$ or an end of sentence (EOS) token is generated. 


The input $\boldsymbol{x}=\left(x_0, \cdots, x_m\right)$ is a task-specific prompt that is used as our initial state $\boldsymbol{s}_0=\left(x_0, \cdots, x_m\right)$, where $s_0 \in \mathcal{S}$ and $\mathcal{S}$ is the state space with $x_m \in \mathcal{V}$.  The transition function $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ deterministically appends an action $a_t$ to the end of the state $s_{t-1}=\left(x_0, \cdots, x_m, a_0, \cdots, a_{t-1}\right)$. This continues until the end of the horizon $t \leq T$ and we obtain a state $s_T=\left(x_0, \cdots, x_m, a_0, \cdots, a_T\right)$. At the end of an episode a reward $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{Y} \rightarrow \mathbb{R}^1$ that depends on the $\left(s_T, \boldsymbol{y}\right)$ (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style -->
</section>
<section id="the-ppo-algorithm">
<span id="chapter-training-sec-llm-alignment-ppo-algorithm"></span><h3><span class="section-number">13.2.4. </span>The PPO Algorithm<a class="headerlink" href="#the-ppo-algorithm" title="Link to this heading">#</a></h3>
<p>PPO is a policy gradient algorithm used to find the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span>. We use the following notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((x, y)\)</span> are prompt input <span class="math notranslate nohighlight">\(x\)</span> and completion <span class="math notranslate nohighlight">\(y\)</span> drawn from distribution <span class="math notranslate nohighlight">\(D_{\pi}\)</span> dependent on policy <span class="math notranslate nohighlight">\(\pi\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\pi(y | x) \)</span> is the trajectory level policy that connects to stepwise policy <span class="math notranslate nohighlight">\(\pi_s (a | s)\)</span> via</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\pi(y | x) = \prod_{t &lt;= T} \pi_s(y_t| y_{( &lt; t)}, x).\]</div>
<p>We <strong>maximize</strong> the following objective function in the PPO RL training:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;\max_{\phi}\operatorname{Objective}_{\text{PPO}}(\phi) \\
=&amp; \max_{\phi} E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)\right] - \operatorname{KL} (\pi_\phi^{\mathrm{RL}} || \pi_\phi^{\mathrm{SFT}}) \\
=&amp; \max_{\phi} E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[\underbrace{r_\theta(x, y)}_{\text{reward gain}}-\beta \underbrace{\log \pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)}_{\text{KL regularization}}\right]
\end{align*}
\end{split}\]</div>
<p>which contains the <strong>reward gain</strong> and <strong>KL regularization penality</strong>.
Here  <span class="math notranslate nohighlight">\(\pi_\phi^{\mathrm{RL}}\)</span> is the RL policy to be optimized, <span class="math notranslate nohighlight">\(\pi^{\mathrm{SFT}}\)</span> is the supervised trained model’s model as regularizer. The KL penality term aims to ensure that the optimized policy does not severely deviate from the original policy and overfit to the reward gain.</p>
<p>Besides the KL penality, to further prevent language modeling performance regression, we can add an auxillary objective to maximize the likelihood on texts sampled from pretraining datasets. The final objective, named <strong>PPO-ptx</strong>, is given by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}_{\text{PPO-ptx}}(\phi)= \operatorname{Objective}_{\text{PPO}}(\phi) + \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{\text {pretrain }}\)</span> is the pretraining distribution.he pretraining loss coefficient, <span class="math notranslate nohighlight">\(\gamma\)</span>, control the strength of the KL penalty and pretraining gradients respectively.</p>
<p>In <a class="reference internal" href="#chapter-training-fig-alignment-ppo-training-demo"><span class="std std-numref">Fig. 13.4</span></a>, we illustrate the basic workflow of a PPO algorithm used to improve the langauge model. A more complete workflow is discussed in <a class="reference internal" href="#chapter-training-sec-llm-alignment-ppo-algorithm-deep-dive"><span class="std std-ref">PPO Deep Dive</span></a>.</p>
<figure class="align-default" id="chapter-training-fig-alignment-ppo-training-demo">
<a class="reference internal image-reference" href="../../_images/RLHF_PPO_training_demo.png"><img alt="../../_images/RLHF_PPO_training_demo.png" src="../../_images/RLHF_PPO_training_demo.png" style="width: 599.55px; height: 263.55px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.4 </span><span class="caption-text">Illustration of PPO based optimization, which uses a Frozen LLM and KL loss to prevent model optimization from going too far and reward model to encourge model to learn to generate highly-rewarded outputs.</span><a class="headerlink" href="#chapter-training-fig-alignment-ppo-training-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 13.2 </span> (exploitation and exploration aspects from <span class="math notranslate nohighlight">\(\gamma\)</span>)</p>
<section class="remark-content" id="proof-content">
<p><span class="math notranslate nohighlight">\(\gamma\)</span> controls the balance between exploitation and exploration. When <span class="math notranslate nohighlight">\(\gamma \rightarrow 0\)</span>, the learning will concentrate on the max reward with full exploitation. When <span class="math notranslate nohighlight">\(\gamma \rightarrow \infty\)</span>, optimal policy will be the same as <span class="math notranslate nohighlight">\(\pi_{\mathrm{sft}}\)</span> with full exploration.</p>
</section>
</div></section>
<section id="ppo-deep-dive">
<span id="chapter-training-sec-llm-alignment-ppo-algorithm-deep-dive"></span><h3><span class="section-number">13.2.5. </span>PPO Deep Dive<a class="headerlink" href="#ppo-deep-dive" title="Link to this heading">#</a></h3>
<p>Given the to-be-optimized poliy <span class="math notranslate nohighlight">\(\pi(\phi)\)</span> and a reference policy <span class="math notranslate nohighlight">\(\pi_{ref}\)</span>, the objective function (to be maximized) used to update <span class="math notranslate nohighlight">\(\pi\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}\left(\theta_{\phi}\right)=\min \left( r(y \mid x)A(s, a), \operatorname{Clip}\left(r(y \mid x), 1-\epsilon, 1+\epsilon\right) A(s, a)\right)
\]</div>
<p>Here</p>
<ul class="simple">
<li><p>Here <span class="math notranslate nohighlight">\(r(y \mid x)\)</span> is the log probability ratio between <span class="math notranslate nohighlight">\(\pi_\phi\)</span> and <span class="math notranslate nohighlight">\(\pi_{ref}\)</span> on generating <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, which is given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[r(y \mid x) = \frac{\pi_\phi(y \mid x)}{\pi_{\theta_{ref}}(y \mid x)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> and the <span class="math notranslate nohighlight">\(\operatorname{Clip}\)</span> are acting to prevent <span class="math notranslate nohighlight">\(\pi_\phi\)</span> from going far away from <span class="math notranslate nohighlight">\(\pi_{ref}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(A(y, x)\)</span> is a scalar representing the advantage, in terms of reward, of <span class="math notranslate nohighlight">\(y\)</span> with respect to other possible <span class="math notranslate nohighlight">\(y'\)</span> under policy <span class="math notranslate nohighlight">\(\pi_\phi\)</span> (we will discuss it shortly).</p></li>
</ul>
<p>The interpretation of the loss function is as follows:</p>
<ul class="simple">
<li><p><strong>Advantage is positive</strong>: The objective function reduces to</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}\left(\pi_\phi\right)=\min \left(r(y \mid x),(1+\epsilon)\right) A(s, a)
\]</div>
<p>Because the advantage is positive, the objective will increase if the action <span class="math notranslate nohighlight">\(y\)</span> becomes more likely (i.e., if <span class="math notranslate nohighlight">\(r(y \mid x) \)</span> increases). Here the min says that if <span class="math notranslate nohighlight">\(r(y \mid x) \)</span> already above <span class="math notranslate nohighlight">\(1 + \epsilon\)</span>, the policy will be not updated.</p>
<ul class="simple">
<li><p><strong>Advantage is negative</strong>: The objective function reduces to</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}\left(\pi_\phi\right)=\max \left(r(y \mid x),(1-\epsilon)\right) A(s, a)
\]</div>
<p>Because the advantage is positive, the objective will improve if the action <span class="math notranslate nohighlight">\(y\)</span> becomes less likely (i.e., if <span class="math notranslate nohighlight">\(r(y \mid x) \)</span> decrease). Here the min says that if <span class="math notranslate nohighlight">\(r(y \mid x) \)</span> already below <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, the policy will be not updated.</p>
<p>A typical implementation of PPO involves <strong>four models</strong>:</p>
<ul class="simple">
<li><p>An actor LLM (to be optimized) which specifies the generation policy.</p></li>
<li><p>An frozen actor LLM, which specifies the reference policy.</p></li>
<li><p>A reward model, which rate the final output <span class="math notranslate nohighlight">\(y\)</span> given prompt <span class="math notranslate nohighlight">\(x\)</span>. Reward model is trained before the PPO.</p></li>
<li><p>A value model, which estimates the expected reward at step <span class="math notranslate nohighlight">\(t\)</span> by following <strong>current policy</strong>. The input to the value model is <span class="math notranslate nohighlight">\((y_{&lt;t}, x)\)</span>.</p></li>
</ul>
<p>Given an output <span class="math notranslate nohighlight">\(y\)</span> under current policy, the advantage of this output is given $<span class="math notranslate nohighlight">\(A(y \mid x) = R(y \mid x) - V^{\pi}(x).\)</span>$</p>
<p>A positive advantage indicates <span class="math notranslate nohighlight">\(y\)</span> is an output better than average output from current policy and is worth reinforced; a negative advantage indicates <span class="math notranslate nohighlight">\(y\)</span> is an output not better than the average, and it needs to be penalized. Note that the value function is a function of current policy, therefore it needs to be updated when the policy is updated.</p>
<p>There are other advanced methods to estimate more fine-grained level advantages on the token-level, as summarized in <span id="id9">[<a class="reference internal" href="#id1573" title="Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, and others. Secrets of rlhf in large language models part i: ppo. arXiv preprint arXiv:2307.04964, 2023.">ZDG+23</a>]</span>.</p>
<figure class="align-default" id="chapter-training-fig-alignment-ppo-deep-dive-workflow">
<a class="reference internal image-reference" href="../../_images/PPO_deep_dive_workflow.png"><img alt="../../_images/PPO_deep_dive_workflow.png" src="../../_images/PPO_deep_dive_workflow.png" style="width: 703.3000000000001px; height: 353.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.5 </span><span class="caption-text">Illustration of complete workflow PPO optimization. Image from <span id="id10">[<a class="reference internal" href="#id1573" title="Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, and others. Secrets of rlhf in large language models part i: ppo. arXiv preprint arXiv:2307.04964, 2023.">ZDG+23</a>]</span></span><a class="headerlink" href="#chapter-training-fig-alignment-ppo-deep-dive-workflow" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- ### The PPO algorithm

There are four models needed in the PPO algorithm:
* Actor LLM 
* Frozen Actor LLM
* Value function model
* Frozen Reward model
  

### PPO implementation



 -->
</section>
<section id="discussion-sft-vs-rlhf">
<h3><span class="section-number">13.2.6. </span>Discussion: SFT vs RLHF<a class="headerlink" href="#discussion-sft-vs-rlhf" title="Link to this heading">#</a></h3>
<p>SFT adopts a teacher-forcing approach, which directly optimizes the likelihood of a demonstration output. Such a token-level training way essentially does <strong>behavior cloning</strong> to imitate the demonstrations behavior.</p>
<p>On the other hand, RLHF firstly learns the reward model from preference data, and then employs it to improve the LLM with RL training (e.g., PPO). <strong>The reward model not just encourages positive behavior, but also discourages undesired responses.</strong></p>
<p>In terms of generating comprehensive demonstration data vs preference labeling, preference labeling is much easier than writing the demonstration data.</p>
<p>RLHF essentially encourages LLMs to learn correct policies by contrasting the self-generated responses (<strong>discriminating between positive and negative responses</strong>). It not just forces the model to imitate external, <strong>positive only</strong> demonstration data but also forces the model to <strong>know when not to imitate</strong> or to know when to reject when there is uncertainty.</p>
<p>These negative penality signal can mitigate the hallucination or over-generalization issues associated with SFT on positive only data.</p>
<div class="proof example admonition" id="example-3">
<p class="admonition-title"><span class="caption-number">Example 13.2 </span> (SFT on positive only data can lead to over-generalization)</p>
<section class="example-content" id="proof-content">
<p>Let’s consider how an LLM learns about geographic facts through SFT. During training, it might see examples like:
Training Data:</p>
<p>Q: “What is the capital of France?” <br />
A: “Paris is the capital of France.” <br />
Q: “What is the capital of Germany?” <br />
A: “Berlin is the capital of Germany.” \</p>
<p>The model learns a simple pattern: “[City] is the capital of [Country].” While this works for legitimate countries, it leads to problematic overgeneralization for fictional/hypothetical places:</p>
<p>Q: “What is the capital of Mars?” <br />
A: “Olympus City is the capital of Mars.”</p>
<p>RLHF can mitigate the issue by introducing explicit examples of what not to do:</p>
<p>Question: “What is the capital of Mars?” <br />
✓ Preferred: “Mars is a planet and does not have a capital city. While there are various geographic features on Mars, including Olympus Mons and Valles Marineris, the concept of a capital city applies to political entities on Earth.” <br />
✗ Rejected: “Olympus Mons is the capital of Mars.” <br />
✗ Rejected: “The Mars Colony Capital was established in 2020.”</p>
</section>
</div><p>Like classic RL algorithms, RLHF has the drawbacks like sample inefficiency, training complexity and instability. When adapted to LLMs, RLHF further relies on a strong SFT model as initial model checkpoint for efficiently achieving good performance</p>
<p>Overall, SFT is particularly useful to increase the model capacity of pre-trained model checkpoints right after pretraining, while RLHF is promising to further improve the model capacity of SFT models.</p>
</section>
<section id="discussion-reward-model-criticality">
<span id="chapter-training-sec-llm-alignment-reward-model-criticality"></span><h3><span class="section-number">13.2.7. </span>Discussion: Reward Model Criticality<a class="headerlink" href="#discussion-reward-model-criticality" title="Link to this heading">#</a></h3>
<p>For aligning LLM to human preference, <strong>reward modeling plays crucial role of human preferences and set the optimization direction for the model</strong> - if the reward model is built incorrectly or inaccruately, the model is optimized towards the wrong direction.</p>
<p>Let the groundtruth reward model be <span class="math notranslate nohighlight">\(R_{GT}\)</span>. In reward modeling, we are training models <span class="math notranslate nohighlight">\(R_0\)</span> to approximate <span class="math notranslate nohighlight">\(R_{GT}\)</span>. The typical reward modeling involves collecting preference label from labeler and build the reward model by learning from preference labels.
The gap <span class="math notranslate nohighlight">\(R_0\)</span> and <span class="math notranslate nohighlight">\(R_{GT}\)</span> is affected by the following factors:</p>
<ul class="simple">
<li><p><strong>The preference data quality, quantity, and distribution</strong>; more specifically,</p>
<ul>
<li><p>label’s consistence with human preference</p></li>
<li><p>more high-quality preference data, the smaller the gap</p></li>
<li><p>the distribution should be broad and diverse to reflect the richness of the input space</p></li>
</ul>
</li>
<li><p><strong>The model’s learning capacity</strong> - a weak model cannot capture intricate aspects of human preferences.</p></li>
</ul>
<p>Suppose we obtain a reward model <span class="math notranslate nohighlight">\(R_0\)</span>. What happen as we optimizes the model policy towards the reward model? Optimizing against reward model <span class="math notranslate nohighlight">\(R_0\)</span> is supposed to make our policy align with human preferences, i.e., <span class="math notranslate nohighlight">\(R_{GT}\)</span>. But the <span class="math notranslate nohighlight">\(R_0\)</span> is not a perfect representation of our labeler preferences, as it has limited capacity and only sees a limited amount of preference data from a likely narrow distribution of inputs. Studies from <span id="id11">[<a class="reference internal" href="#id1569" title="Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, 10835–10866. PMLR, 2023.">GSH23</a>, <a class="reference internal" href="#id1567" title="Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.">SOW+20</a>]</span> show that optimizing towards an imperfect reward model can run into the overfitting risk, leading to a model achieving high score with respect to <span class="math notranslate nohighlight">\(R_0\)</span> but actually low score with respect to <span class="math notranslate nohighlight">\(R_{GT}\)</span> [<a class="reference internal" href="#chapter-training-fig-alignment-reward-model-overfitting"><span class="std std-numref">Fig. 13.6</span></a>]. To minimize the gap between <span class="math notranslate nohighlight">\(R_0\)</span> and <span class="math notranslate nohighlight">\(R_{GT}\)</span>, they also empirically show that one can enlarge the model size as well as the training data size.</p>
<figure class="align-default" id="chapter-training-fig-alignment-reward-model-overfitting">
<a class="reference internal image-reference" href="../../_images/reward_model_overfitting.png"><img alt="../../_images/reward_model_overfitting.png" src="../../_images/reward_model_overfitting.png" style="width: 749.6500000000001px; height: 256.85px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.6 </span><span class="caption-text">(Left) The overfitting phenomonon of optimizing the model towards an imperfect reward model, leading to a model achieving high score with respect to <span class="math notranslate nohighlight">\(R_0\)</span> (dash line) but actually low score with respect to <span class="math notranslate nohighlight">\(R_{GT}\)</span> (solid line). Here the KL distance w.r.t. the initial policy is used to measure the degree of over-optimization. (Right) To reduce the gap to <span class="math notranslate nohighlight">\(R_{GT}\)</span>, one can enlarge model size as well as enlarge preference training data.</span><a class="headerlink" href="#chapter-training-fig-alignment-reward-model-overfitting" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Studies from <span id="id12">[<a class="reference internal" href="#id1568" title="Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, and others. Secrets of rlhf in large language models part ii: reward modeling. arXiv preprint arXiv:2401.06080, 2024.">WZC+24</a>]</span> further reveals that</p>
<ul class="simple">
<li><p><strong>The importance of label quality</strong> - incorrect and ambiguous preference pairs in the dataset may hinder the reward
model from accurately capturing human intent.</p></li>
<li><p><strong>Poor generation of reward model</strong> - reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.</p></li>
</ul>
<p>To mitigate these drawbacks, they proposed that</p>
<ul class="simple">
<li><p>One can measure the strength of preferences within the data using ensemble reward models.</p></li>
<li><p>Use labeling smoothing to reduce the impact of noisy labels (also discussed in <a class="reference internal" href="#chapter-training-sec-llm-alignment-label-smoothing-dpo"><span class="std std-ref">Smoothing preference label</span></a>).</p></li>
<li><p>Use adaptive margin, originated from contrastive learning, in training reward model (also discussed in <a class="reference internal" href="#chapter-training-sec-llm-alignment-simple-dpo"><span class="std std-ref">Simple DPO</span></a>).</p></li>
</ul>
</section>
</section>
<section id="rl-variants">
<h2><span class="section-number">13.3. </span>RL Variants<a class="headerlink" href="#rl-variants" title="Link to this heading">#</a></h2>
<section id="group-relative-policy-optimization-grpo">
<span id="chapter-training-sec-llm-alignment-grpo"></span><h3><span class="section-number">13.3.1. </span>Group Relative Policy Optimization (GRPO)<a class="headerlink" href="#group-relative-policy-optimization-grpo" title="Link to this heading">#</a></h3>
<p>DeepSeek team <span id="id13">[<a class="reference internal" href="#id1632" title="Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.">SWZ+24</a>]</span> proposed a modified PPO, known as GRPO, to reduce the computation cost and improve the value function estimation for the original PPO algorithm. As shown in <code class="xref std std-numref docutils literal notranslate"><span class="pre">shao2024deepseekmath</span></code>, GRPO does not need an additional value model; baseline score for advantage computation is directly estimated from group scores, significantly reducing training resources.</p>
<figure class="align-default" id="chapter-training-fig-alignment-rl-variants-grpo-vs-ppo">
<a class="reference internal image-reference" href="../../_images/PPO_vs_GRPO.png"><img alt="../../_images/PPO_vs_GRPO.png" src="../../_images/PPO_vs_GRPO.png" style="width: 784.85px; height: 347.05px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.7 </span><span class="caption-text">Comparison of PPO and GRPO. GRPO does not need an additional value model; baseline score for advantage computation is directly estimated from group scores, significantly reducing training resources. Image from <span id="id14">[<a class="reference internal" href="#id1632" title="Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.">SWZ+24</a>]</span></span><a class="headerlink" href="#chapter-training-fig-alignment-rl-variants-grpo-vs-ppo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>More specifically, the loss function of GRPO is given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-training-sec-llm-alignment-rl-variants-eq-grpo">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-chapter-training-sec-llm-alignment-rl-variants-eq-grpo" title="Link to this equation">#</a></span>\[\mathcal{J}_{G R P O}(\theta)=\mathbb{E}_{q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)} \left[
\frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left(\min \left[r_{i,t} \hat{A}_{i, t}, \operatorname{clip}\left(r_{i,t}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{K L}\left[\pi_\theta| | \pi_{r e f}\right]\right)\right]
\]</div>
<p>Here</p>
<ul class="simple">
<li><p>Question <span class="math notranslate nohighlight">\(q\)</span> is sampled from distribution <span class="math notranslate nohighlight">\(P(Q)\)</span>; <span class="math notranslate nohighlight">\(G\)</span> output <span class="math notranslate nohighlight">\(\left\{o_i\right\}\)</span> are sampled from the old policy <span class="math notranslate nohighlight">\(\pi_{\theta_{o l d}}(O \mid q)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{i,t}\)</span> is the log probability ratio for output <span class="math notranslate nohighlight">\(i\)</span> at step <span class="math notranslate nohighlight">\(t\)</span>, which is given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[r_{i,t} = \frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{A}_{i, t}\)</span> is the group-estiamted advantage based on reward model <span class="math notranslate nohighlight">\(R\)</span> for output <span class="math notranslate nohighlight">\(i\)</span> at step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<ol class="arabic simple">
<li><p>If the reward model is an <strong>outcome reward model</strong> that provides rewards only at the end of the each output <span class="math notranslate nohighlight">\(o_t\)</span>, then</p></li>
</ol>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{A}_{i, t}= \hat{A}_{i} = \frac{R_{i}-\operatorname{mean}\left(\left\{R_{1}, R_{2}, \cdots, r_{G}\right\}\right)}{\operatorname{std}\left(\left\{R_{1}, R_{2}, \cdots, r_{G}\right\}\right)}\]</div>
<ol class="arabic simple" start="2">
<li><p>If the reward model is a <strong>process reward model</strong> that provides token-level rewards, then</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\hat{A}_{i, t}=\frac{R_{i,t}-\operatorname{mean}\left(\left\{R_{1,t}, R_{2,t}, \cdots, r_{G,t}\right\}\right)}{\operatorname{std}\left(\left\{R_{1,t}, R_{2,t}, \cdots, r_{G,t}\right\}\right)}\]</div>
<ul class="simple">
<li><p>The computation of KL divergence is based on an modified <a class="reference external" href="http://joschu.net/blog/kl-approx.html">low-variance unbiased estimator</a></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbb{D}_{K L}\left[\pi_\theta| | \pi_{r e f}\right]_{i,t}=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-1\]</div>
<p>When estimating advantages, GRPO directly use sample uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline.</p>
<p>Removing value function has critical benefits:</p>
<ul class="simple">
<li><p>The value function employed in PPO is typically another model of comparable size as
the policy model - training the value function itself brings a substantial computational burden</p></li>
<li><p>In the LLM context, usually only the last token is assigned a reward score by the reward model, such sparse reward also presents challenge to train an accurate value function.</p></li>
</ul>
<p>We summarize the GRPO algorithm in the following.</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 13.1 </span> (Iterative Group Relative Policy Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong>  Initial policy model <span class="math notranslate nohighlight">\(\pi_{\text{init}}\)</span>; reward models <span class="math notranslate nohighlight">\(R\)</span>; task prompts <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>;  Reward model iteration <span class="math notranslate nohighlight">\(I\)</span>, number of batches <span class="math notranslate nohighlight">\(M\)</span>, and gradient steps <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Output:</strong> <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span></p>
<ol class="arabic simple">
<li><p>Initialize policy model <span class="math notranslate nohighlight">\(\pi_{\theta} \leftarrow \pi_{\text{init}}\)</span></p></li>
<li><p><strong>For</strong> iteration = 1, …, <span class="math notranslate nohighlight">\(I\)</span> <strong>do</strong></p>
<ol class="arabic simple">
<li><p>Set reference model <span class="math notranslate nohighlight">\(\pi_{\text{ref}} \leftarrow \pi_{\theta}\)</span></p></li>
<li><p><strong>For</strong> step = 1, …, <span class="math notranslate nohighlight">\(M\)</span> <strong>do</strong></p>
<ol class="arabic simple">
<li><p>Sample a batch of tasks <span class="math notranslate nohighlight">\(\mathcal{D}_b\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>Update the old policy model <span class="math notranslate nohighlight">\(\pi_{\text{old}} \leftarrow \pi_{\theta}\)</span></p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(G\)</span> outputs <span class="math notranslate nohighlight">\(\{o_i\}_{i=1}^{G} \sim \pi_{\theta}(\cdot | q) \)</span> for each question <span class="math notranslate nohighlight">\( q \in \mathcal{D}_b \)</span></p></li>
<li><p>Compute rewards for each <span class="math notranslate nohighlight">\(o_i\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\hat{A_{i,t}}\)</span> for the <span class="math notranslate nohighlight">\(t\)</span>-th token of <span class="math notranslate nohighlight">\(o_i\)</span> through group relative advantage estimation.</p></li>
<li><p><strong>For</strong> GRPO iteration = 1, …, ( J ) <strong>do</strong></p>
<ol class="arabic simple">
<li><p>Update the policy model ( \pi_{\theta} ) by maximizing the GRPO objective {eq}``</p></li>
</ol>
</li>
</ol>
</li>
<li><p>Update reward model <span class="math notranslate nohighlight">\(R\)</span> through continuous training using a replay mechanism.</p></li>
</ol>
</li>
</ol>
</section>
</div></section>
</section>
<section id="dpo">
<h2><span class="section-number">13.4. </span>DPO<a class="headerlink" href="#dpo" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">13.4.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p><strong>DPO (Direct Preference Optimization)</strong> <span id="id15">[<a class="reference internal" href="#id36" title="Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: https://arxiv.org/abs/2305.18290, arXiv:2305.18290.">RSM+24</a>]</span> improves the classical RLHF-PPO algorithm from the following two aspects:</p>
<ul class="simple">
<li><p><strong>Additional reward model is no longer need</strong>; Instead, the LLM itself can act as a reward model itself; preference data is directly used to train an aligned model in one step.</p></li>
<li><p><strong>Reinforcement learning no longer need</strong>. Optimizing the policy of an LLM towards a reward model is mathematially equivalent to directly training the LLM as a reward model on the preference data.</p></li>
</ul>
<p>The following illustrates from the DPO paper to visually compare the differences between RLHF-PPO and DPO</p>
<figure class="align-default" id="chapter-training-fig-alignmenet-dpo-ppo-comparison">
<a class="reference internal image-reference" href="../../_images/DPO_PPO_comparison.png"><img alt="../../_images/DPO_PPO_comparison.png" src="../../_images/DPO_PPO_comparison.png" style="width: 814.95px; height: 167.85px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.8 </span><span class="caption-text">DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and
human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.
In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification
objective. Image from <span id="id16">[<a class="reference internal" href="#id36" title="Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: https://arxiv.org/abs/2305.18290, arXiv:2305.18290.">RSM+24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignmenet-dpo-ppo-comparison" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The DPO training pipelines consists of the following two steps:</p>
<ul class="simple">
<li><p><strong>Preference data construction</strong> - sample completions <span class="math notranslate nohighlight">\(y_1, y_2 \sim \pi_{\text {ref }}(\cdot \mid x)\)</span> for every prompt <span class="math notranslate nohighlight">\(x\)</span>, label with human preferences to construct the offline dataset of preferences <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\right\}_{i=1}^N\)</span></p></li>
<li><p><strong>Optimize the language model as a reward model</strong>, which is equivalent to optimize <span class="math notranslate nohighlight">\(\pi_\theta\)</span> to minimize <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{DPO}}\)</span> for the given <span class="math notranslate nohighlight">\(\pi_{\text {ref }}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and desired <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right].
\]</div>
<p>Note: Since the preference datasets are sampled using <span class="math notranslate nohighlight">\(\pi^{\mathrm{SFT}}\)</span>, we initialize <span class="math notranslate nohighlight">\(\pi_{\text {ref }}=\pi^{\mathrm{SFT}}\)</span> whenever available.</p>
</section>
<section id="preliminary-preference-modeling">
<h3><span class="section-number">13.4.2. </span>Preliminary: Preference modeling<a class="headerlink" href="#preliminary-preference-modeling" title="Link to this heading">#</a></h3>
<p>The <strong>Bradley-Terry model</strong> <span id="id17">[<a class="reference internal" href="#id1553" title="Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: i. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. URL: http://www.jstor.org/stable/2334029 (visited on 2024-09-20).">BT52</a>]</span> is a probability model for the outcome of pairwise comparisons between items, teams, or objects. Given a pair of items <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> drawn from some population, it estimates the probability that the pairwise comparison <span class="math notranslate nohighlight">\(i&gt;j\)</span> turns out true, as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Pr}(i\succ j)=\frac{s_i}{s_i+s_j}
\]</div>
<p>where <span class="math notranslate nohighlight">\(s_i\)</span> is a positive real-valued score assigned to individual <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="math notranslate nohighlight">
\[
P\left(y_1 \succ y_2 \mid x\right)=\frac{\exp \left[r\left(x, y_1\right)\right]}{\exp \left[r\left(x, y_1\right)\right]+\exp \left[r\left(x, y_2\right)\right]}\]</div>
<div class="proof remark admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 13.3 </span> (Relationship to logistic regression)</p>
<section class="remark-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\operatorname{logit} \operatorname{Pr}(i&gt;j)=\log \frac{\operatorname{Pr}(i&gt;j)}{1-\operatorname{Pr}(i&gt;j)}=\log \frac{\operatorname{Pr}(i&gt;j)}{\operatorname{Pr}(j&gt;i)}=\beta_i-\beta_j
\]</div>
</section>
</div></section>
<section id="driving-the-dpo">
<h3><span class="section-number">13.4.3. </span>Driving the DPO<a class="headerlink" href="#driving-the-dpo" title="Link to this heading">#</a></h3>
<p>Here we outline the key steps to derive the DPO objective function, and explain why optimizing LLM as a reward model is equivalent to optimizing its policy.</p>
<p>First we start with the objective of LLM alignment with a given <strong>fixed reward function</strong> <span class="math notranslate nohighlight">\(r\)</span> with a KL constraint,</p>
<div class="math notranslate nohighlight">
\[
\max_\pi \mathbb{E}_{x, y \sim \mathcal{D}_{\pi}}[r(x, y)]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi(y \mid x) \| \pi_{\text {ref }}(y \mid x)\right].
\]</div>
<p>It turns out that we can obtain the theoretical solution of <span class="math notranslate nohighlight">\(\pi_r(y|x)\)</span> given by</p>
<div class="math notranslate nohighlight">
\[\pi_r(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x) \exp(\frac{1}{\beta}r(x, y)),\]</div>
<p>where <span class="math notranslate nohighlight">\(Z(x)\)</span> is partition funciton dependent only on <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\pi_{\text{ref}}\)</span>, which is given by <span class="math notranslate nohighlight">\(Z(x)=\sum_y \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)\)</span>.</p>
<p>With some algebra, we can also represent the reward funciton with <span class="math notranslate nohighlight">\(\pi_r(y|x)\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[r(x, y) = \beta \log \frac{\pi_r (y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z.\]</div>
<div class="proof remark admonition" id="remark-6">
<p class="admonition-title"><span class="caption-number">Remark 13.4 </span> (Implicit reward)</p>
<section class="remark-content" id="proof-content">
<p>Note that here the reward can be interpreted as the log ratio of the likelihood of a response between the current policy model and the reference model. And the policy is the maximizer of the objective function given the reward.</p>
</section>
</div><p>Note that we have just shown that that reward function <span class="math notranslate nohighlight">\(r(x, y)\)</span> and its corresponding optimal policy <span class="math notranslate nohighlight">\(\pi_{\text{ref}}(y|x)\)</span> are inter-convertable, with a funciton <span class="math notranslate nohighlight">\(Z(x)\)</span> independent of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>This means that instead of <strong>numerically optimizing policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can also choose optimize the reward function.</strong> When the reward function is optimized, the policy is also optimized at the same time (in other words, we can analytically solve the optimal policy).</p>
<p>Given the available preference data, one formulation to optimize the reward function is the Bradley-Terry (BT) objective, that is</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{BT} = -\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma (r(y_w, x) - r(r_l, x)) \right].\]</div>
<p>By leveraging the relationship between reward <span class="math notranslate nohighlight">\(r\)</span> and policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can arrive at the DPO loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right].
\]</div>
<p>where the terms <span class="math notranslate nohighlight">\(\beta \log Z\)</span> are canceled during subtraction.</p>
<div class="proof remark admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 13.5 </span> (How DPO loss work)</p>
<section class="remark-content" id="proof-content">
<p>The gradient of DPO loss function is given by:</p>
<div class="math notranslate nohighlight">
\[\nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_w, y_l\right)=-\left(1-\hat{p}_\theta\right) [\underbrace{\nabla_\theta \log \pi_\theta\left(y_w\right)}_{\text {upweight } y_w}-\underbrace{\nabla_\theta \log \pi_\theta\left(y_l\right)}_{\text {downweight } y_l}]
\]</div>
<p>where for the preference completion pair <span class="math notranslate nohighlight">\(y_w \succ y_l\)</span>, as long as <span class="math notranslate nohighlight">\(\hat{p} &lt; 1\)</span>, there will gradients to upweight the probability of generating <span class="math notranslate nohighlight">\(y_w\)</span> and downweight the probability of generating <span class="math notranslate nohighlight">\(y_l\)</span>.</p>
</section>
</div><div class="proof remark admonition" id="remark-8">
<p class="admonition-title"><span class="caption-number">Remark 13.6 </span> (Monitor DPO training process)</p>
<section class="remark-content" id="proof-content">
<p>The DPO algorithm aims to make winning responses have higher probability and losing responses have lower probability. If the training works as expected, beside the overall loss is descreasing, we will see</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}\)</span> becomes larger for the same <span class="math notranslate nohighlight">\(y_w\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\log \frac{\pi_\theta\left(y_l  \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\)</span> becomes smaller for the same <span class="math notranslate nohighlight">\(y_l\)</span>.</p></li>
</ul>
</section>
</div><!-- ### Additional remark RL vs SFT vs DPO

% https://mp.weixin.qq.com/s/WKuEcsyMFkaKf19o20Ci3g -->
</section>
<section id="discussion-dpo-vs-rl">
<h3><span class="section-number">13.4.4. </span>Discussion: DPO vs RL<a class="headerlink" href="#discussion-dpo-vs-rl" title="Link to this heading">#</a></h3>
<p><strong>DPO and RL-PPO have the same objective - optimizing LLM’s generation behavior towards what human prefer.</strong> Since there is no theorectially correct human preference model <span class="math notranslate nohighlight">\(R_{GT} \)</span>available; instead of, we use colect preference label data <span class="math notranslate nohighlight">\(D\)</span> from human labelers to reflect what human prefer.</p>
<p>In the RL-PPO approach, a proxy reward model <span class="math notranslate nohighlight">\(R_0\)</span> is first learned from human-labeled data to approximate <span class="math notranslate nohighlight">\(R_{GT}\)</span>; then the PPO algorithm is used to optimize the model policy toward <span class="math notranslate nohighlight">\(R_0\)</span>, with the hope that optimizing towards <span class="math notranslate nohighlight">\(R_0\)</span> will more or less optimizing towards <span class="math notranslate nohighlight">\(R_{GT}\)</span>. As a comparison, instead of learning a reward model, DPO directly optimizes the policy over preference data.</p>
<p>Normally, the finite-sized <span class="math notranslate nohighlight">\(D\)</span> cannot cover the whole input-output space, and the proxy reward model <span class="math notranslate nohighlight">\(R_0\)</span> often performs poorly in the face of out-ofdistribution data [Also see <a class="reference internal" href="#chapter-training-sec-llm-alignment-reward-model-criticality"><span class="std std-ref">Discussion: Reward Model Criticality</span></a>]. Studies from <span id="id18">[<a class="reference internal" href="#id1570" title="Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024.">XFG+24</a>]</span> show that with an imperfect reward model, the policy <span class="math notranslate nohighlight">\(\pi_{\text{DPO}}\)</span> from DPO, which is trained to maximize <span class="math notranslate nohighlight">\(y_w\)</span> and minimize <span class="math notranslate nohighlight">\(y_l\)</span> probability, <strong>can unexpectedly favor out of distribution responses</strong> (i.e., output <span class="math notranslate nohighlight">\(y\)</span> that is different from <span class="math notranslate nohighlight">\(y_w\)</span> and <span class="math notranslate nohighlight">\(y_l\)</span> ) and lead to unpredictable behaviors.</p>
<p>The reason for the lack of robustness for DPO compared to RL-PPO is that</p>
<ul class="simple">
<li><p>DPO is learning from <strong>limited, offline generated</strong> preference data and there is no additional exploration of the input output space during training. The resulting model has a hard time to generate well beyond what is included in the preference data. In the loss function, this is a limited regularization effect on <span class="math notranslate nohighlight">\(\pi_{\text{DPO}}(y)\)</span>, when <span class="math notranslate nohighlight">\(y\)</span> is largely different from <span class="math notranslate nohighlight">\(y_w\)</span> or <span class="math notranslate nohighlight">\(y_l\)</span> in the training data.</p></li>
<li><p>RL-PPO is <strong>online</strong> learning with exploration. After obtaining the reward model, RL-PPO does not need offline generated data to train the policy <span class="math notranslate nohighlight">\(\pi_{\text{PPO}}\)</span>; the LLM agent itself generates outputs and learn from the reward signal from reward model. The self-generation approach enables RL to have in theory <strong>unlimited amount of data to cover much large ranges of input and output distributions</strong>. Even an output <span class="math notranslate nohighlight">\(y\)</span> is largely different from <span class="math notranslate nohighlight">\(y_w\)</span> or <span class="math notranslate nohighlight">\(y_l\)</span> in the training data, such <span class="math notranslate nohighlight">\(y\)</span> might be covered in the online exploration process; As a result, at least the KL regularization in the loss function can still properly guide <span class="math notranslate nohighlight">\(\pi_{\text{PPO}}(y)\)</span> to not to be far away from <span class="math notranslate nohighlight">\(\pi_{\text{ref}}(y)\)</span>.</p></li>
</ul>
<p>Nevertheless, from reward modeling perspective, DPO and RL-PPO share the vulnerbility to reward model</p>
<ul class="simple">
<li><p>Both DPO and RL-PPO is approximating the ground-truth reward model using limited, offline generated preference data. The size and distribution of preference data affect the gap of trained reward model and the ground-truth reward model. The gap in the reward model will translate to the gap of policy.</p></li>
<li><p>To overcome the limitation of reward modeling on offline generated data, we can use iterative approach to improve reward modeling (i.e., collecting additional preference data label on DPO or RL-PPO policies for continous reward modeling improvement).</p></li>
</ul>
<div class="proof remark admonition" id="remark-9">
<p class="admonition-title"><span class="caption-number">Remark 13.7 </span> (Effective implementing PPO is critical)</p>
<section class="remark-content" id="proof-content">
<p>Although RL-PPO has better robustness to imperfect reward model <span id="id19">[<a class="reference internal" href="#id1570" title="Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024.">XFG+24</a>]</span>, an effective implementation of PPO is critical. This involves tricks like advantage normalization, large batch size, and exponential moving average update for the reference model, etc.</p>
</section>
</div></section>
<section id="iterative-dpo">
<h3><span class="section-number">13.4.5. </span>Iterative DPO<a class="headerlink" href="#iterative-dpo" title="Link to this heading">#</a></h3>
<p>DPO can also be used iteratively (e.g., 3-5 iterations)to enhance the alignment results. As shown in <a class="reference internal" href="#chapter-training-fig-alignmenet-iterative-dpo"><span class="std std-numref">Fig. 13.9</span></a>,</p>
<ul class="simple">
<li><p>Starting with a SFT model checkpoint <span class="math notranslate nohighlight">\(M_0\)</span>, one can go through the DPO data annotation and training process to arrive at <span class="math notranslate nohighlight">\(M_1\)</span></p></li>
<li><p>Preference pair data will be collected from <span class="math notranslate nohighlight">\(M_1\)</span> and used to train <span class="math notranslate nohighlight">\(M_2\)</span>.</p></li>
</ul>
<p>Iterative DPO has demonstrated its effectiveness in different scenarios <span id="id20">[<a class="reference internal" href="#id1641" title="Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.">DJP+24</a>, <a class="reference internal" href="#id550" title="Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. LLaMA 2: open foundation and fine-tuned chat models. ArXiv:2307.09288, 2023b.">TMS+3b</a>, <a class="reference internal" href="#id1644" title="Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.">CDY+24</a>, <a class="reference internal" href="reasoning.html#id1635" title="Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.">PYC+24</a>, <a class="reference internal" href="#id1643" title="Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.">YPC+24</a>]</span>.</p>
<figure class="align-default" id="chapter-training-fig-alignmenet-iterative-dpo">
<a class="reference internal image-reference" href="../../_images/Iterative_DPO.png"><img alt="../../_images/Iterative_DPO.png" src="../../_images/Iterative_DPO.png" style="width: 734.4px; height: 276.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.9 </span><span class="caption-text">Workflow of iterative DPO Image from <span id="id21">[<a class="reference internal" href="#id36" title="Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: https://arxiv.org/abs/2305.18290, arXiv:2305.18290.">RSM+24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignmenet-iterative-dpo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="dpo-variants">
<h2><span class="section-number">13.5. </span>DPO Variants<a class="headerlink" href="#dpo-variants" title="Link to this heading">#</a></h2>
<section id="smoothing-preference-label">
<span id="chapter-training-sec-llm-alignment-label-smoothing-dpo"></span><h3><span class="section-number">13.5.1. </span>Smoothing preference label<a class="headerlink" href="#smoothing-preference-label" title="Link to this heading">#</a></h3>
<p>When the preference label is noisy (e.g., annotation error/noise), a more robust DPO approach is desired <span id="id22">[<a class="reference internal" href="#id38" title="Eric Mitchell. A note on dpo with noisy preferences &amp; relationship to ipo. 2023. URL: https://ericmitchell.ai/cdpo.pdf.">Mit23</a>]</span>. It assumes that the labels have been flipped with some small probability <span class="math notranslate nohighlight">\(\epsilon \in(0,0.5)\)</span>. We can use a conservative target distribution instead, <span class="math notranslate nohighlight">\(p\left(y_w \succ y_l\right)=1-\epsilon\)</span>, giving BCE loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right) &amp; =-(1-\epsilon) \log \hat{p}_\theta\left(y_w \succ y_l\right)-\epsilon \log \left(1-\hat{p}_\theta\left(y_w \succ y_l\right)\right) \\
&amp; =(1-\epsilon) \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_w, y_l\right)+\epsilon \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_l, y_w\right)
\end{aligned}
\end{split}\]</div>
<p>The gradient of <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right)\)</span> is reduced to the simplified form :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right) &amp; =-\left((1-\epsilon)\left(1-\hat{p}_\theta\right)-\epsilon \hat{p}_\theta\right)[\underbrace{\nabla_\theta \log \pi_\theta\left(y_w\right)}_{\text {upweight } y_w}-\underbrace{\nabla_\theta \log \pi_\theta\left(y_l\right)}_{\text {downweight } y_l}] \\
&amp; =\quad\left(\hat{p}_\theta-(1-\epsilon)\right)\left[\nabla_\theta \log \pi_\theta\left(y_w\right)-\nabla_\theta \log \pi_\theta\left(y_l\right)\right]
\end{aligned}
\end{split}\]</div>
<!-- The gradient is zero when $\hat{p}_\theta\left(y_w \succ y_l\right)=(1-\epsilon)$, i.e., our (implicit) reward assigns the desired confidence level in this training example under the Bradley-Terry model [2]. For normal DPO, the gradient is never zero! Using the shorthand $h_{\pi_\theta}^{y_w, y_l}=\log \frac{\pi_\theta\left(y_w\right)}{\pi_{\text {ref }}\left(y_w\right)}-\log \frac{\pi_\theta\left(y_l\right)}{\pi_{\text {ref }}\left(y_l\right)}$, let's compare the conservative DPO (cDPO?) -->
</section>
<section id="simple-dpo">
<span id="chapter-training-sec-llm-alignment-simple-dpo"></span><h3><span class="section-number">13.5.2. </span>Simple DPO<a class="headerlink" href="#simple-dpo" title="Link to this heading">#</a></h3>
<p>The simple DPO<span id="id23">[<a class="reference internal" href="#id39" title="Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: simple preference optimization with a reference-free reward. 2024. URL: https://arxiv.org/abs/2405.14734, arXiv:2405.14734.">MXC24</a>]</span> improve the original DPO from two aspects:</p>
<ul class="simple">
<li><p>Make the sequence likelihood function used in implicit reward be aligned with the likelihood of actual sequence decoding.</p></li>
<li><p>Add a margin to encourage larger reward gap between positive sequence and negative sequence, which will help generalzation.</p></li>
</ul>
<p>First, the authors argue that original DPO derives the closed form implicit reward as the log ratio of the likelihood of a response between the current policy model and the reference model plus a constant only depending on <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[r(x, y)=\beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} + Const(x).\]</div>
<p>In actual decoding process, the likelihood of a response is usually <strong>length averaged</strong> (see <a class="reference internal" href="../chapter_inference/inference_fundamentals.html#chapter-inference-sec-deconding-beam-search"><span class="std std-ref">Beam search decoding</span></a>), for example,</p>
<div class="math notranslate nohighlight">
\[p_\theta(y \mid x)=\frac{1}{|y|} \log \pi_\theta(y \mid x)=\frac{1}{|y|} \sum_{i=1}^{|y|} \log \pi_\theta\left(y_i \mid x, y_{&lt;i}\right)\]</div>
<p>Naturally, we consider replacing the reward formulation in DPO with <span class="math notranslate nohighlight">\(p_\theta\)</span> above, so that it aligns with the likehood metric that guides generation. This results in a length-normalized reward:</p>
<div class="math notranslate nohighlight">
\[
r_{\mathrm{SimPO}}(x, y)=\frac{\beta}{|y|} \log \pi_\theta(y \mid x)=\frac{\beta}{|y|} \sum_{i=1}^{|y|} \log \pi_\theta\left(y_i \mid x, y_{&lt;i}\right)
\]</div>
<p>The author argues that normalizing the reward with response lengths is crucial: <strong>removing the length normalization term from the reward formulation results in a bias toward generating longer but lower-quality sequences.</strong></p>
<p>Additionally,a target <strong>reward margin term</strong>, <span class="math notranslate nohighlight">\(\gamma&gt;0\)</span>, is added to the Bradley-Terry objective to ensure that the reward for the winning response, <span class="math notranslate nohighlight">\(r\left(x, y_w\right)\)</span>, exceeds the reward for the losing response, <span class="math notranslate nohighlight">\(r\left(x, y_l\right)\)</span>, by at least <span class="math notranslate nohighlight">\(\gamma\)</span>. This is margin idea is also commonly used in constrastive learning.</p>
<div class="math notranslate nohighlight">
\[
p\left(y_w \succ y_l \mid x\right)=\sigma\left(r\left(x, y_w\right)-r\left(x, y_l\right)-\gamma\right)
\]</div>
<p>Combined these ideas together, we arrive at the SimPO loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\operatorname{SimPO}}\left(\pi_\theta\right)= -\mathbb{E}\left[\log \sigma\left(\frac{\beta}{\left|y_w\right|} \log \pi_\theta\left(y_w \mid x\right)-\frac{\beta}{\left|y_l\right|} \log \pi_\theta\left(y_l \mid x\right)-\gamma\right)\right]
\]</div>
<p>Note that, unlike the traditional DPO, SimPO does not require a reference model, making it more lightweight and easier to implement.</p>
</section>
<section id="dpo-positive-and-regularized-dpo">
<span id="chapter-training-sec-llm-alignment-dpo-variant-dpop-regularized-dpo"></span><h3><span class="section-number">13.5.3. </span>DPO-Positive and Regularized DPO<a class="headerlink" href="#dpo-positive-and-regularized-dpo" title="Link to this heading">#</a></h3>
<p>DPO and its variant usually perform well when the preference paired data consists of strong contrastive pairs, i.e., positive example and negative example are sharply different from edit distance perspective. For these examples, DPO can enhance the probability of generating the positive and reduce the probability of generating the negative.</p>
<p>However, for paired data that is small edit distance (i.e., positive and negative pairs look similiar), DPO algorithm can lead to <strong>failure mode - that is, both the generating probability of positive and negative example decrease</strong> (although negative ones decrease more).</p>
<p>Authors from <span id="id24">[<a class="reference internal" href="#id40" title="Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024.">PKD+24</a>]</span> not only provides an theoretical understanding of above phenomonon, they will propose one approach to mitigate the failure mode, known as <strong>DPO-Positive</strong> or <strong>DPO-P</strong>.</p>
<p>The key idea is to add a penality term when the model reduces the probability of positive examples. The modified loss function is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}_{\mathrm{DPOP}}\left(\pi_\theta\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \sigma \left(\beta \left(\log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}\right.\right.\right.  -\log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)} \\
 \left.\left.\left.-\lambda \cdot \max \left(0, \log \frac{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}{\pi_\theta\left(y_w \mid x\right)}\right)\right)\right)\right]
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> is a hyperparameter determining the strength of the penalty. From the Bradley-Terry modeling framework,</p>
<ul class="simple">
<li><p>for the negative <span class="math notranslate nohighlight">\(y_l\)</span>, the loss function is encourging <strong>minimize</strong> term of</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\beta \cdot \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}
\]</div>
<ul class="simple">
<li><p>for the positive <span class="math notranslate nohighlight">\(y_w\)</span>, the loss function is encouraging <strong>maximizing</strong> the term of</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\beta\left[\log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}(y_w \mid x)}-\lambda \cdot \max \left(0, \log \frac{\pi_{\mathrm{ref}}(y_w \mid x)}{\pi_\theta(y_w \mid x)}\right)\right].
\]</div>
<p>Clearly, if we want to maximize these terms for <span class="math notranslate nohighlight">\(y_w\)</span>, we need to ensure that the generating probability <span class="math notranslate nohighlight">\(\pi_{\theta}(y_w|x)\)</span> not to reduce too much from <span class="math notranslate nohighlight">\(\pi_{\text{ref}}(y_w|x)\)</span>.</p>
<p>On a similar line of thinking, <span id="id25">[<a class="reference internal" href="reasoning.html#id1635" title="Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.">PYC+24</a>]</span> proposed using the negative log likelihood (NLL) loss as a regularizer, which gives the following loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\mathrm{DPO-R}}\left(\pi_\theta\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \sigma \left(\beta \left(\log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)} -\log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right) + \frac{\lambda}{|y_w|}\log\left(\pi_\theta(y_w|x)\right) \right]
\]</div>
<p>The authors found that in reasoning tasks, having the NLL loss regularization is critical to promote the likelihood of positive (chosen) sequences [<a class="reference internal" href="#chapter-training-fig-alignmenet-dpo-variant-dpo-nll-regularizer"><span class="std std-numref">Fig. 13.10</span></a>].</p>
<figure class="align-default" id="chapter-training-fig-alignmenet-dpo-variant-dpo-nll-regularizer">
<a class="reference internal image-reference" href="../../_images/DPO_NLL_regularizer.png"><img alt="../../_images/DPO_NLL_regularizer.png" src="../../_images/DPO_NLL_regularizer.png" style="width: 685.3000000000001px; height: 228.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.10 </span><span class="caption-text">Effect on NLL regularizer loss on applying DPO in reasoning tasks. (left) Without regularizer loss, the likelihood of chosen sequences is decreasing when the model is initialized from Llama (left) and a positive-example FT checkpoint (right). After FT, the decreasing of the likelihood is more severe.   Image from <span id="id26">[<a class="reference internal" href="reasoning.html#id1635" title="Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.">PYC+24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignmenet-dpo-variant-dpo-nll-regularizer" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="cringe-loss">
<h3><span class="section-number">13.5.4. </span>Cringe Loss<a class="headerlink" href="#cringe-loss" title="Link to this heading">#</a></h3>
<p><span id="id27">[<a class="reference internal" href="#id1646" title="Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The cringe loss: learning what language not to model. arXiv preprint arXiv:2211.05826, 2022.">AGX+22</a>, <a class="reference internal" href="#id1645" title="Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.">XLSW23</a>]</span></p>
<!-- 提问：DPO的变体有哪些，主要解决DPO的什么问题？

回答：

RSO [1]：由于DPO的蒙特卡洛采样很难达到，所以其实DPO几乎是off-policy的采样方式，RSO主要从DPO的采样方式来解决DPO的问题。
Iterative DPO [2]：同样由于DPO的蒙特卡洛采样很难达到，所以通过on-policy的方式采样来替代off-policy的采样。
IPO [3]：由于BT model的目标是最大化正负response的reward gap，但其实其中忽略了真实情况下我们组的pair可能会有噪音，那么无限去扩大reward gap其实是不准确的，也就是overfit了preference的pair数据，那么解决方案是需要限制这个gap的范围。
DPOP [4]：由于LLM model很难区分编辑距离较小的pair，那么当持续去区分这批case的时候，模型效果会崩塌，现象是正例子和负例子的概率都往下掉。那么DPOP用了一个新项来惩罚正例往下掉的pair，使得正例概率继续提升。
[1] Liu T, Zhao Y, Joshi R, et al. Statistical rejection sampling improves preference optimization[J]. arXiv preprint arXiv:2309.06657, 2023.

[2] Yuan W, Pang R Y, Cho K, et al. Self-rewarding language models[J]. arXiv preprint arXiv:2401.10020, 2024.

[3] Azar M G, Rowland M, Piot B, et al. A general theoretical paradigm to understand learning from human preferences[J]. arXiv preprint arXiv:2310.12036, 2023.

[4] Pal A, Karkhanis D, Dooley S, et al. g: Fixing Failure Modes of Preference Optimisation with DPO-Positive[J]. arXiv preprint arXiv:2402.13228, 2024. -->
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">13.6. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id28">
<div role="list" class="citation-list">
<div class="citation" id="id1646" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">AGX+22</a><span class="fn-bracket">]</span></span>
<p>Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The cringe loss: learning what language not to model. <em>arXiv preprint arXiv:2211.05826</em>, 2022.</p>
</div>
<div class="citation" id="id547" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">BKK+22</a><span class="fn-bracket">]</span></span>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, and et al. Constitutional AI: harmlessness from AI feedback. <em>ArXiv:2212.08073</em>, 2022.</p>
</div>
<div class="citation" id="id1553" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">BT52</a><span class="fn-bracket">]</span></span>
<p>Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: i. the method of paired comparisons. <em>Biometrika</em>, 39(3/4):324–345, 1952. URL: <a class="reference external" href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a> (visited on 2024-09-20).</p>
</div>
<div class="citation" id="id1644" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">CDY+24</a><span class="fn-bracket">]</span></span>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. <em>arXiv preprint arXiv:2401.01335</em>, 2024.</p>
</div>
<div class="citation" id="id1641" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">DJP+24</a><span class="fn-bracket">]</span></span>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. <em>arXiv preprint arXiv:2407.21783</em>, 2024.</p>
</div>
<div class="citation" id="id1569" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">GSH23</a><span class="fn-bracket">]</span></span>
<p>Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In <em>International Conference on Machine Learning</em>, 10835–10866. PMLR, 2023.</p>
</div>
<div class="citation" id="id1585" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">HLT24</a><span class="fn-bracket">]</span></span>
<p>Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio. <em>arXiv e-prints</em>, pages arXiv–2403, 2024.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">MXC24</a><span class="fn-bracket">]</span></span>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: simple preference optimization with a reference-free reward. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2405.14734">https://arxiv.org/abs/2405.14734</a>, <a class="reference external" href="https://arxiv.org/abs/2405.14734">arXiv:2405.14734</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">Mit23</a><span class="fn-bracket">]</span></span>
<p>Eric Mitchell. A note on dpo with noisy preferences &amp; relationship to ipo. 2023. URL: <a class="reference external" href="https://ericmitchell.ai/cdpo.pdf">https://ericmitchell.ai/cdpo.pdf</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>OWJ+22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>,<a role="doc-backlink" href="#id8">4</a>)</span>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>, <a class="reference external" href="https://arxiv.org/abs/2203.02155">arXiv:2203.02155</a>.</p>
</div>
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">PKD+24</a><span class="fn-bracket">]</span></span>
<p>Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: fixing failure modes of preference optimisation with dpo-positive. <em>arXiv preprint arXiv:2402.13228</em>, 2024.</p>
</div>
<div class="citation" id="id1642" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PYC+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id20">1</a>,<a role="doc-backlink" href="#id25">2</a>,<a role="doc-backlink" href="#id26">3</a>)</span>
<p>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. <em>arXiv preprint arXiv:2404.19733</em>, 2024.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RSM+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id15">1</a>,<a role="doc-backlink" href="#id16">2</a>,<a role="doc-backlink" href="#id21">3</a>)</span>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>, <a class="reference external" href="https://arxiv.org/abs/2305.18290">arXiv:2305.18290</a>.</p>
</div>
<div class="citation" id="id1561" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">SWD+17</a><span class="fn-bracket">]</span></span>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>, <a class="reference external" href="https://arxiv.org/abs/1707.06347">arXiv:1707.06347</a>.</p>
</div>
<div class="citation" id="id1632" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SWZ+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. <em>arXiv preprint arXiv:2402.03300</em>, 2024.</p>
</div>
<div class="citation" id="id1567" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SOW+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id7">2</a>,<a role="doc-backlink" href="#id11">3</a>)</span>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. <em>Advances in Neural Information Processing Systems</em>, 33:3008–3021, 2020.</p>
</div>
<div class="citation" id="id550" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">TMS+3b</a><span class="fn-bracket">]</span></span>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. LLaMA 2: open foundation and fine-tuned chat models. <em>ArXiv:2307.09288</em>, 2023b.</p>
</div>
<div class="citation" id="id1568" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">WZC+24</a><span class="fn-bracket">]</span></span>
<p>Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, and others. Secrets of rlhf in large language models part ii: reward modeling. <em>arXiv preprint arXiv:2401.06080</em>, 2024.</p>
</div>
<div class="citation" id="id1645" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">XLSW23</a><span class="fn-bracket">]</span></span>
<p>Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: preference optimization with the pairwise cringe loss. <em>arXiv preprint arXiv:2312.16682</em>, 2023.</p>
</div>
<div class="citation" id="id1570" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XFG+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id18">1</a>,<a role="doc-backlink" href="#id19">2</a>)</span>
<p>Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. <em>arXiv preprint arXiv:2404.10719</em>, 2024.</p>
</div>
<div class="citation" id="id1643" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">YPC+24</a><span class="fn-bracket">]</span></span>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. <em>arXiv preprint arXiv:2401.10020</em>, 2024.</p>
</div>
<div class="citation" id="id1573" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZDG+23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, and others. Secrets of rlhf in large language models part i: ppo. <em>arXiv preprint arXiv:2307.04964</em>, 2023.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="finetuning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>LLM Finetuning</p>
      </div>
    </a>
    <a class="right-next"
       href="reasoning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>LLM Reasoning (WIP)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-overview">13.1. Motivation and Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-using-rlhf">13.2. Alignment Using RLHF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-methodology">13.2.1. Overall methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-and-reward-modeling">13.2.2. Preference Data and Reward Modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp-and-reinforcement-learning">13.2.3. Markov Decision Process (MDP) and Reinforcement learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ppo-algorithm">13.2.4. The PPO Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ppo-deep-dive">13.2.5. PPO Deep Dive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-sft-vs-rlhf">13.2.6. Discussion: SFT vs RLHF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-reward-model-criticality">13.2.7. Discussion: Reward Model Criticality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rl-variants">13.3. RL Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#group-relative-policy-optimization-grpo">13.3.1. Group Relative Policy Optimization (GRPO)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">13.4. DPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">13.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-preference-modeling">13.4.2. Preliminary: Preference modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#driving-the-dpo">13.4.3. Driving the DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-dpo-vs-rl">13.4.4. Discussion: DPO vs RL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-dpo">13.4.5. Iterative DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-variants">13.5. DPO Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-preference-label">13.5.1. Smoothing preference label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-dpo">13.5.2. Simple DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-positive-and-regularized-dpo">13.5.3. DPO-Positive and Regularized DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cringe-loss">13.5.4. Cringe Loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">13.6. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>