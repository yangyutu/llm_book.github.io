
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14. LLM Training Acceleration &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/accelerated_training';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="16. LLM Decoding" href="../chapter_inference/inference_fundamentals.html" />
    <link rel="prev" title="13. LLM Alignement and Preference learning" href="alignment.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MOE sparse models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">12. LLM finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignment.html">13. LLM Alignement and Preference learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. LLM Training Acceleration</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. LLM Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference acceleration</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">20. Basic prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">21. Advanced prompt techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">22. Basic RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">23. Information Retrieval Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">24. Application of LLM in IR</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Training Acceleration</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">14. LLM Training Acceleration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-requirement-for-training-llm">14.1. The Memory Requirement For Training LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-states">14.1.1. Model States</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">14.1.2. Activations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-part">14.1.2.1. MLP Part</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-part">14.1.2.2. Self-attention Part</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-checkpointing-techniques">14.1.3. Activation Checkpointing Techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-training">14.2. Mixed Precision Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">14.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">14.2.2. Training Process</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-parallel-training">14.3. Distributed Parallel Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-parallel-training-techniques">14.3.1. Overview of parallel training techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelism-tensor-parallelism">14.3.2. Model parallelism (tensor parallelism)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-via-deepspeed">14.4. ZeRO Via DeepSpeed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-allocation">14.4.1. GPU Memory Allocation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-stage-one">14.4.2. ZeRO-Stage-One</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention">14.5. Flash Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-softmax-motivation">14.5.1. Online Softmax Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-softmax-algorithm">14.5.2. Online Softmax Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-online-softmax-to-flash-attention">14.5.3. From Online Softmax To Flash Attention</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">15. Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-data-types">15.1. Floating Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-parallel-operations">15.2. GPU Parallel Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">15.3. Bibliography</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-training-acceleration">
<h1><span class="section-number">14. </span>LLM Training Acceleration<a class="headerlink" href="#llm-training-acceleration" title="Link to this heading">#</a></h1>
<section id="the-memory-requirement-for-training-llm">
<h2><span class="section-number">14.1. </span>The Memory Requirement For Training LLM<a class="headerlink" href="#the-memory-requirement-for-training-llm" title="Link to this heading">#</a></h2>
<!-- % https://medium.com/@maxshapp/understanding-and-estimating-gpu-memory-demands-for-training-llms-in-practise-c5ef20a4baff -->
<p>We will discuss the following in this section</p>
<ul class="simple">
<li><p>How much GPU memory do you need to train <span class="math notranslate nohighlight">\(X\)</span> billion Transformer based LLM per each GPU device.</p></li>
<li><p>What is the formula to estimate memory requirements.</p></li>
<li><p>What would you do in practise to reduce the memory needs if the model does not fit.</p></li>
</ul>
<section id="model-states">
<h3><span class="section-number">14.1.1. </span>Model States<a class="headerlink" href="#model-states" title="Link to this heading">#</a></h3>
<p>Consider the case that we train a LLM using Adam optimizer, we need to have enough GPU memory to store</p>
<ul class="simple">
<li><p>Copy of model parameter</p></li>
<li><p>Copy of model parameter gradients</p></li>
<li><p>Copy of optimizer states, include copy of the model parameters, momentum, and variance.</p></li>
</ul>
<p>Assume that</p>
<ul class="simple">
<li><p>Model parameters and graidents are stored in FP16 (2 bytes),</p></li>
<li><p>Optimizer states are stored in FP32 (4 bytes) for stable training
then training a <span class="math notranslate nohighlight">\(X\)</span> billion model requires following GPU memory amount just to store the model and training states</p></li>
</ul>
<div class="math notranslate nohighlight">
\[(2 + 2 + 12) X ~\text{(GB)}.\]</div>
<p>The following table gives the example for the memory requirement for the common 7B and 70B models.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model Size</p></th>
<th class="head text-right"><p>GPU Memory (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>7B</p></td>
<td class="text-right"><p>112 B</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>70B</p></td>
<td class="text-right"><p>1120 B</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="activations">
<h3><span class="section-number">14.1.2. </span>Activations<a class="headerlink" href="#activations" title="Link to this heading">#</a></h3>
<p>First, let’s have the following notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> - number of transformer layers</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span> - sequence length</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span> - batch size</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> - hidden dimension size</p></li>
<li><p><span class="math notranslate nohighlight">\(a\)</span> - number of attention heads</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> - precision</p></li>
</ul>
<section id="mlp-part">
<h4><span class="section-number">14.1.2.1. </span>MLP Part<a class="headerlink" href="#mlp-part" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The output of the first linear layer, which is <span class="math notranslate nohighlight">\(4psbh\)</span> (as this linear enlarges the output dimension to <span class="math notranslate nohighlight">\(4h\)</span>).</p></li>
<li><p>The output of the GeLU activation, which is <span class="math notranslate nohighlight">\(4psbh\)</span> bytes.</p></li>
<li><p>The output of the second linear layer, which is <span class="math notranslate nohighlight">\(psbh\)</span> bytes</p></li>
<li><p>The binary dropout marks specifies which dimensions are droped, which is <span class="math notranslate nohighlight">\(sbh\)</span>
So in total, MLP part will require to store: <span class="math notranslate nohighlight">\(9psbh + sbh\)</span> bytes for activations.</p></li>
</ul>
</section>
<section id="self-attention-part">
<h4><span class="section-number">14.1.2.2. </span>Self-attention Part<a class="headerlink" href="#self-attention-part" title="Link to this heading">#</a></h4>
<p>Attention block: which includes self attention followed by a linear projection and an attention dropout.</p>
<ul class="simple">
<li><p>Before entering the self-attention block, query, key, and value are passed through a linear projection layer, whose outputs requires in totoal <span class="math notranslate nohighlight">\(3psbh\)</span> bytes</p></li>
<li><p>The Softmax output is a <span class="math notranslate nohighlight">\(b \times s\times s\)</span> attention matrix for each head, which in total is <span class="math notranslate nohighlight">\(pas^2b\)</span> bytes; The Softmax input is the logis, which is also <span class="math notranslate nohighlight">\(pas^2b\)</span> bytes.</p></li>
<li><p>The Dropout mask after the Softmax layer needs <span class="math notranslate nohighlight">\(as^2b\)</span></p></li>
<li><p>Output from Self-Attention, which will require <span class="math notranslate nohighlight">\(psbh\)</span> bytes</p></li>
<li><p>Output from the Linear layer, which will require <span class="math notranslate nohighlight">\(psbh\)</span> bytes.</p></li>
<li><p>Dropout mask after the linear layer, this will require <span class="math notranslate nohighlight">\(sbh\)</span> bytes.</p></li>
</ul>
<p>To sum up, we need <span class="math notranslate nohighlight">\(5psbh + sbh + 2pas²b + as²b\)</span> bytes for Attention part.</p>
<p>Additionally, there are 2 Norm Layers in the Transformer Layer, the output from each such layer will require to store psbh bytes, so in total 2psbh bytes.</p>
<p>total amount of bytes required to store the activations will be approximately:</p>
<div class="math notranslate nohighlight">
\[Lpsbh\left(16+\frac{2}{p}+\frac{2 a s}{h}+\frac{a s}{p h}\right)\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">activations_memory</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="s2">&quot;Returns amount of GPU VRAM (in GB) required to store intermediate activations for traditional Transformer Encoder block&quot;</span>
    <span class="n">mem_bytes</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="p">(</span>
        <span class="mi">16</span> <span class="o">+</span> <span class="mi">2</span><span class="o">/</span><span class="n">precision</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">num_heads</span><span class="o">*</span><span class="n">seq_len</span><span class="o">/</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">num_heads</span><span class="o">*</span><span class="n">seq_len</span><span class="o">/</span><span class="p">(</span><span class="n">precision</span><span class="o">*</span><span class="n">hidden_dim</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">mem_bytes</span> <span class="o">/</span> <span class="mi">10</span><span class="o">**</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="activation-checkpointing-techniques">
<h3><span class="section-number">14.1.3. </span>Activation Checkpointing Techniques<a class="headerlink" href="#activation-checkpointing-techniques" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="mixed-precision-training">
<h2><span class="section-number">14.2. </span>Mixed Precision Training<a class="headerlink" href="#mixed-precision-training" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">14.2.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>Deep Neural Networks (DNNs) have lead to breakthroughs in a number of areas, including image processing and understanding, language modeling, language translation, speech processing, game playing, and many others. DNN complexity has been increasing to achieve these results, which in turn has increased the computational resources required to train these networks. Mixed-precision training<span id="id1">[<a class="reference internal" href="#id1492" title="Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.">MNA+17</a>]</span> lowers the required resources by using lower-precision arithmetic, which has the following benefits.</p>
<ul class="simple">
<li><p>Decrease the required amount of memory. Half-precision floating point format (FP16) uses 16 bits, compared to 32 bits for single precision (FP32). Lowering the required memory enables training of larger models or training with larger minibatches.</p></li>
<li><p>Shorten the training or inference time. Execution time can be sensitive to memory or arithmetic bandwidth. Half-precision halves the number of bytes accessed, thus reducing the time spent in memory-limited layers.</p></li>
</ul>
<p>Half-precision floating point format consists of 1 sign bit, 5 bits of exponent, and 10 fractional bits.  Supported exponent values fall into the <span class="math notranslate nohighlight">\([-24, 15]\)</span> range, which means the format supports non-zero value magnitudes in the <span class="math notranslate nohighlight">\([2^{-24}, 65,504]\)</span> range. Since this is narrower than the <span class="math notranslate nohighlight">\([2-149, \sim3.4\times1038]\)</span> range supported by single-precision format, training some networks requires extra consideration.</p>
</section>
<section id="training-process">
<h3><span class="section-number">14.2.2. </span>Training Process<a class="headerlink" href="#training-process" title="Link to this heading">#</a></h3>
<p>This section describes three techniques for successful training of DNNs with half precision: accumulation of FP16 products into FP32; loss scaling; and an FP32 master copy of weights. With these techniques NVIDIA and Baidu Research were able to match single-precision result accuracy for all networks that were trained.<span id="id2">[<a class="reference internal" href="#id1492" title="Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.">MNA+17</a>]</span></p>
<figure class="align-default" id="chapter-training-fig-efficient-training-mixed-precision-process-demo">
<a class="reference internal image-reference" href="../../_images/mixed_precision_process_demo.png"><img alt="../../_images/mixed_precision_process_demo.png" src="../../_images/mixed_precision_process_demo.png" style="width: 484.04999999999995px; height: 452.54999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.1 </span><span class="caption-text">Model training step with mixed precision using classifical Adam algorithm [<a class="reference internal" href="training_fundamentals.html#Adam_stochastic_gradient_descent_algorithm">Algorithm 11.2</a>].</span><a class="headerlink" href="#chapter-training-fig-efficient-training-mixed-precision-process-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>key elements in the mixed-precision training</p>
<ul class="simple">
<li><p>Maintain a master copy of model parameters, optimizer momentums and variances with fp32 precision.</p></li>
<li><p>Before the model forward pass begins, allocate new storage to save model parameters in the fp16 format.</p></li>
<li><p>Perform forward pass, the produced activations will be saved as fp16.</p></li>
<li><p>Perform backward pass, the produced gradients will be saved as fp16.</p></li>
<li><p>Use fp16 gradients to update model parameters that are saved as fp32.</p></li>
</ul>
<p>We can estimate the memory storage consumption according to the following table. Denote the model parameter size by <span class="math notranslate nohighlight">\(\Phi\)</span>. Let the storage unit be byte. We need <span class="math notranslate nohighlight">\(16\Phi\)</span> memory storage in total.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1516">
<caption><span class="caption-number">Table 14.1 </span><span class="caption-text">Storage requirement for different components during LLM training using Adam.</span><a class="headerlink" href="#id1516" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Type</p></th>
<th class="head text-left"><p>Storage Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Parameter (fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Momentum(fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Variance (fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Parameter (fp16)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 \Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Gradients (fp16)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Total</strong>:</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(16 \Phi\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>\begin{remark}[the size of activations and model parameters]
The number of activations is typically much smaller than the size of the model parameters.
Take an MLP with input dim <span class="math notranslate nohighlight">\(D_0\)</span>, hidden dim <span class="math notranslate nohighlight">\(D_1\)</span>, and output dim <span class="math notranslate nohighlight">\(D_2\)</span> as an example. The total number of activations, including initial inputs, are <span class="math notranslate nohighlight">\(D_0 + D_1 + D_2\)</span>.</p>
<p>The total number of model parameters are <span class="math notranslate nohighlight">\(D_0\times D_1 + D_1\times D_2\)</span>.
\end{remark}</p>
<p>\begin{remark}[Precision options fp16, bf16, and fp8]
Both BF16 and FP16 are 16-bit floating-point formats used in deep learning models. The difference between them is how the bits are divided between the exponent and fraction. BF16 has the same exponent range as FP32 but with fewer bits for the fraction1. Each number has one sign bit.</p>
<p>Although having similar theoretical performance benefits, BF16 and FP16 can have different speeds in practice. It’s recommended to try both formats and use the one with best speed while maintaining the desired numeric behavior.</p>
<p>Out-of-the-box mixed precision training with either float16 or bfloat16 is effective at speeding up the convergence of many deep learning models, but some models may require more careful numerical accuracy management. Here are Best practice
Figure out by experimentation if your network is sensitive to range and/or precision of a format. For example fine-tuning bfloat16-pretrained models in float16 can easily run into range issues in float16 because of the potentially large range from training in bfloat16, so users should stick with bfloat16 fine-tuning if the model was trained in bfloat16.</p>
<p>\end{remark}</p>
</section>
</section>
<section id="distributed-parallel-training">
<h2><span class="section-number">14.3. </span>Distributed Parallel Training<a class="headerlink" href="#distributed-parallel-training" title="Link to this heading">#</a></h2>
<section id="overview-of-parallel-training-techniques">
<h3><span class="section-number">14.3.1. </span>Overview of parallel training techniques<a class="headerlink" href="#overview-of-parallel-training-techniques" title="Link to this heading">#</a></h3>
</section>
<section id="model-parallelism-tensor-parallelism">
<span id="chapter-training-sec-distributed-parallel-training-model-parallelism"></span><h3><span class="section-number">14.3.2. </span>Model parallelism (tensor parallelism)<a class="headerlink" href="#model-parallelism-tensor-parallelism" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="zero-via-deepspeed">
<h2><span class="section-number">14.4. </span>ZeRO Via DeepSpeed<a class="headerlink" href="#zero-via-deepspeed" title="Link to this heading">#</a></h2>
<p>Data parallelism is the most widely used technique because it is simple and easy to implement. However, tt is typically challenging to apply the vanilla flavor data parallelism since it requires each GPU to store the parameters of the whole model. As a result, the size of GPU memory becomes the ceiling of the model scale we can train.</p>
<p>Model parallelism (e.g, Megatron) [<a class="reference internal" href="#chapter-training-sec-distributed-parallel-training-model-parallelism"><span class="std std-ref">Model parallelism (tensor parallelism)</span></a>], desipte its success in T5 (11B) and Megatron-LM (8.3B) is hard to scale beyond model sizes that cannot fit into a single GPU node. This is because model parallelism typically partitions the model weights or layers across GPU devices, incurring a significant communication between devices.</p>
<p>ZeRO (Zero Redundancy Optimizer) <span id="id3">[<a class="reference internal" href="#id1509" title="Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. 2020. URL: https://arxiv.org/abs/1910.02054, arXiv:1910.02054.">RRRH20</a>]</span> adopt the data parallism paradigm, and optimize memory efficieny and commnication efficiency.</p>
<section id="gpu-memory-allocation">
<h3><span class="section-number">14.4.1. </span>GPU Memory Allocation<a class="headerlink" href="#gpu-memory-allocation" title="Link to this heading">#</a></h3>
<p>GPU memory is mainly allocated into two parts: <strong>model states</strong> and <strong>residual states</strong>.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-zero-gpu-memory-allocation">
<a class="reference internal image-reference" href="../../_images/gpu_memory_allocation.png"><img alt="../../_images/gpu_memory_allocation.png" src="../../_images/gpu_memory_allocation.png" style="width: 697.2px; height: 278.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.2 </span><span class="caption-text">Model training step with mixed precision.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-zero-gpu-memory-allocation" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Model states refer to the content that is closely related to the model itself and must be stored. Specifically, they include:
\begin{itemize}
\item Optimizer states: momentum and variance in the Adam optimization algorithm.
\item Gradients: model gradients
\item Parameters: model parameters
\end{itemize}</p>
<p>Residual States refer to the content that is not necessary for the model, but is generated during the training process. Specifically, they include:
\begin{itemize}
\item Activation: activation values. We have discussed this in detail in pipeline parallelism. It is used when calculating gradients using the chain rule in the backward process. It can speed up gradient calculation, but it is not necessary to store because it can be calculated by redoing the Forward process.
\item Temporary buffers: temporary storage. For example, storage generated when aggregating gradients sent to a GPU for summation.
\item Unusable fragment memory: fragmented storage space. Although the total storage space is sufficient, if contiguous storage space cannot be obtained, related requests will also fail. Memory defragmentation can solve this type of space waste.
\end{itemize}</p>
</section>
<section id="zero-stage-one">
<h3><span class="section-number">14.4.2. </span>ZeRO-Stage-One<a class="headerlink" href="#zero-stage-one" title="Link to this heading">#</a></h3>
<p>Here’s the English translation of the provided text:</p>
<p>(1) <span class="math notranslate nohighlight">\(P_{os}\)</span> (Optimizer State Partitioning)
ZeRO-Stage-One reduces the required memory on each device by partitioning the optimizer state across <span class="math notranslate nohighlight">\(N_d\)</span> data-parallel processes. Each process only stores and updates its corresponding partition of the optimizer state, which is <span class="math notranslate nohighlight">\(\frac{1}{N_d}\)</span> of the total optimizer state. At the end of each training step, results from each process are collected to obtain the overall updated state parameters.</p>
<p>(2) The result after ZeRO-Stage1 memory optimization, mainly targeting the optimizer state :</p>
<div class="math notranslate nohighlight">
\[
(2+2) \Psi+\frac{K \times \Psi}{N_d}
\]</div>
<p>As can be seen, the optimizer state memory has a divisor of <span class="math notranslate nohighlight">\(N_d\)</span> compared to the original.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 14.1 </span></p>
<section class="example-content" id="proof-content">
<p>For a 7.5B parameter model, the standard case requires 120 GB of memory, but using <span class="math notranslate nohighlight">\(P_{os}\)</span> with <span class="math notranslate nohighlight">\(N_d=64\)</span> only requires 31.4 GB of memory.
When <span class="math notranslate nohighlight">\(N_d\)</span> is very large, the memory consumption approaches:</p>
<div class="math notranslate nohighlight">
\[
(2+2) \Psi+\frac{K \times \Psi}{N_d} \approx 4 \Psi
\]</div>
<p>The ratio compared to the original:</p>
<div class="math notranslate nohighlight">
\[
\frac{4}{4+K}
\]</div>
<p>When <span class="math notranslate nohighlight">\(K=12\)</span>, this becomes <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span>, meaning the memory usage is <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span> of the original.</p>
</section>
</div></section>
</section>
<section id="flash-attention">
<h2><span class="section-number">14.5. </span>Flash Attention<a class="headerlink" href="#flash-attention" title="Link to this heading">#</a></h2>
<section id="online-softmax-motivation">
<h3><span class="section-number">14.5.1. </span>Online Softmax Motivation<a class="headerlink" href="#online-softmax-motivation" title="Link to this heading">#</a></h3>
<p>A typical computation of Softmax
$<span class="math notranslate nohighlight">\(a_i = \mathbf{Softmax} (x_i | x_1,...,x_N) = \frac{\exp(x_i)}{\sum_{j}^N \exp(x_j)}\)</span>$
within the self-attention module involves the following three steps:</p>
<ol class="arabic simple">
<li><p>Motivated by preventing overflow, we first find the maximum value <span class="math notranslate nohighlight">\(m\)</span> of <span class="math notranslate nohighlight">\(\{x_1,...,x_N\}\)</span>
$<span class="math notranslate nohighlight">\(
\begin{aligned}
\text{for }  i &amp; \leftarrow 1, N \text{ do} \\
m &amp; = \max(m, x_i)
\end{aligned}
\)</span>$</p></li>
<li><p>Calculate the denominator of Softmax (with offset to <span class="math notranslate nohighlight">\(m\)</span>)
$<span class="math notranslate nohighlight">\(
\begin{aligned}
\text{for } i &amp; \leftarrow 1, N \text{ do} \\
d_i &amp; = d_{i-1} + \exp(x_i-m)
\end{aligned}
\)</span>$</p></li>
<li><p>Calculate the Softmax for each corresponding position</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{for } i &amp; \leftarrow 1, N \text{ do} \\
a_i &amp; = \frac{\exp(x_i-m)}{d_N}
\end{aligned}
\end{split}\]</div>
<p>Without any optimization, at least six communications with the GPU are required (three writes and three reads). If we apply some parallel partitioning to each step of the for loop, we would also need to add the communication costs of operations like reduce_sum and reduce_max. Is it possible to fuse certain operations to reduce communication?</p>
<p>Based on previous experience with layernorm parallelization, we need to look for an Online Algorithm.</p>
</section>
<section id="online-softmax-algorithm">
<h3><span class="section-number">14.5.2. </span>Online Softmax Algorithm<a class="headerlink" href="#online-softmax-algorithm" title="Link to this heading">#</a></h3>
<p>Nvidia :cite:<code class="docutils literal notranslate"><span class="pre">milakov2018onlinenormalizercalculationsoftmax</span></code></p>
<p>Since we’re looking for an Online algorithm, we need to find a recursive expression.
For the second step, <span class="math notranslate nohighlight">\(d_i=d_{i-1}+e^{x_i-m_N}\)</span>, we aim to remove its dependency on <span class="math notranslate nohighlight">\(m_N\)</span>.
Let <span class="math notranslate nohighlight">\(d_i^{\prime}=\sum_j^i e^{x_j-m_i}\)</span>, note that here we subtract the current maximum instead of the global maximum. This expression has the following property:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
d_i^{\prime} &amp; =\sum_j^i e^{x_j-m_i} \\
&amp; =\sum_j^{i-1} e^{x_j-m_i}+e^{x_i-m_i} \\
&amp; =\sum_j^{i-1} e^{x_j-m_{i-1}+m_{i-1}-m_i}+e^{x_i-m_i} \\
&amp; =\left(\sum_j^{i-1} e^{x_j-m_{i-1}}\right) e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
&amp; =d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}
\end{split}\]</div>
<p>We can see that the calculation of <span class="math notranslate nohighlight">\(d_i^{\prime}\)</span> depends on <span class="math notranslate nohighlight">\(d_{i-1}^{\prime}, m_i, m_{i-1}\)</span>, allowing us to merge the first two steps. The previous three steps can be reduced to two:</p>
<p>Find the maximum value <span class="math notranslate nohighlight">\(m\)</span> of <span class="math notranslate nohighlight">\(x\)</span>, calculate the denominator of softmax.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \text { for } i \leftarrow 1, N \text { do } \\
&amp; \quad m_i=\max \left(m_i, x_i\right) \\
&amp; \quad d_i^{\prime}=d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}
\end{split}\]</div>
<p>Calculate the softmax for each position</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text { for } i &amp; \leftarrow 1, N \text { do } \\
a_i &amp; =\frac{e^{x_i-m_N}}{d_N}
\end{aligned}
\end{split}\]</div>
<p>Can we further fuse operators? Not really, because the denominator in the second step depends on the calculation from the first step.
However, we can use GPU shared memory to store intermediate results and implement the above two steps in a single kernel. This way, we only need to communicate with global memory twice: once to write data and once to read results.</p>
</section>
<section id="from-online-softmax-to-flash-attention">
<h3><span class="section-number">14.5.3. </span>From Online Softmax To Flash Attention<a class="headerlink" href="#from-online-softmax-to-flash-attention" title="Link to this heading">#</a></h3>
<p>Using a method similar to Online Softmax, we can place all Attention operations into a single for loop (implementable in one Kernel).
Let’s first look at how it’s calculated:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text { for } i &amp; \leftarrow 1, N \text { do } \\
a_i &amp; =\frac{e^{x_i-m_N}}{d_N^{\prime}} \\
o_i &amp; =\sum_j^i a_j v_j=\sum_j^i \frac{e^{x_j-m_N}}{d_N^{\prime}} v_j
\end{aligned}
\end{split}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(o_i\)</span> contains <span class="math notranslate nohighlight">\(m_N\)</span> and <span class="math notranslate nohighlight">\(d_N^{\prime}\)</span>. We want to eliminate these dependencies. Similar to Online Softmax, we define:</p>
<div class="math notranslate nohighlight">
\[
o_i^{\prime}=\sum_j^i \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j
\]</div>
<p>Let’s find the recursive expression. The key is to identify <span class="math notranslate nohighlight">\(o_{i-1}^{\prime}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sigma_i^{\prime} &amp; =\sum_j^i \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j \\
&amp; =\sum_j^{i-1} \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
&amp; =\sum_j^{i-1} \frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}} \frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}} \frac{d_{i-1}^{\prime}}{d_i^{\prime}} v_j+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
&amp; =\left(\sum_j^{i-1} \frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}} v_j\right) \frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}} \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
&amp; =o_{i-1}^{\prime}\left(e^{m_{i-1}-m_i}\right) \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i
\end{aligned}
\end{split}\]</div>
<p>We can see that the calculation of <span class="math notranslate nohighlight">\(o_i\)</span> depends on <span class="math notranslate nohighlight">\(o_{i-1}, m_i, m_{i-1}, d_i^{\prime}, d_{i-1}^{\prime}\)</span>. This allows us to place the entire Attention calculation into a single for loop:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text { for } i &amp; \leftarrow 1, N  \text{ do } \\
m_i &amp; =\max \left(m_i, x_i\right) \\
d_i^{\prime} &amp; =d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
o_i^{\prime} &amp; =o_{i-1}^{\prime}\left(e^{m_{i-1}-m_i}\right) \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i
\end{aligned}
\end{split}\]</div>
<p>From the above, we can see that all operations within the for loop satisfy the associative property. This means the for loop can be block-processed, allowing for more efficient parallel computation on GPUs.
This is the mathematical principle behind Flash Attention’s parallel acceleration.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="appendix">
<h1><span class="section-number">15. </span>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h1>
<section id="floating-data-types">
<h2><span class="section-number">15.1. </span>Floating Data Types<a class="headerlink" href="#floating-data-types" title="Link to this heading">#</a></h2>
<p>Float32 (FP32) stands for the standardized IEEE 32-bit floating point representation. With this data type it is possible to represent a wide range of floating numbers. In FP32, 8 bits are reserved for the “exponent”, 23 bits for the “mantissa” and 1 bit for the sign of the number. In addition to that, most of the hardware supports FP32 operations and instructions.</p>
<p>In the float16 (FP16) data type, 5 bits are reserved for the exponent and 10 bits are reserved for the mantissa. This makes the representable range of FP16 numbers much lower than FP32. This exposes FP16 numbers to the risk of overflowing (trying to represent a number that is very large) and underflowing (representing a number that is very small).</p>
<p>For example, if you do 10k * 10k you end up with 100M which is not possible to represent in FP16, as the largest number possible is 64k. And thus you’d end up with NaN (Not a Number) result and if you have sequential computation like in neural networks, all the prior work is destroyed. Usually, loss scaling is used to overcome this issue, but it doesn’t always work well.</p>
<p>A new format, bfloat16 (BF16), was created to avoid these constraints. In BF16, 8 bits are reserved for the exponent (which is the same as in FP32) and 7 bits are reserved for the fraction.</p>
<p>This means that in BF16 we can retain the same dynamic range as FP32. But we lose 3 bits of precision with respect to FP16. Now there is absolutely no problem with huge numbers, but the precision is worse than FP16 here.</p>
<p>In the Ampere architecture, NVIDIA also introduced TensorFloat-32 (TF32) precision format, combining the dynamic range of BF16 and precision of FP16 to only use 19 bits. It’s currently only used internally during certain operations.</p>
<p>In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are referred to as half-precision (2 bytes). On top of that, the int8 (INT8) data type consists of an 8-bit representation that can store <span class="math notranslate nohighlight">\(2^8\)</span> different values (between [0, 255] or [-128, 127] for signed integers).</p>
<p>While, ideally the training and inference should be done in FP32, it is two times slower than FP16/BF16 and therefore a mixed precision approach is used where the weights are held in FP32 as a precise “main weights” reference, while computation in a forward and backward pass are done for FP16/BF16 to enhance training speed. The FP16/BF16 gradients are then used to update the FP32 main weights.</p>
<p>During training, the main weights are always stored in FP32, but in practice, the half-precision weights often provide similar quality during inference as their FP32 counterpart – a precise reference of the model is only needed when it receives multiple gradient updates. This means we can use the half-precision weights and use half the GPUs to accomplish the same outcome.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-different-precision-demo">
<a class="reference internal image-reference" href="../../_images/different_precision_demo.png"><img alt="../../_images/different_precision_demo.png" src="../../_images/different_precision_demo.png" style="width: 387.90000000000003px; height: 297.90000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.1 </span><span class="caption-text">Comparison of different float number types.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-different-precision-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="gpu-parallel-operations">
<h2><span class="section-number">15.2. </span>GPU Parallel Operations<a class="headerlink" href="#gpu-parallel-operations" title="Link to this heading">#</a></h2>
<p>References <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#:~:text=The%20ReduceScatter%20operation%20performs%20the%20same%20operation%20as,mapping%20since%20the%20ranks%20determine%20the%20data%20layout">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#:~:text=The ReduceScatter operation performs the same operation as,mapping since the ranks determine the data layout</a>.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-broadcast">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_broadcast.png"><img alt="../../_images/gpu_parallel_operation_broadcast.png" src="../../_images/gpu_parallel_operation_broadcast.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.2 </span><span class="caption-text">Broadcast operation: data in one device is sent to all other devices.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-broadcast" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-scatter">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_scatter.png"><img alt="../../_images/gpu_parallel_operation_scatter.png" src="../../_images/gpu_parallel_operation_scatter.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.3 </span><span class="caption-text">Scatter operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-scatter" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-gather">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_gather.png"><img alt="../../_images/gpu_parallel_operation_gather.png" src="../../_images/gpu_parallel_operation_gather.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.4 </span><span class="caption-text">Gather operation: every device broadcasts their data patition to a designated devices. Eventually, this desigated device has the complete data.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-gather" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-reduce">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_reduce.png"><img alt="../../_images/gpu_parallel_operation_reduce.png" src="../../_images/gpu_parallel_operation_reduce.png" style="width: 801.0px; height: 199.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.5 </span><span class="caption-text">Reduce operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-reduce" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allgather">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allgather.png"><img alt="../../_images/gpu_parallel_operation_allgather.png" src="../../_images/gpu_parallel_operation_allgather.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.6 </span><span class="caption-text">AllGather operation: every device broadcasts their chuck of data to all other devices. Eventually, every device has a complete data copy.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allgather" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allgather-ring">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allgather_ring.png"><img alt="../../_images/gpu_parallel_operation_allgather_ring.png" src="../../_images/gpu_parallel_operation_allgather_ring.png" style="width: 639.0px; height: 459.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.7 </span><span class="caption-text">Communication efficient implementation for AllGather via ring style. Every device sends its chuck of data to the next device in the ring.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allgather-ring" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-reducescatter">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_reducescatter.png"><img alt="../../_images/gpu_parallel_operation_reducescatter.png" src="../../_images/gpu_parallel_operation_reducescatter.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.8 </span><span class="caption-text">ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks across devices.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-reducescatter" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allreduce">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allreduce.png"><img alt="../../_images/gpu_parallel_operation_allreduce.png" src="../../_images/gpu_parallel_operation_allreduce.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 15.9 </span><span class="caption-text">AllReduce operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allreduce" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>A <strong>naive AllReduce</strong> implementation would be two steps:</p>
<ol class="arabic simple">
<li><p>Reduce: All devices first send their data to Rank0 device, and performing reduce operation on Rank0.</p></li>
<li><p>Broadcast: the reduce results are sent to all other devices.
This amounts to a total <span class="math notranslate nohighlight">\(2(N_d-1)\Phi\)</span> communication volume, in which the step 1 has <span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span> and step 2 has <span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span>. The naive implementation has the communication load imbalance issue as all data are sent into and sent out from Rank0 device.</p></li>
</ol>
<p>The <strong>RingAllReduce</strong> address the load imbalance issue by engaging all devices in data communication and reduction (i.e., more parallelism). The RingAllReduce is equivalent to first RingReduceScatter and then AllGather.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1517">
<caption><span class="caption-number">Table 15.1 </span><span class="caption-text">Communication volumne summary for different operations. Let <span class="math notranslate nohighlight">\(\Phi\)</span> be the total data size in one device and <span class="math notranslate nohighlight">\(N\)</span> be the total number of devices.</span><a class="headerlink" href="#id1517" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Type</p></th>
<th class="head text-left"><p>Storage Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Broadcast</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((N_d-1) \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Scatter</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Reduce</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Gather</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>AllGather</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi \times N_d = (N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ReduceScatter</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi \times N_d = (N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>AllReduce(Ring)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2(N_d-1) \Phi\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="bibliography">
<h2><span class="section-number">15.3. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id4">
<div role="list" class="citation-list">
<div class="citation" id="id1492" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MNA+17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>)</span>
<p>Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. <em>arXiv preprint arXiv:1710.03740</em>, 2017.</p>
</div>
<div class="citation" id="id1509" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">RRRH20</a><span class="fn-bracket">]</span></span>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. 2020. URL: <a class="reference external" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a>, <a class="reference external" href="https://arxiv.org/abs/1910.02054">arXiv:1910.02054</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="alignment.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>LLM Alignement and Preference learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_inference/inference_fundamentals.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">16. </span>LLM Decoding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">14. LLM Training Acceleration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-requirement-for-training-llm">14.1. The Memory Requirement For Training LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-states">14.1.1. Model States</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">14.1.2. Activations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-part">14.1.2.1. MLP Part</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-part">14.1.2.2. Self-attention Part</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-checkpointing-techniques">14.1.3. Activation Checkpointing Techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-training">14.2. Mixed Precision Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">14.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">14.2.2. Training Process</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-parallel-training">14.3. Distributed Parallel Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-parallel-training-techniques">14.3.1. Overview of parallel training techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelism-tensor-parallelism">14.3.2. Model parallelism (tensor parallelism)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-via-deepspeed">14.4. ZeRO Via DeepSpeed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-allocation">14.4.1. GPU Memory Allocation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-stage-one">14.4.2. ZeRO-Stage-One</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flash-attention">14.5. Flash Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-softmax-motivation">14.5.1. Online Softmax Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-softmax-algorithm">14.5.2. Online Softmax Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-online-softmax-to-flash-attention">14.5.3. From Online Softmax To Flash Attention</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">15. Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-data-types">15.1. Floating Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-parallel-operations">15.2. GPU Parallel Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">15.3. Bibliography</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>