{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: Minimal LLama\n",
    "\n",
    "Here we present a simplified llama implementation based [Huggingface implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L731) to illustrate different components on the Llama decoder model.\n",
    "\n",
    "The key components are\n",
    "* RMS Norm\n",
    "* Rotary Position Embedding\n",
    "* Grouped Query Attention\n",
    "* Feedfoward network (FFN)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "RMSNorm  is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula\n",
    "\n",
    "$$\n",
    "\\operatorname{RMSNorm}(x)=\\frac{x}{\\sqrt{\\frac{1}{H} \\sum_{i=1}^H x_i^2}} \\cdot \\gamma\n",
    "$$(chapter_LLM_arch_RMS_nomalization_formula)\n",
    "\n",
    "where $\\gamma$ is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # float32 is needed for numeric stability. float16 is not enough.\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # The variance of the hidden_states is computed along the last dimension using the pow(2).\n",
    "        # mean(-1, keepdim=True) operations, which square the values, compute the mean, and \n",
    "        # retain the dimensions for broadcasting.\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        \n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.gamma * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "\n",
    "Rotary position embedding consists of pre-computing cosine, sine at different frequences (from 0 to 1/(10000)) and different position ids (from 0 to max_seq_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        self.original_max_seq_len = max_position_embeddings\n",
    "\n",
    "\n",
    "        #inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        \n",
    "        # inv freq is a tensor of shape (dim // 2)\n",
    "        # (0, 1/10000^(2/dim),..., 1/10000^((dim-2)/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        # Use None to add two new dimensions to the inv_freq\n",
    "        # use expand to repeat the inv_freq along the batch dimension\n",
    "        # inv_freq_expanded has shape (batch_size, dim // 2, 1), dim // 2 is the number of frequencies\n",
    "        # position_ids_expanded has shape (batch_size, 1, seq_len)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        # inv_freq_expanded.float() @ position_ids_expanded.float() gives shape (batch_size, dim // 2, seq_len)\n",
    "        # after transpose, we get (batch_size, seq_len, dim // 2)\n",
    "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "        # emb has shape (batch_size, seq_len, dim), the concat is on the frequency dimension\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # x1 is the first half of the hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # x2 is the second half of the hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# q (`torch.Tensor`): The query tensor, which has shape [batch_size, heads, seq_len, head_dim].\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    # add a dimension to the cos and sin tensors to account for the number of heads\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    # Here has a different order in the frequency dimension, as described in the paper https://arxiv.org/pdf/2104.09864 page 7\n",
    "    # in the paper, the order is \n",
    "    # [cos m theta 1, cos m theta 1, ..., cos m theta (d//2), cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 1, ..., sin m theta (d//2), sin m theta (d//2)]\n",
    "    # here the order is\n",
    "    # [cos m theta 1, cos m theta 2, ...cos m theta (d//2), cos m theta 1, cos m theta 2, ...cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 2, ...sin m theta (d//2), sin m theta 1, sin m theta 2, ...sin m theta (d//2)]\n",
    "    # that is, the frequency order is permuted\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "Attention layer implements the grouped query attention; Note that the rotary position encoding are implemented by rotating the query encoding and key encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for Group query attention\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, seqlen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, seqlen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, seqlen, head_dim)\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # Here supports GQA, which specifies the number of key value heads << num_heads\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.o_bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  \n",
    "    ):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "\n",
    "        # projetion of the hidden states into query, key and value\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Get the rotary embeddings cosines and sines functions\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # apply the rotary embeddings to the query and key states\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # Copy kv for matching the number of heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        # applied scaled dot product attention\n",
    "        # attn_weights has shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        # attn_output has shape (batch_size,  seq_len, num_heads, head_dim) after transpose\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # attn_output output has shape (batch_size, seq_len, num_heads * head_dim) after reshape\n",
    "        # which is equivalent to concatenating the heads\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        # apply the output projection\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN Layer\n",
    "\n",
    "Llama uses Swish function in the GLU, we can obtain the following variations:\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}_{SwiGLU} = (\\text{Swish}_1(\\underbrace{xW_1}_{\\text{Gate Projection}})\\otimes \\underbrace{xV}_{\\text{Up Projection}} ) \\underbrace{W_2}_{\\text{Down Projection}}\n",
    "$$\n",
    "\n",
    "with $\\operatorname{Swish}_1(x)=x \\cdot \\sigma(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        # silu is the same as swish\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama Decoder Layer\n",
    "\n",
    "Each decoder layer has\n",
    "* Two Pre-RMSNorm layers, one before the self-attention sublayer and one before the FFN layer\n",
    "* GQA attention layer\n",
    "* FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "        # FFN layer\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        # pre layer norm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        # pre layer norm before FFN layer\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Decoder layers\n",
    "\n",
    "In the stacked decoder layer, \n",
    "* There are L decoder layers\n",
    "* Rotary embeddings (i.e., elements in the rotation matrices) are shared across layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        # apply to last layer hidden state\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # rotary embedding matrices are shared across the decoder layers\n",
    "        self.rotary_emb = LlamaRotaryEmbedding( dim=config.hidden_size // config.num_attention_heads,\n",
    "                                                max_position_embeddings=config.max_position_embeddings,\n",
    "                                                base=config.rope_theta,)\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ):\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.shape[1], dtype=torch.int64)\n",
    "            position_ids = position_ids.expand(input_ids.shape[0], -1)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder for language modeling\n",
    "\n",
    "Decoder with language modeling is the previous stacked decoder layer plus a linear layer as language prediction head. The langauge prediciton head linearly transforms the hidden state into the logits distributed over the vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    #_tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "    ):\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  from omegaconf import OmegaConf\n",
    "  model_config = {\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"bos_token_id\": 151643,\n",
    "    \"eos_token_id\": 151643,\n",
    "    \"pad_token_id\": 151643,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 896,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4864,\n",
    "    \"max_position_embeddings\": 32768,\n",
    "    \"max_window_layers\": 24,\n",
    "    \"model_type\": \"qwen2\",\n",
    "    \"num_attention_heads\": 14,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"rope_theta\": 1000000.0,\n",
    "    \"tie_word_embeddings\": True,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"transformers_version\": \"4.47.1\",\n",
    "    \"use_cache\": True,\n",
    "    \"use_mrope\": False,\n",
    "    \"vocab_size\": 151936,\n",
    "    \"qkv_bias\": True,\n",
    "    \"o_bias\": False,\n",
    "    \"mlp_bias\": False\n",
    "  }\n",
    "\n",
    "  model_config = OmegaConf.create(model_config)\n",
    "  custom_model = LlamaForCausalLM(model_config)\n",
    "  \n",
    "  # load model weight from Huggingface\n",
    "  import transformers\n",
    "  from transformers import AutoModelForCausalLM\n",
    "  \n",
    "  model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  custom_model.load_state_dict(model.state_dict(), strict=False)\n",
    "  \n",
    "  # test input\n",
    "  input_ids = torch.LongTensor([[1, 2, 3]])\n",
    "  custom_model(input_ids)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
