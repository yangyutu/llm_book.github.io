{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import os, sys\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Function calls itself,  \\nLayers of truth intertwine—  \\nEndless return flows.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_llm_with_full_text(gpt_model, input):\n",
    "    # Join all lines to form a single string\n",
    "    try:\n",
    "      response = client.chat.completions.create(\n",
    "         model=gpt_model,\n",
    "         messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "            {\"role\": \"user\", \"content\": input}\n",
    "         ],\n",
    "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
    "        )\n",
    "      text = response.choices[0].message.content.strip()\n",
    "      wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "      wrapped_text = wrapper.fill(text=text)\n",
    "      return wrapped_text\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for \"Retrieval-Augmented Generation.\" It is a framework used in\n",
      "natural language processing that combines the strengths of retrieval-based and\n",
      "generative models to improve the quality and relevance of generated text.  In a\n",
      "RAG system, the process typically involves two main components:  1.\n",
      "**Retrieval**: The system first retrieves relevant documents or pieces of\n",
      "information from a large corpus based on a given query or input. This step\n",
      "ensures that the model has access to up-to-date and contextually relevant\n",
      "information.  2. **Generation**: After retrieving the relevant information, a\n",
      "generative model (often based on transformer architectures like GPT) uses this\n",
      "information to produce coherent and contextually appropriate responses or text.\n",
      "The generative model can incorporate the retrieved content to enhance its\n",
      "output, making it more informative and accurate.  RAG is particularly useful in\n",
      "applications like question answering, dialogue systems, and content creation,\n",
      "where having access to external knowledge can significantly improve the quality\n",
      "of the generated responses. By leveraging both retrieval and generation, RAG\n",
      "systems can provide more accurate and contextually relevant answers than\n",
      "traditional generative models alone.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "prompt = \"define what is rag\"\n",
    "text = call_llm_with_full_text(gpt_model=gpt_model, input=prompt)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the movie review can be classified as **neutral**.   While the\n",
      "reviewer mentions positive aspects such as \"stunning visuals\" and \"action\n",
      "sequences,\" they also highlight significant drawbacks like a \"thin plot\" and\n",
      "\"characters lacked depth.\" The conclusion that it was an \"entertaining but\n",
      "forgettable experience\" suggests a mixed sentiment, leading to a neutral overall\n",
      "impression.\n"
     ]
    }
   ],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "prompt = \"\"\"Classify the sentiment of the following movie review as positive, negative, or neutral:\n",
    "        **Review**: 'The visuals were stunning and the action sequences kept me on the edge of my seat. \n",
    "        However, the plot was thin and the characters lacked depth. Overall, it was an entertaining but forgettable experience.\"\"\"\n",
    "text = call_llm_with_full_text(gpt_model=gpt_model, input=prompt)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To extract the title, h1, and body text from the provided HTML document, we can\n",
      "identify the relevant tags and their contents. Here’s the extraction:  -\n",
      "**Title**: A simple page - **H1**: Hello World - **Body Text**: This is some\n",
      "text in a simple html page.  If you need further assistance or additional\n",
      "information, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "prompt = \"\"\"Extract the title, h1, and body text from the following HTML\n",
    "\n",
    "Document:\n",
    "\n",
    "<head><title>A simple page</title></head><body><h1>Hello World</h1><p>This is some text in a simple html page.</p></ body>.\"\"\"\n",
    "text = call_llm_with_full_text(gpt_model=gpt_model, input=prompt)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "print(openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "RMSNorm  is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula\n",
    "\n",
    "$$\n",
    "\\operatorname{RMSNorm}(x)=\\frac{x}{\\sqrt{\\frac{1}{H} \\sum_{i=1}^H x_i^2}} \\cdot \\gamma\n",
    "$$(chapter_LLM_arch_RMS_nomalization_formula)\n",
    "\n",
    "where $\\gamma$ is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # float32 is needed for numeric stability. float16 is not enough.\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # The variance of the hidden_states is computed along the last dimension using the pow(2).\n",
    "        # mean(-1, keepdim=True) operations, which square the values, compute the mean, and \n",
    "        # retain the dimensions for broadcasting.\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        \n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.gamma * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "\n",
    "Rotary position embedding consists of pre-computing cosine, sine at different frequences (from 0 to 1/(10000)) and different position ids (from 0 to max_seq_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=None,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        self.original_max_seq_len = max_position_embeddings\n",
    "\n",
    "\n",
    "        #inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        \n",
    "        # inv freq is a tensor of shape (dim // 2)\n",
    "        # (0, 1/10000^(2/dim),..., 1/10000^((dim-2)/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        # Use None to add two new dimensions to the inv_freq\n",
    "        # use expand to repeat the inv_freq along the batch dimension\n",
    "        # inv_freq_expanded has shape (batch_size, dim // 2, 1), dim // 2 is the number of frequencies\n",
    "        # position_ids_expanded has shape (batch_size, 1, seq_len)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        # inv_freq_expanded.float() @ position_ids_expanded.float() gives shape (batch_size, dim // 2, seq_len)\n",
    "        # after transpose, we get (batch_size, seq_len, dim // 2)\n",
    "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "        # emb has shape (batch_size, seq_len, dim), the concat is on the frequency dimension\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # x1 is the first half of the hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # x2 is the second half of the hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# q (`torch.Tensor`): The query tensor, which has shape [batch_size, heads, seq_len, head_dim].\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    # add a dimension to the cos and sin tensors to account for the number of heads\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    # Here has a different order in the frequency dimension, as described in the paper https://arxiv.org/pdf/2104.09864 page 7\n",
    "    # in the paper, the order is \n",
    "    # [cos m theta 1, cos m theta 1, ..., cos m theta (d//2), cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 1, ..., sin m theta (d//2), sin m theta (d//2)]\n",
    "    # here the order is\n",
    "    # [cos m theta 1, cos m theta 2, ...cos m theta (d//2), cos m theta 1, cos m theta 2, ...cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 2, ...sin m theta (d//2), sin m theta 1, sin m theta 2, ...sin m theta (d//2)]\n",
    "    # that is, the frequency order is permuted\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "Attention layer implements the grouped query attention; Note that the rotary position encoding are implemented by rotating the query encoding and key encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for Group query attention\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, seqlen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, seqlen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, seqlen, head_dim)\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # Here supports GQA, which specifies the number of key value heads << num_heads\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  \n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "\n",
    "        # projetion of the hidden states into query, key and value\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Get the rotary embeddings cosines and sines functions\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # apply the rotary embeddings to the query and key states\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # Copy kv for matching the number of heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        # applied scaled dot product attention\n",
    "        # attn_weights has shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            # he slicing operation [attention_mask[:, :, :, : key_states.shape[-2]]](http://vscodecontentref/5) ensures that the mask is aligned\n",
    "            # with the dimensions of the key_states tensor. Specifically, it slices the attention_mask along the \n",
    "            # last dimension to match the second-to-last dimension of the key_states tensor. \n",
    "            # This alignment is crucial for correctly applying the mask to the attention weights.\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "\n",
    "        # attn_output has shape (batch_size,  seq_len, num_heads, head_dim) after transpose\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # attn_output output has shape (batch_size, seq_len, num_heads * head_dim) after reshape\n",
    "        # which is equivalent to concatenating the heads\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        # apply the output projection\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN Layer\n",
    "\n",
    "Llama uses Swish function in the GLU, we can obtain the following variations:\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}_{SwiGLU} = (\\text{Swish}_1(\\underbrace{xW_1}_{\\text{Gate Projection}})\\otimes \\underbrace{xV}_{\\text{Up Projection}} ) \\underbrace{W_2}_{\\text{Down Projection}}\n",
    "$$\n",
    "\n",
    "with $\\operatorname{Swish}_1(x)=x \\cdot \\sigma(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        # silu is the same as swish\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama Decoder Layer\n",
    "\n",
    "Each decoder layer has\n",
    "* Two Pre-RMSNorm layers, one before the self-attention sublayer and one before the FFN layer\n",
    "* GQA attention layer\n",
    "* FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "        # FFN layer\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor] = None, \n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*):\n",
    "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
    "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "            kwargs (`dict`, *optional*):\n",
    "                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n",
    "                into the model\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        # pre layer norm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        # pre layer norm before FFN layer\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Decoder layers\n",
    "\n",
    "In the stacked decoder layer, \n",
    "* There are L decoder layers\n",
    "* Rotary embeddings (i.e., elements in the rotation matrices) are shared across layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        # apply to last layer hidden state\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # rotary embedding matrices are shared across the decoder layers\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position\n",
    "        )\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "    ):\n",
    "\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        target_length = sequence_length + 1\n",
    "\n",
    "        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n",
    "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=sequence_length,\n",
    "            target_length=target_length,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "        )\n",
    "\n",
    "        return causal_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype,\n",
    "        device: torch.device,\n",
    "        cache_position: torch.Tensor,\n",
    "        batch_size: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n",
    "        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n",
    "\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n",
    "                `(batch_size, 1, query_length, key_value_length)`.\n",
    "            sequence_length (`int`):\n",
    "                The sequence length being processed.\n",
    "            target_length (`int`):\n",
    "                The target length: when generating with static cache, the mask should be as long as the static cache,\n",
    "                to account for the 0 padding, the part of the cache that is not filled yet.\n",
    "            dtype (`torch.dtype`):\n",
    "                The dtype to use for the 4D attention mask.\n",
    "            device (`torch.device`):\n",
    "                The device to plcae the 4D attention mask on.\n",
    "            cache_position (`torch.Tensor`):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence.\n",
    "            batch_size (`torch.Tensor`):\n",
    "                Batch size.\n",
    "        \"\"\"\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            )\n",
    "        # The torch.triu function returns the upper triangular part of the matrix, setting all elements below the specified diagonal to zero. \n",
    "        # By setting the diagonal parameter to 1, the function ensures that the diagonal and all elements above it are retained, \n",
    "        # effectively creating a mask that prevents each position from attending to future positions.\n",
    "            if sequence_length != 1:\n",
    "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                    padding_mask, min_dtype\n",
    "                )\n",
    "\n",
    "        return causal_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder for language modeling\n",
    "\n",
    "Decoder with language modeling is the previous stacked decoder layer plus a linear layer as language prediction head. The langauge prediciton head linearly transforms the hidden state into the logits distributed over the vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        ```\"\"\"\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
