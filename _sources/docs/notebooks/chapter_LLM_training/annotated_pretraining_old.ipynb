{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: LLM Pretraining\n",
    "\n",
    "Here we directly leverage the decoder architecture we made from previous sections. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os, urllib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from omegaconf import OmegaConf\n",
    "from llm_lab.model.vanilla_decoder import VanillaDecoderModel\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTPretrainDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenizer the entire text\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        # use a sliding window approach to chunk the input text corpus\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # shift by one\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_data_loader(text, batch_size=4, max_length=256, \n",
    "                       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    \n",
    "    dataset = GPTPretrainDataset(text=text,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_length=max_length,\n",
    "                                 stride=stride)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=drop_last,\n",
    "                             num_workers=num_workers)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_data(file_path, url):\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "            \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  423,  4750,   326,  ...,   262,  8216,    13],\n",
      "        [   11,   508,   550,  ...,   526,   198,   198],\n",
      "        [  271, 10899,   550,  ..., 29543,  2745,    11],\n",
      "        [   26,   475,   314,  ...,   287,   683,   438]]), tensor([[ 4750,   326,  9074,  ...,  8216,    13,   314],\n",
      "        [  508,   550, 18459,  ...,   198,   198,  3347],\n",
      "        [10899,   550,   366,  ...,  2745,    11,   314],\n",
      "        [  475,   314,  2936,  ...,   683,   438,   273]])]\n"
     ]
    }
   ],
   "source": [
    "def test_data_component():\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "    text_data = read_text_data(file_path, url)\n",
    "    train_loader = create_data_loader(text=text_data)\n",
    "    for batch in train_loader:\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "test_data_component()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_and_chunk_text(examples):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    flat_targets = target_batch.flatten() \n",
    "    flat_logits = logits.flatten(0, 1)# flatten the first two dimensions \n",
    "    loss = F.cross_entropy(flat_logits, flat_targets)\n",
    "    return loss\n",
    "\n",
    "def train_model_epoch(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                optimizer,\n",
    "                device,\n",
    "                num_epochs):\n",
    "    \n",
    "    train_losses, val_losses, track_token_seen = [],[],[]\n",
    "    tokens_seen = 0\n",
    "    global_steps = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_steps += 1\n",
    "            train_losses.append(loss.detach().item())\n",
    "        \n",
    "    return train_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model_config, train_settings):\n",
    "    \n",
    "    torch.manual_seed(train_settings.seed)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    text_data = read_text_data(train_settings.file_path, train_settings.url)\n",
    "            \n",
    "    model = LLM(config=model_config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=train_settings.learning_rate,\n",
    "                                  weight_decay=train_settings.weight_decay)\n",
    "    \n",
    "    # set up dataloader\n",
    "    train_ratio = 0.90\n",
    "    split_position = int(len(text_data) * train_ratio)\n",
    "    \n",
    "    train_data = text_data[:split_position]\n",
    "    val_data = text_data[split_position:]\n",
    "    \n",
    "    train_loader = create_data_loader(text=train_data,\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      max_length=model_config.context_length,\n",
    "                                      stride=train_settings.stride,\n",
    "                                      drop_last=True,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = create_data_loader(text=val_data,\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      max_length=model_config.context_length,\n",
    "                                      stride=train_settings.stride,\n",
    "                                      drop_last=True,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0\n",
    "    )\n",
    "    \n",
    "    train_losses, model = train_model_epoch(model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=train_settings.num_epochs,\n",
    "                device=device)\n",
    "    \n",
    "    return train_losses, model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.889654159545898, 10.55605697631836, 10.390718460083008, 9.955503463745117, 9.866605758666992, 9.824695587158203, 9.696834564208984, 9.723470687866211, 9.311979293823242, 9.14416790008545, 8.976319313049316, 8.796220779418945, 8.653111457824707, 8.667469024658203, 8.531828880310059, 8.036416053771973, 7.521772861480713, 7.099956512451172, 6.590849876403809, 5.911465644836426, 5.8712005615234375, 5.545934677124023, 5.661034107208252, 5.0880608558654785, 5.054011344909668, 4.862744331359863, 4.2214741706848145, 4.4426798820495605, 4.785312175750732, 3.9887170791625977, 3.7415623664855957, 4.1082444190979, 4.18638801574707, 4.1566009521484375, 2.5507609844207764, 2.6057915687561035, 3.7073099613189697, 2.823371648788452, 2.622760057449341, 3.088858127593994, 1.9870753288269043, 2.2322254180908203, 3.128173351287842, 1.7956585884094238, 3.165675401687622, 1.8216800689697266, 2.4981274604797363, 2.861833095550537, 2.2204294204711914, 2.6053457260131836, 1.7304390668869019, 0.8385039567947388, 1.3179033994674683, 0.849096417427063, 1.3006774187088013, 0.968055009841919, 1.5471502542495728, 1.2076023817062378, 1.3883811235427856, 1.1976746320724487, 1.0030306577682495, 0.9050904512405396, 0.8037070035934448, 1.3230046033859253, 0.9978317618370056, 1.0717201232910156, 1.0735877752304077, 1.2748712301254272, 0.5854703783988953, 0.5534721612930298, 0.6194265484809875, 0.41815683245658875, 0.30828776955604553, 0.5358526110649109, 0.23895448446273804, 0.8637996912002563, 0.46475347876548767, 0.5977479219436646, 0.688923716545105, 0.5963003635406494, 0.6153844594955444, 0.5597606897354126, 0.5610265731811523, 0.5555013418197632, 0.7592775821685791, 0.2165975123643875, 0.2848913073539734, 0.502203643321991, 0.256918340921402, 0.4975152611732483, 0.1541220098733902, 0.37131962180137634, 0.4221187233924866, 0.2747355103492737, 0.3208323121070862, 0.40156105160713196, 0.3431472182273865, 0.3511956036090851, 0.30832594633102417, 0.46910417079925537, 0.3208274245262146, 0.24321489036083221, 0.234857439994812, 0.16864658892154694, 0.21279782056808472, 0.11771243065595627, 0.08758139610290527, 0.1501040756702423, 0.10985501110553741, 0.16980576515197754, 0.22996433079242706, 0.33185669779777527, 0.1144712045788765, 0.1931455284357071, 0.12477972358465195, 0.06445721536874771, 0.1658274382352829, 0.15538781881332397, 0.2233043611049652, 0.05726563185453415, 0.05852453410625458, 0.08412479609251022, 0.10208554565906525, 0.054613832384347916, 0.08875849843025208, 0.05197812244296074, 0.04636172950267792, 0.05462772026658058, 0.08739487081766129, 0.11179701238870621, 0.06971258670091629, 0.0769914910197258, 0.09413014352321625, 0.19633081555366516, 0.13653340935707092, 0.13567930459976196, 0.06037088483572006, 0.033027324825525284, 0.048016272485256195, 0.03828873857855797, 0.06446892023086548, 0.07514037191867828, 0.08328281342983246, 0.05378653109073639, 0.08564770221710205, 0.02886047028005123, 0.044668205082416534, 0.09021760523319244, 0.0558529794216156, 0.041125692427158356, 0.07640326768159866, 0.03789590671658516, 0.023197127506136894, 0.01567663997411728, 0.0588991753757, 0.026836492121219635, 0.02880910411477089, 0.007800598628818989, 0.02600269578397274, 0.059050846844911575, 0.04371192306280136, 0.02440521866083145, 0.03444734588265419, 0.014517992734909058, 0.04039938002824783, 0.04059940576553345, 0.013116082176566124, 0.036287613213062286, 0.03766665980219841, 0.026512518525123596]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_config = {\n",
    "        \"vocab_size\": 50257,    # Vocabulary size\n",
    "        \"context_length\": 256,  # Shortened context length (orig: 1024)\n",
    "        \"stride\": 128,\n",
    "        \"d_model\": 768,         # model dimension\n",
    "        \"num_heads\": 12,          # Number of attention heads\n",
    "        \"num_layers\": 12,         # Number of layers\n",
    "        \"dropout\": 0.1,       # Dropout rate\n",
    "        \"qkv_bias\": False       # Query-key-value bias\n",
    "    }\n",
    "    \n",
    "    model_config = OmegaConf.create(model_config)\n",
    "    train_settings = {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 2,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"stride\": 128,\n",
    "        \"seed\": 1,\n",
    "        \"file_path\":\"./pretraining/the-verdict.txt\",\n",
    "        \"url\":\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    }\n",
    "    \n",
    "    train_settings = OmegaConf.create(train_settings)\n",
    "    \n",
    "    # train model\n",
    "    train_losses, model = train_main(model_config=model_config,\n",
    "                       train_settings=train_settings)\n",
    "    \n",
    "    print(train_losses)\n",
    "    # save model\n",
    "    #torch.save(model.state_dict(), \"model.pth\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
