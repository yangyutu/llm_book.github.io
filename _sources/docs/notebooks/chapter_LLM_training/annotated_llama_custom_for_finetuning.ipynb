{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: Annotated Finetuning\n",
    "\n",
    "Here we present a simplified llama implementation based [Huggingface implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L731) to illustrate different components on the Llama decoder model.\n",
    "\n",
    "The key components are\n",
    "* RMS Norm\n",
    "* Rotary Position Embedding\n",
    "* Grouped Query Attention\n",
    "* Feedfoward network (FFN)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "RMSNorm  is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula\n",
    "\n",
    "$$\n",
    "\\operatorname{RMSNorm}(x)=\\frac{x}{\\sqrt{\\frac{1}{H} \\sum_{i=1}^H x_i^2}} \\cdot \\gamma\n",
    "$$(chapter_LLM_arch_RMS_nomalization_formula)\n",
    "\n",
    "where $\\gamma$ is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # float32 is needed for numeric stability. float16 is not enough.\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # The variance of the hidden_states is computed along the last dimension using the pow(2).\n",
    "        # mean(-1, keepdim=True) operations, which square the values, compute the mean, and \n",
    "        # retain the dimensions for broadcasting.\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        \n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.gamma * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "\n",
    "Rotary position embedding consists of pre-computing cosine, sine at different frequences (from 0 to 1/(10000)) and different position ids (from 0 to max_seq_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        self.original_max_seq_len = max_position_embeddings\n",
    "\n",
    "\n",
    "        #inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        \n",
    "        # inv freq is a tensor of shape (dim // 2)\n",
    "        # (0, 1/10000^(2/dim),..., 1/10000^((dim-2)/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        # Use None to add two new dimensions to the inv_freq\n",
    "        # use expand to repeat the inv_freq along the batch dimension\n",
    "        # inv_freq_expanded has shape (batch_size, dim // 2, 1), dim // 2 is the number of frequencies\n",
    "        # position_ids_expanded has shape (batch_size, 1, seq_len)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        # inv_freq_expanded.float() @ position_ids_expanded.float() gives shape (batch_size, dim // 2, seq_len)\n",
    "        # after transpose, we get (batch_size, seq_len, dim // 2)\n",
    "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "        # emb has shape (batch_size, seq_len, dim), the concat is on the frequency dimension\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # x1 is the first half of the hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # x2 is the second half of the hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# q (`torch.Tensor`): The query tensor, which has shape [batch_size, heads, seq_len, head_dim].\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    # add a dimension to the cos and sin tensors to account for the number of heads\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    # Here has a different order in the frequency dimension, as described in the paper https://arxiv.org/pdf/2104.09864 page 7\n",
    "    # in the paper, the order is \n",
    "    # [cos m theta 1, cos m theta 1, ..., cos m theta (d//2), cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 1, ..., sin m theta (d//2), sin m theta (d//2)]\n",
    "    # here the order is\n",
    "    # [cos m theta 1, cos m theta 2, ...cos m theta (d//2), cos m theta 1, cos m theta 2, ...cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 2, ...sin m theta (d//2), sin m theta 1, sin m theta 2, ...sin m theta (d//2)]\n",
    "    # that is, the frequency order is permuted\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "Attention layer implements the grouped query attention; Note that the rotary position encoding are implemented by rotating the query encoding and key encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for Group query attention\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, seqlen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, seqlen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, seqlen, head_dim)\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # Here supports GQA, which specifies the number of key value heads << num_heads\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.o_bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  \n",
    "    ):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "\n",
    "        # projetion of the hidden states into query, key and value\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Get the rotary embeddings cosines and sines functions\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # apply the rotary embeddings to the query and key states\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # Copy kv for matching the number of heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        # applied scaled dot product attention\n",
    "        # attn_weights has shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        # attn_output has shape (batch_size,  seq_len, num_heads, head_dim) after transpose\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # attn_output output has shape (batch_size, seq_len, num_heads * head_dim) after reshape\n",
    "        # which is equivalent to concatenating the heads\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        # apply the output projection\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN Layer\n",
    "\n",
    "Llama uses Swish function in the GLU, we can obtain the following variations:\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}_{SwiGLU} = (\\text{Swish}_1(\\underbrace{xW_1}_{\\text{Gate Projection}})\\otimes \\underbrace{xV}_{\\text{Up Projection}} ) \\underbrace{W_2}_{\\text{Down Projection}}\n",
    "$$\n",
    "\n",
    "with $\\operatorname{Swish}_1(x)=x \\cdot \\sigma(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        # silu is the same as swish\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama Decoder Layer\n",
    "\n",
    "Each decoder layer has\n",
    "* Two Pre-RMSNorm layers, one before the self-attention sublayer and one before the FFN layer\n",
    "* GQA attention layer\n",
    "* FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "        # FFN layer\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        # pre layer norm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        # pre layer norm before FFN layer\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Decoder layers\n",
    "\n",
    "In the stacked decoder layer, \n",
    "* There are L decoder layers\n",
    "* Rotary embeddings (i.e., elements in the rotation matrices) are shared across layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        # apply to last layer hidden state\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # rotary embedding matrices are shared across the decoder layers\n",
    "        self.rotary_emb = LlamaRotaryEmbedding( dim=config.hidden_size // config.num_attention_heads,\n",
    "                                                max_position_embeddings=config.max_position_embeddings,\n",
    "                                                base=config.rope_theta,)\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ):\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.shape[1], dtype=torch.int64, device=hidden_states.device)\n",
    "            position_ids = position_ids.expand(input_ids.shape[0], -1)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder for language modeling\n",
    "\n",
    "Decoder with language modeling is the previous stacked decoder layer plus a linear layer as language prediction head. The langauge prediciton head linearly transforms the hidden state into the logits distributed over the vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    #_tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "    ):\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  from omegaconf import OmegaConf\n",
    "  model_config = {\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"bos_token_id\": 151643,\n",
    "    \"eos_token_id\": 151643,\n",
    "    \"pad_token_id\": 151643,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 896,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4864,\n",
    "    \"max_position_embeddings\": 32768,\n",
    "    \"max_window_layers\": 24,\n",
    "    \"model_type\": \"qwen2\",\n",
    "    \"num_attention_heads\": 14,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"rope_theta\": 1000000.0,\n",
    "    \"tie_word_embeddings\": True,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"transformers_version\": \"4.47.1\",\n",
    "    \"use_cache\": True,\n",
    "    \"use_mrope\": False,\n",
    "    \"vocab_size\": 151936,\n",
    "    \"qkv_bias\": True,\n",
    "    \"o_bias\": False,\n",
    "    \"mlp_bias\": False\n",
    "  }\n",
    "\n",
    "  model_config = OmegaConf.create(model_config)\n",
    "  custom_model = LlamaForCausalLM(model_config)\n",
    "\n",
    "  # load model weight from Huggingface\n",
    "  import transformers\n",
    "  from transformers import AutoModelForCausalLM\n",
    "\n",
    "  model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  custom_model.load_state_dict(model.state_dict(), strict=False)\n",
    "\n",
    "  # test input\n",
    "  input_ids = torch.LongTensor([[1, 2, 3]])\n",
    "  custom_model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text + \"\\n\\n### Response:\\n\"\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.prompt_with_completions = []\n",
    "        self.completions = []\n",
    "        \n",
    "        for entry in data:\n",
    "            \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            completion = entry['output']\n",
    "            \n",
    "            self.prompt_with_completions.append(instruction_plus_input + completion)\n",
    "            self.completions.append(completion)\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_with_completions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompt_with_completions[idx], self.completions[idx]\n",
    "\n",
    "def custom_collate_fn(batch, tokenizer, ignore_idx=-100):\n",
    "    \n",
    "    prompt_with_completions, completions = zip(*batch)\n",
    "    \n",
    "    padded = tokenizer(list(prompt_with_completions), padding='longest', truncation=True, return_tensors='pt')\n",
    "    padded = padded['input_ids']\n",
    "    # targets are shifted by 1\n",
    "    # The last token in padded is not used in inputs as there is no corresponding target.\n",
    "    inputs = padded[:,:-1]\n",
    "    targets = padded[:,1:]\n",
    "    \n",
    "    mask = (targets == tokenizer.pad_token_id)\n",
    "    \n",
    "    targets = targets.masked_fill(mask, ignore_idx)\n",
    "    \n",
    "    return inputs, targets\n",
    "    \n",
    "     \n",
    "def create_data_loader(data, tokenizer, batch_size=4, max_length=256, \n",
    "                       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    dataset = InstructionDataset(data=data)\n",
    "    \n",
    "    collate_fn = partial(custom_collate_fn, tokenizer=tokenizer, ignore_idx=-100)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=drop_last,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_data(file_path, url):\n",
    "    import urllib\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  31115,    264,  15908,  40158,   6524,    382,\n",
      "          14374,   5571,    510,     40,   1079,  14589,    369,    537,  33094,\n",
      "            847,  16319,    389,    882,    382,  14374,   5949,    510,  30665,\n",
      "            508,    675,  49088,     40,  36879,    369,  33094,    847,  16319,\n",
      "           3309,     13,    358,   3535,    429,  33094,    975,    389,    882,\n",
      "            374,   1376,    311,    697,   6950,    304,    752,     13,    358,\n",
      "           1079,   8480,    369,    279,   7626,    323,    358,   1896,   2480,\n",
      "          38142,    369,    432,     13,    358,  58283,  22231,    894,  60009,\n",
      "           8881,    553,    847,   6168,    382,     40,  15440,    429,    358,\n",
      "            686,   1896,    678,   5871,   7354,    311,   5978,    429,   1741,\n",
      "            458,  10455,   1558,    537,   3537,   1549,     13,   9063,  27378,\n",
      "             11,    358,    686,    653,    847,   1850,    311,   1281,    705,\n",
      "            369,    847,  20643,    382,     40,   1401,   4637,    311,    279,\n",
      "           6638,    315,  49103,    697,   6950,    304,    752,    382,     50,\n",
      "          86091,    345,     58,   7771,   3988],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  60424,   2326,  10295,    315,  65060,    315,\n",
      "            220,     21,    382,  14374,   5949,    510,  19641,  10295,    315,\n",
      "          65060,    315,    220,     21,    525,    220,     21,     11,    220,\n",
      "             16,     17,     11,    323,    220,     16,     23,     13, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  22550,   1493,   5837,   4092,    311,    862,\n",
      "          42807,  51749,   5643,    271,  14374,   5571,    510,  24347,     11,\n",
      "           9856,     11,   9625,     11,   6747,    271,  14374,   5949,    510,\n",
      "          24347,     11,   9856,     11,   6747,     11,   9625, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,   4340,   2310,    374,    279,  88575,    315,\n",
      "          31392,   1939,  14374,   5949,    510,    785,  88575,    315,  31392,\n",
      "            374,    916,    220,     16,     18,     19,   1635,   2310,     11,\n",
      "           3432,   1012,   8145,    304,   6527,    220,     16,     23,     23,\n",
      "             21,     13, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643]]), tensor([[  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         31115,   264, 15908, 40158,  6524,   382, 14374,  5571,   510,    40,\n",
      "          1079, 14589,   369,   537, 33094,   847, 16319,   389,   882,   382,\n",
      "         14374,  5949,   510, 30665,   508,   675, 49088,    40, 36879,   369,\n",
      "         33094,   847, 16319,  3309,    13,   358,  3535,   429, 33094,   975,\n",
      "           389,   882,   374,  1376,   311,   697,  6950,   304,   752,    13,\n",
      "           358,  1079,  8480,   369,   279,  7626,   323,   358,  1896,  2480,\n",
      "         38142,   369,   432,    13,   358, 58283, 22231,   894, 60009,  8881,\n",
      "           553,   847,  6168,   382,    40, 15440,   429,   358,   686,  1896,\n",
      "           678,  5871,  7354,   311,  5978,   429,  1741,   458, 10455,  1558,\n",
      "           537,  3537,  1549,    13,  9063, 27378,    11,   358,   686,   653,\n",
      "           847,  1850,   311,  1281,   705,   369,   847, 20643,   382,    40,\n",
      "          1401,  4637,   311,   279,  6638,   315, 49103,   697,  6950,   304,\n",
      "           752,   382,    50, 86091,   345,    58,  7771,  3988,    60],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         60424,  2326, 10295,   315, 65060,   315,   220,    21,   382, 14374,\n",
      "          5949,   510, 19641, 10295,   315, 65060,   315,   220,    21,   525,\n",
      "           220,    21,    11,   220,    16,    17,    11,   323,   220,    16,\n",
      "            23,    13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         22550,  1493,  5837,  4092,   311,   862, 42807, 51749,  5643,   271,\n",
      "         14374,  5571,   510, 24347,    11,  9856,    11,  9625,    11,  6747,\n",
      "           271, 14374,  5949,   510, 24347,    11,  9856,    11,  6747,    11,\n",
      "          9625,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "          4340,  2310,   374,   279, 88575,   315, 31392,  1939, 14374,  5949,\n",
      "           510,   785, 88575,   315, 31392,   374,   916,   220,    16,    18,\n",
      "            19,  1635,  2310,    11,  3432,  1012,  8145,   304,  6527,   220,\n",
      "            16,    23,    23,    21,    13,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]]))\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "def test_data_component():\n",
    "    file_path = \"instruction-data2.json\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"        \n",
    "    )\n",
    "\n",
    "    text_data = read_text_data(file_path, url)\n",
    "    tokenizer_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    #tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    for batch in train_loader:\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "test_data_component()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    flat_targets = target_batch.flatten() \n",
    "    flat_logits = logits.flatten(0, 1)# flatten the first two dimensions \n",
    "    loss = F.cross_entropy(flat_logits, flat_targets) # tokens with ignore idx will not contribute to loss \n",
    "    return loss\n",
    "\n",
    "def train_model_epoch(model, \n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                num_epochs):\n",
    "    \n",
    "    train_losses, val_losses, track_token_seen = [],[],[]\n",
    "    tokens_seen = 0\n",
    "    global_steps = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_steps += 1\n",
    "            train_losses.append(loss.detach().item())\n",
    "            print(train_losses[-1])\n",
    "        \n",
    "    return train_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model_config, train_settings):\n",
    "    \n",
    "    torch.manual_seed(train_settings.seed)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    text_data = read_text_data(train_settings.file_path, train_settings.url)\n",
    "            \n",
    "    model = LlamaForCausalLM(config=model_config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=train_settings.learning_rate,\n",
    "                                  weight_decay=train_settings.weight_decay)\n",
    "    \n",
    "    # set up dataloader\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(train_settings.pretrained_model_name)\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    \n",
    "    \n",
    "    train_loader = create_data_loader(data=text_data,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      drop_last=True,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0\n",
    "    )\n",
    "        \n",
    "    train_losses, model = train_model_epoch(model=model,\n",
    "                train_loader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=train_settings.num_epochs,\n",
    "                device=device)\n",
    "    \n",
    "    return train_losses, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.114983558654785\n",
      "11.961651802062988\n",
      "11.927282333374023\n",
      "11.826778411865234\n",
      "11.707758903503418\n",
      "11.793996810913086\n",
      "11.565061569213867\n",
      "11.341635704040527\n",
      "11.44568920135498\n",
      "11.51699161529541\n",
      "11.538795471191406\n",
      "11.205238342285156\n",
      "11.476774215698242\n",
      "11.2799654006958\n",
      "10.608583450317383\n",
      "10.964029312133789\n",
      "11.31577205657959\n",
      "10.75229549407959\n",
      "10.765582084655762\n",
      "10.845158576965332\n",
      "10.3082857131958\n",
      "10.781938552856445\n",
      "11.183990478515625\n",
      "10.394356727600098\n",
      "10.934983253479004\n",
      "9.947456359863281\n",
      "10.557378768920898\n",
      "10.630148887634277\n",
      "10.455920219421387\n",
      "10.409931182861328\n",
      "10.108236312866211\n",
      "10.182458877563477\n",
      "9.501925468444824\n",
      "10.636826515197754\n",
      "10.441350936889648\n",
      "9.442732810974121\n",
      "10.65182113647461\n",
      "9.005826950073242\n",
      "10.056744575500488\n",
      "9.071742057800293\n",
      "9.719274520874023\n",
      "10.539457321166992\n",
      "8.109013557434082\n",
      "9.988809585571289\n",
      "9.503620147705078\n",
      "8.56149673461914\n",
      "8.963241577148438\n",
      "9.929883003234863\n",
      "9.429275512695312\n",
      "9.508111953735352\n",
      "9.378183364868164\n",
      "8.728569984436035\n",
      "10.021723747253418\n",
      "8.781039237976074\n",
      "9.219264030456543\n",
      "8.941544532775879\n",
      "9.30162525177002\n",
      "9.398658752441406\n",
      "8.79794979095459\n",
      "8.475982666015625\n",
      "8.320345878601074\n",
      "8.690967559814453\n",
      "8.79287338256836\n",
      "8.732223510742188\n",
      "9.572741508483887\n",
      "9.09702205657959\n",
      "9.091935157775879\n",
      "9.1818265914917\n",
      "9.378776550292969\n",
      "8.197397232055664\n",
      "8.443197250366211\n",
      "8.796965599060059\n",
      "8.650116920471191\n",
      "9.56566047668457\n",
      "9.037643432617188\n",
      "9.316316604614258\n",
      "9.480766296386719\n",
      "8.105834007263184\n",
      "7.8174004554748535\n",
      "8.75154972076416\n",
      "8.871994972229004\n",
      "7.084001064300537\n",
      "6.267117977142334\n",
      "8.413370132446289\n",
      "8.928682327270508\n",
      "8.569966316223145\n",
      "8.723995208740234\n",
      "7.712878227233887\n",
      "8.335203170776367\n",
      "8.576348304748535\n",
      "9.202093124389648\n",
      "8.326104164123535\n",
      "8.715723037719727\n",
      "8.051654815673828\n",
      "7.702801704406738\n",
      "7.767918109893799\n",
      "7.44614315032959\n",
      "8.499092102050781\n",
      "6.835850715637207\n",
      "8.17809009552002\n",
      "8.459793090820312\n",
      "8.364670753479004\n",
      "7.678229808807373\n",
      "8.883283615112305\n",
      "8.294384956359863\n",
      "7.772454738616943\n",
      "8.872432708740234\n",
      "8.260662078857422\n",
      "8.852313041687012\n",
      "9.336148262023926\n",
      "8.217145919799805\n",
      "8.837427139282227\n",
      "7.094305515289307\n",
      "8.249174118041992\n",
      "6.225741386413574\n",
      "7.286722183227539\n",
      "8.037725448608398\n",
      "8.328179359436035\n",
      "7.948586463928223\n",
      "6.679088592529297\n",
      "8.223731994628906\n",
      "8.198479652404785\n",
      "8.246973991394043\n",
      "7.424270153045654\n",
      "8.886054039001465\n",
      "8.324112892150879\n",
      "8.785794258117676\n",
      "8.4138765335083\n",
      "8.507905960083008\n",
      "8.868498802185059\n",
      "7.550424575805664\n",
      "8.014437675476074\n",
      "8.699291229248047\n",
      "7.966353416442871\n",
      "9.073481559753418\n",
      "6.768507957458496\n",
      "7.203961372375488\n",
      "7.526586055755615\n",
      "8.272527694702148\n",
      "8.836536407470703\n",
      "7.6048583984375\n",
      "6.950989246368408\n",
      "7.622305870056152\n",
      "8.43606948852539\n",
      "8.453845024108887\n",
      "8.930305480957031\n",
      "7.952410697937012\n",
      "8.807299613952637\n",
      "8.922638893127441\n",
      "8.14686393737793\n",
      "7.115427494049072\n",
      "7.830827236175537\n",
      "8.790492057800293\n",
      "8.53681755065918\n",
      "7.681835651397705\n",
      "7.839694976806641\n",
      "7.047902584075928\n",
      "7.7306365966796875\n",
      "7.827072620391846\n",
      "8.690933227539062\n",
      "8.336493492126465\n",
      "7.476668834686279\n",
      "9.079100608825684\n",
      "7.954265594482422\n",
      "7.466475486755371\n",
      "8.256246566772461\n",
      "7.341320037841797\n",
      "6.160398006439209\n",
      "8.395797729492188\n",
      "6.977738380432129\n",
      "8.152826309204102\n",
      "7.279878616333008\n",
      "7.9665141105651855\n",
      "7.751248836517334\n",
      "7.755195617675781\n",
      "8.081643104553223\n",
      "8.407286643981934\n",
      "7.913785934448242\n",
      "6.948304653167725\n",
      "7.02336311340332\n",
      "7.258242607116699\n",
      "8.707741737365723\n",
      "5.176688194274902\n",
      "7.005913734436035\n",
      "8.04478645324707\n",
      "7.365195274353027\n",
      "6.96026611328125\n",
      "7.431405067443848\n",
      "6.723588466644287\n",
      "8.396443367004395\n",
      "8.315401077270508\n",
      "8.87647533416748\n",
      "7.090518951416016\n",
      "6.729640483856201\n",
      "7.5797553062438965\n",
      "7.677322864532471\n",
      "8.530302047729492\n",
      "5.831056118011475\n",
      "8.948013305664062\n",
      "8.121613502502441\n",
      "7.95388650894165\n",
      "5.317086696624756\n",
      "5.587782382965088\n",
      "7.112850666046143\n",
      "7.076551914215088\n",
      "8.187629699707031\n",
      "5.603550434112549\n",
      "7.574467658996582\n",
      "7.634267807006836\n",
      "7.89480447769165\n",
      "7.144584655761719\n",
      "6.720107078552246\n",
      "7.302224636077881\n",
      "7.600544452667236\n",
      "7.489965438842773\n",
      "8.439957618713379\n",
      "6.317536354064941\n",
      "7.296192169189453\n",
      "7.806612491607666\n",
      "6.755681991577148\n",
      "7.698017597198486\n",
      "7.634511470794678\n",
      "7.9502949714660645\n",
      "5.371891021728516\n",
      "5.99277925491333\n",
      "7.765937805175781\n",
      "7.635364055633545\n",
      "5.691554069519043\n",
      "7.497350692749023\n",
      "8.27408504486084\n",
      "7.323247909545898\n",
      "7.470743179321289\n",
      "7.872669696807861\n",
      "7.903525352478027\n",
      "7.573783874511719\n",
      "7.304293155670166\n",
      "7.692304611206055\n",
      "6.821817398071289\n",
      "7.150716781616211\n",
      "8.094555854797363\n",
      "7.496853351593018\n",
      "7.204647064208984\n",
      "6.9287333488464355\n",
      "7.656194686889648\n",
      "6.684568405151367\n",
      "8.101051330566406\n",
      "6.001102447509766\n",
      "6.5603532791137695\n",
      "7.81414794921875\n",
      "7.483189582824707\n",
      "7.4107441902160645\n",
      "7.123326778411865\n",
      "7.163662910461426\n",
      "7.600272178649902\n",
      "7.92359733581543\n",
      "6.966070652008057\n",
      "6.676620960235596\n",
      "6.071258068084717\n",
      "5.694700717926025\n",
      "8.357487678527832\n",
      "7.621312618255615\n",
      "7.154609680175781\n",
      "8.327323913574219\n",
      "6.954722881317139\n",
      "5.917107582092285\n",
      "7.6910786628723145\n",
      "6.665449619293213\n",
      "6.7606987953186035\n",
      "8.196247100830078\n",
      "7.23905086517334\n",
      "5.622346878051758\n",
      "8.084579467773438\n",
      "7.474020481109619\n",
      "6.9481024742126465\n",
      "6.517650604248047\n",
      "7.622471332550049\n",
      "6.6400299072265625\n",
      "7.146424770355225\n",
      "8.000862121582031\n",
      "7.315019607543945\n",
      "7.4658331871032715\n",
      "7.999532699584961\n",
      "6.73932409286499\n",
      "6.784018039703369\n",
      "7.335762023925781\n",
      "6.795717716217041\n",
      "7.183656692504883\n",
      "6.980317115783691\n",
      "6.3821120262146\n",
      "7.172737121582031\n",
      "6.515994071960449\n",
      "8.037397384643555\n",
      "6.697264671325684\n",
      "6.97719144821167\n",
      "6.995906829833984\n",
      "7.219350337982178\n",
      "7.621826648712158\n",
      "6.60056209564209\n",
      "6.303577899932861\n",
      "6.741073131561279\n",
      "6.910447120666504\n",
      "5.5270466804504395\n",
      "7.173192501068115\n",
      "7.292308330535889\n",
      "7.583615779876709\n",
      "6.667201995849609\n",
      "7.151587963104248\n",
      "7.602296352386475\n",
      "7.106971740722656\n",
      "7.574258804321289\n",
      "7.1769914627075195\n",
      "5.854578495025635\n",
      "7.636929512023926\n",
      "8.061482429504395\n",
      "7.62721586227417\n",
      "7.101871013641357\n",
      "7.244905471801758\n",
      "6.899890422821045\n",
      "6.74027681350708\n",
      "7.143261909484863\n",
      "7.437134265899658\n",
      "7.466038227081299\n",
      "6.8156819343566895\n",
      "7.31174373626709\n",
      "6.805547714233398\n",
      "7.186622142791748\n",
      "7.483814716339111\n",
      "6.559063911437988\n",
      "5.888976573944092\n",
      "7.138240814208984\n",
      "7.423436641693115\n",
      "7.507336616516113\n",
      "7.050281047821045\n",
      "5.500576972961426\n",
      "7.398224353790283\n",
      "7.023150444030762\n",
      "6.995843887329102\n",
      "7.583437442779541\n",
      "7.764934539794922\n",
      "6.284381866455078\n",
      "6.54485559463501\n",
      "7.271533966064453\n",
      "6.747768878936768\n",
      "5.4070963859558105\n",
      "7.711648464202881\n",
      "6.8751044273376465\n",
      "4.873578071594238\n",
      "5.798689842224121\n",
      "8.163131713867188\n",
      "6.023693561553955\n",
      "6.624054431915283\n",
      "7.368297576904297\n",
      "6.57619571685791\n",
      "7.637344837188721\n",
      "6.854536056518555\n",
      "6.881252765655518\n",
      "6.129610061645508\n",
      "7.047771453857422\n",
      "7.673618316650391\n",
      "7.853394985198975\n",
      "7.695700645446777\n",
      "6.286881446838379\n",
      "4.918391704559326\n",
      "6.60928201675415\n",
      "7.443287372589111\n",
      "7.012099266052246\n",
      "6.712862968444824\n",
      "6.353724956512451\n",
      "6.589232444763184\n",
      "7.810102939605713\n",
      "6.120845317840576\n",
      "7.180410385131836\n",
      "5.988944053649902\n",
      "7.331676006317139\n",
      "7.220089912414551\n",
      "7.535629749298096\n",
      "6.000095367431641\n",
      "7.269332408905029\n",
      "6.273829936981201\n",
      "6.7589945793151855\n",
      "6.78818941116333\n",
      "7.716200351715088\n",
      "6.104673862457275\n",
      "7.209777355194092\n",
      "6.009293556213379\n",
      "6.337612152099609\n",
      "7.183322906494141\n",
      "6.3463053703308105\n",
      "8.53585433959961\n",
      "6.489135265350342\n",
      "6.748711585998535\n",
      "5.630690574645996\n",
      "6.174975395202637\n",
      "6.681833744049072\n",
      "6.685907363891602\n",
      "6.3267621994018555\n",
      "6.549046039581299\n",
      "7.512974739074707\n",
      "6.26035213470459\n",
      "6.584530353546143\n",
      "6.311887741088867\n",
      "5.779975414276123\n",
      "6.518989562988281\n",
      "6.98706579208374\n",
      "7.305261135101318\n",
      "5.529804706573486\n",
      "6.763954162597656\n",
      "6.788674831390381\n",
      "7.4324140548706055\n",
      "7.752645969390869\n",
      "6.385810375213623\n",
      "7.1607208251953125\n",
      "7.199484825134277\n",
      "6.8876633644104\n",
      "7.428523063659668\n",
      "7.391116142272949\n",
      "7.399779796600342\n",
      "6.628757476806641\n",
      "6.763996601104736\n",
      "6.302529811859131\n",
      "6.880929946899414\n",
      "7.014496803283691\n",
      "5.598532199859619\n",
      "6.9200592041015625\n",
      "7.126089096069336\n",
      "6.731667518615723\n",
      "6.363903999328613\n",
      "6.017136573791504\n",
      "6.837329864501953\n",
      "6.472388744354248\n",
      "5.30039644241333\n",
      "7.211876392364502\n",
      "6.566425323486328\n",
      "6.906378746032715\n",
      "7.059329509735107\n",
      "5.300551414489746\n",
      "6.950073719024658\n",
      "7.633367538452148\n",
      "6.883755207061768\n",
      "5.330907344818115\n",
      "5.959106922149658\n",
      "6.559053421020508\n",
      "7.327077388763428\n",
      "6.252851486206055\n",
      "6.105184078216553\n",
      "6.396198272705078\n",
      "6.692835807800293\n",
      "7.1359076499938965\n",
      "7.052042007446289\n",
      "7.576384544372559\n",
      "6.252415657043457\n",
      "6.510921478271484\n",
      "6.675665378570557\n",
      "4.823562145233154\n",
      "6.835978031158447\n",
      "6.80307149887085\n",
      "7.005776882171631\n",
      "6.290078163146973\n",
      "5.8772687911987305\n",
      "6.2689290046691895\n",
      "6.407567024230957\n",
      "4.917119979858398\n",
      "5.427750110626221\n",
      "5.4736328125\n",
      "7.155660629272461\n",
      "6.854784965515137\n",
      "7.569583892822266\n",
      "6.052281379699707\n",
      "7.647436618804932\n",
      "6.95413064956665\n",
      "7.043063640594482\n",
      "6.3223795890808105\n",
      "7.029530048370361\n",
      "6.009037494659424\n",
      "5.369438171386719\n",
      "6.471277713775635\n",
      "5.596868991851807\n",
      "5.728212833404541\n",
      "6.57578182220459\n",
      "6.205798625946045\n",
      "7.723975658416748\n",
      "6.969112873077393\n",
      "6.523230075836182\n",
      "7.158575057983398\n",
      "6.533547401428223\n",
      "6.217704772949219\n",
      "4.9049072265625\n",
      "7.263994216918945\n",
      "6.208010673522949\n",
      "6.861074447631836\n",
      "6.295699119567871\n",
      "6.602899551391602\n",
      "6.556525230407715\n",
      "6.597959041595459\n",
      "6.1291351318359375\n",
      "6.438089370727539\n",
      "6.826323509216309\n",
      "6.69783878326416\n",
      "6.3540425300598145\n",
      "7.287336826324463\n",
      "4.87325382232666\n",
      "7.027651309967041\n",
      "5.817617416381836\n",
      "6.585550308227539\n",
      "5.752303123474121\n",
      "5.535881042480469\n",
      "6.355349540710449\n",
      "5.66028356552124\n",
      "4.829235553741455\n",
      "6.75210428237915\n",
      "6.757355690002441\n",
      "6.3517656326293945\n",
      "6.707007884979248\n",
      "5.924886226654053\n",
      "6.391934394836426\n",
      "4.579033374786377\n",
      "6.863368034362793\n",
      "6.487936019897461\n",
      "6.390007495880127\n",
      "6.895957946777344\n",
      "6.898429870605469\n",
      "6.337359428405762\n",
      "6.9140095710754395\n",
      "6.807152271270752\n",
      "6.988922595977783\n",
      "7.189905166625977\n",
      "6.269647121429443\n",
      "6.8126726150512695\n",
      "4.816813945770264\n",
      "5.724527359008789\n",
      "7.713352203369141\n",
      "5.766097545623779\n",
      "6.72987174987793\n",
      "6.434146881103516\n",
      "6.73057746887207\n",
      "7.03439474105835\n",
      "5.549989223480225\n",
      "6.1966233253479\n",
      "6.8368377685546875\n",
      "7.853939533233643\n",
      "6.491785049438477\n",
      "6.493218898773193\n",
      "5.554964065551758\n",
      "5.743686199188232\n",
      "7.60521936416626\n",
      "6.051425457000732\n",
      "5.890924453735352\n",
      "6.29083251953125\n",
      "6.18527889251709\n",
      "6.9394731521606445\n",
      "5.673720836639404\n",
      "5.949153900146484\n",
      "6.510037422180176\n",
      "6.879708290100098\n",
      "6.112819671630859\n",
      "6.374730110168457\n",
      "7.098811626434326\n",
      "5.41429328918457\n",
      "5.876171588897705\n",
      "6.408968448638916\n",
      "7.09466552734375\n",
      "6.505105972290039\n",
      "6.930153846740723\n",
      "5.188701629638672\n",
      "6.083053112030029\n",
      "6.429739475250244\n",
      "6.784379005432129\n",
      "6.117002010345459\n",
      "6.531440258026123\n",
      "6.997178077697754\n",
      "5.864298343658447\n",
      "6.955168724060059\n",
      "5.899875164031982\n",
      "6.1008782386779785\n",
      "6.774710655212402\n",
      "5.519045352935791\n",
      "5.862921714782715\n",
      "6.601291179656982\n",
      "5.76458215713501\n",
      "7.934217929840088\n",
      "6.3184404373168945\n",
      "5.798949718475342\n",
      "5.4431843757629395\n",
      "6.169183731079102\n",
      "5.487637042999268\n",
      "6.239485263824463\n",
      "6.50405740737915\n",
      "6.234323978424072\n",
      "6.967166423797607\n",
      "5.920042514801025\n",
      "4.90633487701416\n",
      "5.497448921203613\n",
      "6.810624122619629\n",
      "5.948454856872559\n",
      "5.56923770904541\n",
      "5.366409778594971\n",
      "6.629382610321045\n",
      "6.370241641998291\n",
      "5.956958293914795\n",
      "5.130191326141357\n",
      "6.562791347503662\n",
      "5.687262058258057\n",
      "4.979550361633301\n",
      "6.183032512664795\n",
      "6.493767261505127\n",
      "6.1848344802856445\n",
      "6.289977550506592\n",
      "5.878566741943359\n",
      "5.262190818786621\n",
      "6.668609619140625\n",
      "6.653968811035156\n",
      "6.002604961395264\n",
      "5.90189790725708\n",
      "6.340113162994385\n",
      "5.9966230392456055\n",
      "5.680359840393066\n",
      "5.483484745025635\n",
      "6.6956562995910645\n",
      "6.696816921234131\n",
      "6.163222789764404\n",
      "6.37417459487915\n",
      "6.459051609039307\n",
      "6.055540084838867\n",
      "6.523402214050293\n",
      "6.372015953063965\n",
      "6.172832489013672\n",
      "5.614804267883301\n",
      "6.371455669403076\n",
      "6.445786476135254\n",
      "6.082972049713135\n",
      "5.580028533935547\n",
      "4.932221412658691\n",
      "6.254146099090576\n",
      "5.9570159912109375\n",
      "7.014609336853027\n",
      "7.235097408294678\n",
      "5.052845001220703\n",
      "5.747809410095215\n",
      "5.098481178283691\n",
      "7.14454984664917\n",
      "6.109280109405518\n",
      "5.398374557495117\n",
      "4.213296890258789\n",
      "5.434049606323242\n",
      "5.272828102111816\n",
      "7.0189290046691895\n",
      "6.264615058898926\n",
      "5.432180404663086\n",
      "5.706062316894531\n",
      "5.067690849304199\n",
      "5.473038673400879\n",
      "7.232213497161865\n",
      "5.500061988830566\n",
      "5.305816173553467\n",
      "5.634389877319336\n",
      "5.7206292152404785\n",
      "6.477426528930664\n",
      "5.866785049438477\n",
      "6.254941940307617\n",
      "6.022000789642334\n",
      "6.145068168640137\n",
      "6.892409324645996\n",
      "6.718930721282959\n",
      "6.42185115814209\n",
      "7.1081318855285645\n",
      "6.033673286437988\n",
      "5.882725715637207\n",
      "6.184030055999756\n",
      "6.220764636993408\n",
      "5.509552001953125\n",
      "5.992305278778076\n",
      "6.082344055175781\n",
      "6.490474700927734\n",
      "6.823636054992676\n",
      "5.924805164337158\n",
      "6.592731475830078\n",
      "6.081788539886475\n",
      "5.7530999183654785\n",
      "5.421682357788086\n",
      "5.761895179748535\n",
      "6.41145658493042\n",
      "5.259069442749023\n",
      "6.91668176651001\n",
      "5.9712347984313965\n",
      "4.9838547706604\n",
      "5.8265886306762695\n",
      "5.741562366485596\n",
      "5.678469657897949\n",
      "6.087961673736572\n",
      "6.12254524230957\n",
      "6.6356282234191895\n",
      "5.999855995178223\n",
      "5.242163181304932\n",
      "5.566010475158691\n",
      "6.29323148727417\n",
      "6.052127838134766\n",
      "6.39547872543335\n",
      "5.888261795043945\n",
      "6.235403537750244\n",
      "6.532500267028809\n",
      "6.24505090713501\n",
      "5.629907608032227\n",
      "6.488248825073242\n",
      "7.229555606842041\n",
      "6.516720771789551\n",
      "6.067878723144531\n",
      "5.0529069900512695\n",
      "5.704999923706055\n",
      "6.138185024261475\n",
      "5.723733425140381\n",
      "6.492624759674072\n",
      "6.21068000793457\n",
      "5.799619197845459\n",
      "5.321924686431885\n",
      "5.703210353851318\n",
      "6.221750259399414\n",
      "6.257620811462402\n",
      "5.067604064941406\n",
      "5.78139591217041\n",
      "6.810388565063477\n",
      "6.10854959487915\n",
      "5.632315635681152\n",
      "6.442394733428955\n",
      "5.245612621307373\n",
      "6.078483581542969\n",
      "5.893668174743652\n",
      "6.5216474533081055\n",
      "6.554015159606934\n",
      "6.825289726257324\n",
      "6.4750823974609375\n",
      "6.112430095672607\n",
      "4.616897106170654\n",
      "5.479202747344971\n",
      "5.688720703125\n",
      "6.061813831329346\n",
      "4.848109722137451\n",
      "5.831615447998047\n",
      "5.852490425109863\n",
      "6.025716781616211\n",
      "6.123955249786377\n",
      "5.684447288513184\n",
      "5.867308616638184\n",
      "5.07834529876709\n",
      "4.90164852142334\n",
      "5.524458408355713\n",
      "5.9056620597839355\n",
      "6.2032294273376465\n",
      "5.065298080444336\n",
      "6.184986114501953\n",
      "6.666153430938721\n",
      "6.254053115844727\n",
      "4.970147609710693\n",
      "6.205985069274902\n",
      "7.008021831512451\n",
      "4.247925281524658\n",
      "6.835949420928955\n",
      "6.042531490325928\n",
      "6.213634490966797\n",
      "5.94294548034668\n",
      "6.186612129211426\n",
      "4.406094074249268\n",
      "5.199366569519043\n",
      "5.122321128845215\n",
      "5.584754943847656\n",
      "5.180679798126221\n",
      "6.580134868621826\n",
      "6.852870941162109\n",
      "5.759560585021973\n",
      "5.577573776245117\n",
      "6.284355640411377\n",
      "5.7807536125183105\n",
      "4.727319717407227\n",
      "5.722000598907471\n",
      "6.277144908905029\n",
      "6.57779598236084\n",
      "5.90477180480957\n",
      "6.326879024505615\n",
      "4.294795036315918\n",
      "5.40275764465332\n",
      "5.876376628875732\n",
      "6.609960079193115\n",
      "5.141951084136963\n",
      "6.104902744293213\n",
      "6.284786701202393\n",
      "6.333367824554443\n",
      "6.107256889343262\n",
      "5.362699031829834\n",
      "5.961429595947266\n",
      "5.0032548904418945\n",
      "5.892342567443848\n",
      "6.109997272491455\n",
      "5.71601676940918\n",
      "5.278239727020264\n",
      "5.517978191375732\n",
      "5.3576202392578125\n",
      "6.091977119445801\n",
      "5.373385906219482\n",
      "5.846373081207275\n",
      "6.528918743133545\n",
      "5.745575428009033\n",
      "6.333774089813232\n",
      "5.838168621063232\n",
      "5.9162068367004395\n",
      "5.3981733322143555\n",
      "5.436718463897705\n",
      "5.078586578369141\n",
      "5.785589694976807\n",
      "4.8114094734191895\n",
      "5.154354572296143\n",
      "5.741503715515137\n",
      "6.176111221313477\n",
      "5.606869220733643\n",
      "5.833148002624512\n",
      "6.833350658416748\n",
      "5.255387783050537\n",
      "3.988438606262207\n",
      "5.452328205108643\n",
      "6.673221111297607\n",
      "6.423011779785156\n",
      "5.384125709533691\n",
      "5.643775939941406\n",
      "6.828341007232666\n",
      "4.667187213897705\n",
      "4.4618144035339355\n",
      "6.1435980796813965\n",
      "6.366528511047363\n",
      "6.081357955932617\n",
      "5.075766086578369\n",
      "5.480937957763672\n",
      "5.268439769744873\n",
      "4.674612998962402\n",
      "5.6378068923950195\n",
      "5.315445423126221\n",
      "5.300681114196777\n",
      "4.9949235916137695\n",
      "5.961142539978027\n",
      "5.220792770385742\n",
      "5.820557594299316\n",
      "5.383841037750244\n",
      "4.481714248657227\n",
      "6.074995517730713\n",
      "6.0024333000183105\n",
      "4.333754062652588\n",
      "5.863290309906006\n",
      "5.75282621383667\n",
      "5.2128376960754395\n",
      "4.999240875244141\n",
      "4.917527198791504\n",
      "6.363485336303711\n",
      "4.912549018859863\n",
      "5.236627101898193\n",
      "5.727591514587402\n",
      "4.884681701660156\n",
      "5.498530387878418\n",
      "5.64660120010376\n",
      "6.371853351593018\n",
      "5.769754409790039\n",
      "4.753459453582764\n",
      "5.481202602386475\n",
      "5.8541741371154785\n",
      "5.7227935791015625\n",
      "6.016483306884766\n",
      "5.684218883514404\n",
      "5.422640323638916\n",
      "5.581953048706055\n",
      "6.139718532562256\n",
      "6.288344383239746\n",
      "6.271264553070068\n",
      "6.095365524291992\n",
      "5.580507755279541\n",
      "5.741294860839844\n",
      "5.82970666885376\n",
      "5.041816234588623\n",
      "5.605682849884033\n",
      "5.488674640655518\n",
      "5.9869866371154785\n",
      "4.876048564910889\n",
      "5.183134078979492\n",
      "6.44551944732666\n",
      "5.580467224121094\n",
      "5.336531639099121\n",
      "5.30568265914917\n",
      "5.528031826019287\n",
      "5.766164302825928\n",
      "5.580226898193359\n",
      "5.539986610412598\n",
      "6.154237747192383\n",
      "6.221808910369873\n",
      "6.68833065032959\n",
      "5.206247806549072\n",
      "5.393226623535156\n",
      "5.483890056610107\n",
      "5.020560264587402\n",
      "4.415234088897705\n",
      "5.692431449890137\n",
      "5.725409030914307\n",
      "6.487700462341309\n",
      "6.484647274017334\n",
      "5.3354811668396\n",
      "6.387298583984375\n",
      "5.872298240661621\n",
      "5.732889175415039\n",
      "5.9597320556640625\n",
      "6.123317718505859\n",
      "6.047027587890625\n",
      "6.713091850280762\n",
      "5.43989896774292\n",
      "5.4997334480285645\n",
      "4.09600830078125\n",
      "4.456425189971924\n",
      "6.081859588623047\n",
      "6.143166542053223\n",
      "5.537712097167969\n",
      "5.485445499420166\n",
      "4.451141357421875\n",
      "4.85955286026001\n",
      "5.603623867034912\n",
      "5.343040943145752\n",
      "4.895185947418213\n",
      "4.088583946228027\n",
      "6.178514003753662\n",
      "5.150128364562988\n",
      "4.811209678649902\n",
      "5.331404685974121\n",
      "5.5498576164245605\n",
      "5.18892765045166\n",
      "6.646106719970703\n",
      "4.1795573234558105\n",
      "6.519378185272217\n",
      "5.144201278686523\n",
      "6.021180152893066\n",
      "5.035079002380371\n",
      "5.4208574295043945\n",
      "4.481831073760986\n",
      "5.347463607788086\n",
      "5.000537395477295\n",
      "5.070650100708008\n",
      "5.531777381896973\n",
      "6.393858432769775\n",
      "5.812191009521484\n",
      "4.502859115600586\n",
      "5.52064323425293\n",
      "4.393649101257324\n",
      "5.5129594802856445\n",
      "6.0064697265625\n",
      "6.120830535888672\n",
      "5.94623327255249\n",
      "5.661935806274414\n",
      "5.367886066436768\n",
      "5.446750640869141\n",
      "5.837181568145752\n",
      "4.475765228271484\n",
      "5.495880603790283\n",
      "6.333951950073242\n",
      "5.231400489807129\n",
      "4.890717029571533\n",
      "5.500189781188965\n",
      "4.826181888580322\n",
      "5.315254211425781\n",
      "5.590473175048828\n",
      "5.152644634246826\n",
      "5.569487571716309\n",
      "4.952450275421143\n",
      "5.208878993988037\n",
      "5.6105570793151855\n",
      "5.393913745880127\n",
      "5.2702107429504395\n",
      "4.821797847747803\n",
      "4.272322654724121\n",
      "6.007287502288818\n",
      "4.845762729644775\n",
      "6.0829901695251465\n",
      "5.326286792755127\n",
      "5.137276649475098\n",
      "5.797081470489502\n",
      "6.583113193511963\n",
      "5.723069190979004\n",
      "4.9189252853393555\n",
      "4.666990756988525\n",
      "6.260522842407227\n",
      "3.943296194076538\n",
      "5.16051721572876\n",
      "4.418442249298096\n",
      "6.377189636230469\n",
      "5.632775783538818\n",
      "4.806467056274414\n",
      "5.3496479988098145\n",
      "4.914324760437012\n",
      "4.750854969024658\n",
      "6.281500816345215\n",
      "6.332863807678223\n",
      "6.1956353187561035\n",
      "5.598954200744629\n",
      "5.23215913772583\n",
      "6.006951332092285\n",
      "4.892027854919434\n",
      "5.189830780029297\n",
      "6.273373603820801\n",
      "6.675600528717041\n",
      "5.774920463562012\n",
      "6.101280689239502\n",
      "5.426880836486816\n",
      "4.9738078117370605\n",
      "5.162805080413818\n",
      "5.998691082000732\n",
      "7.236884117126465\n",
      "5.372688293457031\n",
      "5.440547466278076\n",
      "5.061509609222412\n",
      "4.496494293212891\n",
      "4.642920017242432\n",
      "4.741084098815918\n",
      "6.167695999145508\n",
      "6.227993965148926\n",
      "5.357344627380371\n",
      "5.5911970138549805\n",
      "5.26072883605957\n",
      "5.713385581970215\n",
      "5.094918251037598\n",
      "4.06983757019043\n",
      "6.055233478546143\n",
      "4.5529890060424805\n",
      "5.663405895233154\n",
      "4.537608623504639\n",
      "4.568994998931885\n",
      "5.071442127227783\n",
      "5.107507228851318\n",
      "4.914765357971191\n",
      "5.529574394226074\n",
      "6.10886287689209\n",
      "5.124289035797119\n",
      "5.114251136779785\n",
      "4.528431415557861\n",
      "5.219285011291504\n",
      "4.504852771759033\n",
      "4.710177421569824\n",
      "5.006173133850098\n",
      "5.444087982177734\n",
      "4.8146843910217285\n",
      "4.098686695098877\n",
      "5.489388465881348\n",
      "5.107565879821777\n",
      "5.020852088928223\n",
      "3.529670000076294\n",
      "5.425512790679932\n",
      "4.462956428527832\n",
      "5.309920787811279\n",
      "5.3796772956848145\n",
      "3.767960786819458\n",
      "5.664869785308838\n",
      "5.2069501876831055\n",
      "5.6382832527160645\n",
      "6.12039041519165\n",
      "6.074522972106934\n",
      "5.202847957611084\n",
      "4.132767200469971\n",
      "4.948268890380859\n",
      "5.068182945251465\n",
      "4.452475070953369\n",
      "5.550425052642822\n",
      "4.85136079788208\n",
      "5.359533786773682\n",
      "5.44975471496582\n",
      "4.219789028167725\n",
      "4.911383628845215\n",
      "6.371190547943115\n",
      "5.459372520446777\n",
      "3.7417104244232178\n",
      "4.802001953125\n",
      "5.5048041343688965\n",
      "4.0523810386657715\n",
      "5.197486877441406\n",
      "4.78045129776001\n",
      "5.151343822479248\n",
      "4.98590087890625\n",
      "5.458860397338867\n",
      "4.499148368835449\n",
      "5.1656084060668945\n",
      "3.8167617321014404\n",
      "4.208345413208008\n",
      "5.477724552154541\n",
      "5.438752174377441\n",
      "5.872332572937012\n",
      "5.432761192321777\n",
      "5.280029296875\n",
      "4.3968892097473145\n",
      "5.592565059661865\n",
      "4.774909019470215\n",
      "4.747305393218994\n",
      "3.8931796550750732\n",
      "4.775512218475342\n",
      "5.37682580947876\n",
      "4.683590888977051\n",
      "5.429806232452393\n",
      "6.472513198852539\n",
      "5.522157669067383\n",
      "5.326934814453125\n",
      "4.349497318267822\n",
      "5.28593635559082\n",
      "5.402266025543213\n",
      "3.525822639465332\n",
      "4.730113983154297\n",
      "4.715023994445801\n",
      "5.692702293395996\n",
      "5.210110664367676\n",
      "5.010351657867432\n",
      "4.959136009216309\n",
      "4.635852336883545\n",
      "5.019588947296143\n",
      "4.4624552726745605\n",
      "6.184566020965576\n",
      "5.18332052230835\n",
      "5.158417224884033\n",
      "5.001770973205566\n",
      "4.680325984954834\n",
      "4.394460201263428\n",
      "5.366771697998047\n",
      "5.110700607299805\n",
      "5.0587334632873535\n",
      "5.516447067260742\n",
      "4.91188907623291\n",
      "4.885791301727295\n",
      "5.2201690673828125\n",
      "5.558029651641846\n",
      "4.269782066345215\n",
      "4.678136348724365\n",
      "5.334537982940674\n",
      "5.081141948699951\n",
      "5.799370288848877\n",
      "4.1246867179870605\n",
      "5.551836967468262\n",
      "5.251805305480957\n",
      "4.931658744812012\n",
      "5.085419178009033\n",
      "6.518763065338135\n",
      "6.023902416229248\n",
      "4.544711589813232\n",
      "4.4774370193481445\n",
      "4.468654155731201\n",
      "5.238506317138672\n",
      "4.671813488006592\n",
      "5.610466957092285\n",
      "4.850610256195068\n",
      "5.452916622161865\n",
      "4.79330587387085\n",
      "5.692015171051025\n",
      "5.050431728363037\n",
      "4.477248191833496\n",
      "4.8953351974487305\n",
      "4.819911003112793\n",
      "4.681352615356445\n",
      "4.495570659637451\n",
      "5.924680709838867\n",
      "5.041174411773682\n",
      "5.597620487213135\n",
      "5.588257312774658\n",
      "5.310380458831787\n",
      "5.292008399963379\n",
      "4.303918361663818\n",
      "5.013998031616211\n",
      "4.741250038146973\n",
      "4.334575653076172\n",
      "5.401310443878174\n",
      "4.825384616851807\n",
      "5.175565719604492\n",
      "4.432821750640869\n",
      "4.833383560180664\n",
      "4.086000442504883\n",
      "4.312560081481934\n",
      "3.37199068069458\n",
      "5.275448322296143\n",
      "4.90108060836792\n",
      "5.0184197425842285\n",
      "4.760083198547363\n",
      "4.162322044372559\n",
      "4.785065174102783\n",
      "4.9133124351501465\n",
      "5.1294450759887695\n",
      "3.858340263366699\n",
      "4.199103832244873\n",
      "4.309408664703369\n",
      "4.076613426208496\n",
      "5.038869857788086\n",
      "5.431438446044922\n",
      "5.92586088180542\n",
      "5.584958076477051\n",
      "5.190706729888916\n",
      "5.179817199707031\n",
      "4.977041721343994\n",
      "4.552811145782471\n",
      "4.489042282104492\n",
      "4.988609313964844\n",
      "4.906666278839111\n",
      "5.3543524742126465\n",
      "4.681512355804443\n",
      "4.897405624389648\n",
      "4.640043258666992\n",
      "4.832583427429199\n",
      "4.539644241333008\n",
      "5.7093892097473145\n",
      "4.4353928565979\n",
      "4.786230087280273\n",
      "4.885951042175293\n",
      "4.253942489624023\n",
      "4.543970584869385\n",
      "4.708553314208984\n",
      "4.371676445007324\n",
      "4.889729976654053\n",
      "4.47752046585083\n",
      "4.298480033874512\n",
      "5.192046165466309\n",
      "4.267175197601318\n",
      "4.079553604125977\n",
      "4.766860485076904\n",
      "3.992218255996704\n",
      "3.896338701248169\n",
      "5.822120189666748\n",
      "5.127451419830322\n",
      "4.312523365020752\n",
      "4.221405029296875\n",
      "4.093663692474365\n",
      "4.254385948181152\n",
      "5.113960266113281\n",
      "5.5514726638793945\n",
      "4.9142746925354\n",
      "4.505090713500977\n",
      "4.273721694946289\n",
      "4.132808685302734\n",
      "4.226304531097412\n",
      "3.9441943168640137\n",
      "3.3724923133850098\n",
      "5.139926910400391\n",
      "4.861825942993164\n",
      "5.884730815887451\n",
      "4.888372898101807\n",
      "3.8482837677001953\n",
      "4.29171895980835\n",
      "5.491201877593994\n",
      "5.071336269378662\n",
      "4.183904647827148\n",
      "5.213995456695557\n",
      "5.0411882400512695\n",
      "4.471054553985596\n",
      "5.093935012817383\n",
      "5.739846706390381\n",
      "3.727088212966919\n",
      "5.26678991317749\n",
      "4.275712490081787\n",
      "4.569152355194092\n",
      "3.9288668632507324\n",
      "4.704917907714844\n",
      "5.048269271850586\n",
      "5.00111198425293\n",
      "4.64829683303833\n",
      "5.233859062194824\n",
      "3.707916498184204\n",
      "4.80978536605835\n",
      "4.704864501953125\n",
      "4.434628486633301\n",
      "3.939565896987915\n",
      "4.4315948486328125\n",
      "4.682058334350586\n",
      "4.8262834548950195\n",
      "4.646474838256836\n",
      "4.5544586181640625\n",
      "4.918045997619629\n",
      "4.438453674316406\n",
      "4.848433494567871\n",
      "4.662009239196777\n",
      "4.134390830993652\n",
      "3.9410641193389893\n",
      "4.932360649108887\n",
      "4.71079158782959\n",
      "4.601119041442871\n",
      "4.814231872558594\n",
      "4.428111553192139\n",
      "4.458085536956787\n",
      "3.331404447555542\n",
      "4.60315465927124\n",
      "5.226582050323486\n",
      "4.4721360206604\n",
      "3.9737207889556885\n",
      "5.586897850036621\n",
      "4.276309013366699\n",
      "5.122282028198242\n",
      "3.927736759185791\n",
      "4.7702178955078125\n",
      "4.311150074005127\n",
      "4.297110557556152\n",
      "5.103277206420898\n",
      "3.9419379234313965\n",
      "7.372903347015381\n",
      "4.102071762084961\n",
      "4.7880635261535645\n",
      "4.203221321105957\n",
      "4.306757926940918\n",
      "4.7493696212768555\n",
      "5.061917304992676\n",
      "4.281369686126709\n",
      "4.243719577789307\n",
      "4.889030933380127\n",
      "5.008838176727295\n",
      "4.624549865722656\n",
      "4.673074245452881\n",
      "4.151601791381836\n",
      "4.921731472015381\n",
      "4.423229217529297\n",
      "4.706840991973877\n",
      "4.387340068817139\n",
      "4.423009395599365\n",
      "4.6462178230285645\n",
      "5.667232513427734\n",
      "4.819949626922607\n",
      "5.093469619750977\n",
      "4.925262928009033\n",
      "4.259034156799316\n",
      "4.703780651092529\n",
      "5.020669937133789\n",
      "4.509369850158691\n",
      "3.062737226486206\n",
      "4.142984390258789\n",
      "3.0959885120391846\n",
      "4.622593879699707\n",
      "4.393157005310059\n",
      "3.985677719116211\n",
      "4.704891681671143\n",
      "4.81308650970459\n",
      "4.951192378997803\n",
      "3.4936649799346924\n",
      "4.268042087554932\n",
      "4.648794651031494\n",
      "4.741124629974365\n",
      "5.719524383544922\n",
      "4.132901668548584\n",
      "3.6371138095855713\n",
      "5.121431827545166\n",
      "4.394814491271973\n",
      "4.495635032653809\n",
      "4.504574775695801\n",
      "4.935726165771484\n",
      "5.019296646118164\n",
      "4.411169052124023\n",
      "4.481995105743408\n",
      "4.133650779724121\n",
      "4.87380313873291\n",
      "4.370524883270264\n",
      "4.176878452301025\n",
      "4.607332706451416\n",
      "3.8344197273254395\n",
      "4.280559539794922\n",
      "3.755044937133789\n",
      "5.670968532562256\n",
      "5.552657604217529\n",
      "4.183326244354248\n",
      "4.28824520111084\n",
      "5.536715030670166\n",
      "4.266002655029297\n",
      "4.19411039352417\n",
      "6.059132099151611\n",
      "4.332251071929932\n",
      "3.9927866458892822\n",
      "4.1863226890563965\n",
      "4.6914448738098145\n",
      "4.380285739898682\n",
      "3.6727049350738525\n",
      "3.2017765045166016\n",
      "4.617480754852295\n",
      "4.1258463859558105\n",
      "4.2450714111328125\n",
      "3.4287636280059814\n",
      "4.3055548667907715\n",
      "3.532714605331421\n",
      "4.76379919052124\n",
      "4.456601619720459\n",
      "3.968337297439575\n",
      "3.906172513961792\n",
      "3.850433349609375\n",
      "5.110368728637695\n",
      "4.797332763671875\n",
      "5.18759298324585\n",
      "4.007312297821045\n",
      "3.3863539695739746\n",
      "4.900402545928955\n",
      "3.734208822250366\n",
      "3.912365198135376\n",
      "3.8225502967834473\n",
      "4.254650115966797\n",
      "4.476725101470947\n",
      "3.6067054271698\n",
      "4.895663738250732\n",
      "3.6485166549682617\n",
      "4.374007225036621\n",
      "4.714572906494141\n",
      "3.2213757038116455\n",
      "4.3936262130737305\n",
      "4.517505168914795\n",
      "3.3818368911743164\n",
      "5.01047420501709\n",
      "4.494001865386963\n",
      "4.924842834472656\n",
      "3.715595006942749\n",
      "4.745245933532715\n",
      "4.659289360046387\n",
      "4.77386999130249\n",
      "4.17119836807251\n",
      "4.332029819488525\n",
      "4.262142658233643\n",
      "4.754348278045654\n",
      "4.33411979675293\n",
      "4.801295280456543\n",
      "4.379730224609375\n",
      "3.73159122467041\n",
      "4.242073059082031\n",
      "4.03422737121582\n",
      "4.425178527832031\n",
      "4.544000625610352\n",
      "4.122198581695557\n",
      "4.641853332519531\n",
      "4.717043399810791\n",
      "3.0109546184539795\n",
      "3.969647169113159\n",
      "4.815140247344971\n",
      "4.5420756340026855\n",
      "3.978025436401367\n",
      "4.479019641876221\n",
      "3.8648743629455566\n",
      "4.6873087882995605\n",
      "4.293612957000732\n",
      "4.803051471710205\n",
      "3.643113613128662\n",
      "4.286657810211182\n",
      "4.579311370849609\n",
      "4.196491241455078\n",
      "5.375204563140869\n",
      "3.5504891872406006\n",
      "4.26226806640625\n",
      "4.092382431030273\n",
      "4.8105340003967285\n",
      "4.851036548614502\n",
      "4.675980091094971\n",
      "3.999742269515991\n",
      "3.14591908454895\n",
      "4.148385047912598\n",
      "4.4789605140686035\n",
      "4.4044294357299805\n",
      "3.9658665657043457\n",
      "3.975719928741455\n",
      "4.636819839477539\n",
      "4.951941967010498\n",
      "4.250294208526611\n",
      "4.918561935424805\n",
      "4.413440227508545\n",
      "4.668902397155762\n",
      "3.604459285736084\n",
      "3.9863040447235107\n",
      "4.9204912185668945\n",
      "5.125601768493652\n",
      "4.1385064125061035\n",
      "3.3524281978607178\n",
      "4.106828689575195\n",
      "5.014990329742432\n",
      "4.262915134429932\n",
      "4.117216110229492\n",
      "3.8983609676361084\n",
      "5.001513957977295\n",
      "5.085297107696533\n",
      "4.498516082763672\n",
      "4.717501163482666\n",
      "3.5628833770751953\n",
      "4.373404502868652\n",
      "4.195936679840088\n",
      "3.9484546184539795\n",
      "3.638418674468994\n",
      "3.277264356613159\n",
      "4.325492858886719\n",
      "4.059300899505615\n",
      "5.1374053955078125\n",
      "3.18074893951416\n",
      "4.633436679840088\n",
      "4.509119033813477\n",
      "4.701747894287109\n",
      "3.9330551624298096\n",
      "4.1986823081970215\n",
      "4.333590984344482\n",
      "4.343361854553223\n",
      "3.7630279064178467\n",
      "3.485818862915039\n",
      "4.800738334655762\n",
      "3.7111575603485107\n",
      "3.2779080867767334\n",
      "3.9543941020965576\n",
      "4.173440456390381\n",
      "4.221652030944824\n",
      "3.7074978351593018\n",
      "5.0278215408325195\n",
      "5.362935543060303\n",
      "2.5303852558135986\n",
      "4.012027263641357\n",
      "3.56008243560791\n",
      "4.288504600524902\n",
      "4.370950222015381\n",
      "2.7802093029022217\n",
      "3.4517886638641357\n",
      "3.604100227355957\n",
      "3.3380939960479736\n",
      "4.465144634246826\n",
      "4.477256774902344\n",
      "3.9412453174591064\n",
      "4.64230489730835\n",
      "3.831080675125122\n",
      "4.16613245010376\n",
      "4.270448684692383\n",
      "3.0578880310058594\n",
      "4.9703688621521\n",
      "4.756224632263184\n",
      "4.535632133483887\n",
      "4.590230464935303\n",
      "3.9840877056121826\n",
      "4.123189926147461\n",
      "4.238468647003174\n",
      "4.290414333343506\n",
      "3.093731641769409\n",
      "4.1509199142456055\n",
      "4.418704986572266\n",
      "4.327679634094238\n",
      "4.036285400390625\n",
      "4.836061954498291\n",
      "2.9022767543792725\n",
      "4.3313188552856445\n",
      "4.571550369262695\n",
      "4.4728546142578125\n",
      "4.236362934112549\n",
      "3.199112892150879\n",
      "3.9241669178009033\n",
      "3.897386074066162\n",
      "4.613434791564941\n",
      "4.185161113739014\n",
      "3.925065279006958\n",
      "4.988377571105957\n",
      "4.461446285247803\n",
      "4.10345983505249\n",
      "3.785893201828003\n",
      "4.052631378173828\n",
      "5.360662937164307\n",
      "4.0443267822265625\n",
      "4.380952835083008\n",
      "4.628726005554199\n",
      "4.342960357666016\n",
      "4.625117301940918\n",
      "3.970599412918091\n",
      "4.483315467834473\n",
      "3.3481526374816895\n",
      "4.343332290649414\n",
      "4.479838848114014\n",
      "3.722191333770752\n",
      "3.8040966987609863\n",
      "3.4496119022369385\n",
      "4.264111042022705\n",
      "4.42734432220459\n",
      "4.498323440551758\n",
      "4.079935073852539\n",
      "4.485343933105469\n",
      "4.102511882781982\n",
      "4.7311224937438965\n",
      "4.428226947784424\n",
      "4.183049201965332\n",
      "4.244636058807373\n",
      "5.250489711761475\n",
      "4.576030731201172\n",
      "3.8292932510375977\n",
      "4.445369243621826\n",
      "4.169384956359863\n",
      "3.799320697784424\n",
      "4.040163993835449\n",
      "3.5231332778930664\n",
      "3.967503786087036\n",
      "4.403815269470215\n",
      "3.8736047744750977\n",
      "3.230792760848999\n",
      "3.726780652999878\n",
      "4.437107086181641\n",
      "3.772399663925171\n",
      "4.194373607635498\n",
      "4.763137340545654\n",
      "3.6505937576293945\n",
      "3.4628641605377197\n",
      "3.69024658203125\n",
      "4.0175909996032715\n",
      "3.5197012424468994\n",
      "4.466663837432861\n",
      "4.326220512390137\n",
      "4.175256252288818\n",
      "4.307093143463135\n",
      "3.9244496822357178\n",
      "3.7712979316711426\n",
      "5.011043071746826\n",
      "4.741174697875977\n",
      "3.6721017360687256\n",
      "3.985239267349243\n",
      "3.6444194316864014\n",
      "4.301499366760254\n",
      "4.2039031982421875\n",
      "4.591366767883301\n",
      "3.5909385681152344\n",
      "3.7861552238464355\n",
      "5.076148509979248\n",
      "4.093860149383545\n",
      "4.1424946784973145\n",
      "2.7789058685302734\n",
      "3.8128983974456787\n",
      "4.144306182861328\n",
      "4.5297532081604\n",
      "4.436637878417969\n",
      "4.79390287399292\n",
      "4.35969877243042\n",
      "4.2175068855285645\n",
      "4.142255783081055\n",
      "2.4797074794769287\n",
      "3.424302339553833\n",
      "4.055841445922852\n",
      "3.811124563217163\n",
      "3.865966796875\n",
      "4.39061975479126\n",
      "4.220418930053711\n",
      "3.8117189407348633\n",
      "3.9197351932525635\n",
      "4.392367839813232\n",
      "3.325699806213379\n",
      "4.510807991027832\n",
      "3.6119120121002197\n",
      "3.9340012073516846\n",
      "4.00651741027832\n",
      "5.396156311035156\n",
      "3.110491991043091\n",
      "4.188948154449463\n",
      "3.7603330612182617\n",
      "4.305967330932617\n",
      "4.488309383392334\n",
      "5.538330078125\n",
      "3.3731765747070312\n",
      "3.6834278106689453\n",
      "3.7668848037719727\n",
      "3.8177027702331543\n",
      "3.946823835372925\n",
      "4.566890716552734\n",
      "3.8390822410583496\n",
      "4.709282875061035\n",
      "4.56154203414917\n",
      "3.920564889907837\n",
      "3.241434097290039\n",
      "3.546802282333374\n",
      "3.6206157207489014\n",
      "3.7864863872528076\n",
      "3.8462512493133545\n",
      "4.223905086517334\n",
      "4.0857086181640625\n",
      "3.6958794593811035\n",
      "4.022898197174072\n",
      "3.6428487300872803\n",
      "3.697608232498169\n",
      "4.581442832946777\n",
      "4.021773338317871\n",
      "3.8372912406921387\n",
      "4.571666240692139\n",
      "3.829486131668091\n",
      "3.441150665283203\n",
      "3.6517698764801025\n",
      "4.292393684387207\n",
      "4.078139781951904\n",
      "3.1843748092651367\n",
      "3.7454066276550293\n",
      "3.6411399841308594\n",
      "4.294926166534424\n",
      "3.8582260608673096\n",
      "4.065533638000488\n",
      "4.322475433349609\n",
      "3.8876073360443115\n",
      "4.0208258628845215\n",
      "3.6522207260131836\n",
      "3.754844903945923\n",
      "3.474560022354126\n",
      "3.897862672805786\n",
      "3.6858339309692383\n",
      "4.545587062835693\n",
      "3.529696226119995\n",
      "2.8393359184265137\n",
      "3.7060208320617676\n",
      "4.399806499481201\n",
      "4.552192687988281\n",
      "4.255984306335449\n",
      "4.016759872436523\n",
      "3.625831127166748\n",
      "3.3518853187561035\n",
      "4.29353666305542\n",
      "3.2514617443084717\n",
      "3.471698522567749\n",
      "3.857684373855591\n",
      "3.778233051300049\n",
      "3.8609557151794434\n",
      "3.220532178878784\n",
      "4.027662754058838\n",
      "3.9749789237976074\n",
      "4.608859539031982\n",
      "4.066253662109375\n",
      "4.704662322998047\n",
      "4.302533149719238\n",
      "4.263815879821777\n",
      "3.557234048843384\n",
      "4.829176425933838\n",
      "3.622495174407959\n",
      "2.4797680377960205\n",
      "4.356051445007324\n",
      "3.8928182125091553\n",
      "3.6973631381988525\n",
      "3.6811306476593018\n",
      "3.018761157989502\n",
      "4.308478832244873\n",
      "3.288642406463623\n",
      "5.292236328125\n",
      "3.486827850341797\n",
      "3.7694032192230225\n",
      "4.51907205581665\n",
      "4.186308860778809\n",
      "3.453352451324463\n",
      "5.641413688659668\n",
      "2.3495795726776123\n",
      "3.8703227043151855\n",
      "3.7028074264526367\n",
      "2.5017807483673096\n",
      "4.795068740844727\n",
      "3.7880818843841553\n",
      "3.1056838035583496\n",
      "1.8197818994522095\n",
      "4.091830253601074\n",
      "4.003237247467041\n",
      "3.412205457687378\n",
      "3.2250871658325195\n",
      "4.508213996887207\n",
      "4.324937343597412\n",
      "4.12956428527832\n",
      "3.198347806930542\n",
      "4.062714576721191\n",
      "4.020719528198242\n",
      "4.229344367980957\n",
      "3.664598226547241\n",
      "3.740163803100586\n",
      "4.019469738006592\n",
      "3.9562225341796875\n",
      "3.671119213104248\n",
      "3.932027578353882\n",
      "3.6534948348999023\n",
      "4.141721725463867\n",
      "3.853605031967163\n",
      "3.4653306007385254\n",
      "4.126978397369385\n",
      "3.931668281555176\n",
      "4.3219523429870605\n",
      "3.3943068981170654\n",
      "4.1223673820495605\n",
      "3.297027826309204\n",
      "3.9820597171783447\n",
      "3.500579357147217\n",
      "3.761293649673462\n",
      "4.243106842041016\n",
      "4.091506481170654\n",
      "4.122265338897705\n",
      "3.3765709400177\n",
      "4.614431858062744\n",
      "4.184084415435791\n",
      "3.4628005027770996\n",
      "2.754215717315674\n",
      "4.568009376525879\n",
      "3.366093635559082\n",
      "3.605772018432617\n",
      "3.6651949882507324\n",
      "3.279453992843628\n",
      "3.399399518966675\n",
      "3.2980282306671143\n",
      "2.5693602561950684\n",
      "3.987048387527466\n",
      "3.667112112045288\n",
      "4.843852519989014\n",
      "3.6833784580230713\n",
      "3.264045238494873\n",
      "3.4469170570373535\n",
      "4.17556619644165\n",
      "3.9998772144317627\n",
      "3.918137550354004\n",
      "2.993241786956787\n",
      "4.472402095794678\n",
      "4.45728063583374\n",
      "3.6219611167907715\n",
      "4.032745361328125\n",
      "3.2302520275115967\n",
      "4.0369696617126465\n",
      "4.5954437255859375\n",
      "4.0077738761901855\n",
      "3.125786542892456\n",
      "3.9569365978240967\n",
      "4.275812149047852\n",
      "3.068316698074341\n",
      "3.6098673343658447\n",
      "3.2976996898651123\n",
      "4.362468719482422\n",
      "5.298332691192627\n",
      "3.916520118713379\n",
      "4.779604434967041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [82], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m train_settings \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mcreate(train_settings)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m train_losses, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtrain_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_losses)\n",
      "Cell \u001b[1;32mIn [79], line 30\u001b[0m, in \u001b[0;36mtrain_main\u001b[1;34m(model_config, train_settings)\u001b[0m\n\u001b[0;32m     18\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m create_data_loader(data\u001b[38;5;241m=\u001b[39mtext_data, \n\u001b[0;32m     19\u001b[0m                                   tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m     22\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m create_data_loader(data\u001b[38;5;241m=\u001b[39mtext_data,\n\u001b[0;32m     23\u001b[0m                                   tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     24\u001b[0m                                   batch_size\u001b[38;5;241m=\u001b[39mtrain_settings\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m                                     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m train_losses, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, model\n",
      "Cell \u001b[1;32mIn [78], line 26\u001b[0m, in \u001b[0;36mtrain_model_epoch\u001b[1;34m(model, train_loader, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_batch_loss(input_batch, target_batch, model, device)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m tokens_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32mc:\\Users\\yangy\\anaconda3\\envs\\pytorch_latest\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yangy\\anaconda3\\envs\\pytorch_latest\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_config = {\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"bos_token_id\": 151643,\n",
    "        \"eos_token_id\": 151643,\n",
    "        \"pad_token_id\": 151643,\n",
    "        \"hidden_act\": \"silu\",\n",
    "        \"hidden_size\": 896,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 4864,\n",
    "        \"max_position_embeddings\": 32768,\n",
    "        \"max_window_layers\": 24,\n",
    "        \"model_type\": \"qwen2\",\n",
    "        \"num_attention_heads\": 14,\n",
    "        \"num_hidden_layers\": 24,\n",
    "        \"num_key_value_heads\": 2,\n",
    "        \"rms_norm_eps\": 1e-06,\n",
    "        \"rope_theta\": 1000000.0,\n",
    "        \"tie_word_embeddings\": True,\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"transformers_version\": \"4.47.1\",\n",
    "        \"use_cache\": True,\n",
    "        \"use_mrope\": False,\n",
    "        \"vocab_size\": 151936,\n",
    "        \"qkv_bias\": True,\n",
    "        \"o_bias\": False,\n",
    "        \"mlp_bias\": False\n",
    "    }\n",
    "\n",
    "    model_config = OmegaConf.create(model_config)\n",
    "    train_settings = {\n",
    "        \"pretrained_model_name\": \"Qwen/Qwen2.5-0.5B\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"stride\": 128,\n",
    "        \"seed\": 1,\n",
    "        \"file_path\":\"./instruction_data/instruction-data2.json\",\n",
    "        \"url\":\"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
    "    }\n",
    "    \n",
    "    train_settings = OmegaConf.create(train_settings)\n",
    "    \n",
    "    # train model\n",
    "    train_losses, model = train_main(model_config=model_config,\n",
    "                       train_settings=train_settings)\n",
    "    \n",
    "    print(train_losses)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
