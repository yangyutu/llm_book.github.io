{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab DPO Training\n",
    "\n",
    "Here we present a simplified llama implementation based [Huggingface implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L731) to illustrate different components on the Llama decoder model.\n",
    "\n",
    "The key components are\n",
    "* RMS Norm\n",
    "* Rotary Position Embedding\n",
    "* Grouped Query Attention\n",
    "* Feedfoward network (FFN)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "RMSNorm  is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula\n",
    "\n",
    "$$\n",
    "\\operatorname{RMSNorm}(x)=\\frac{x}{\\sqrt{\\frac{1}{H} \\sum_{i=1}^H x_i^2}} \\cdot \\gamma\n",
    "$$(chapter_LLM_arch_RMS_nomalization_formula)\n",
    "\n",
    "where $\\gamma$ is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # float32 is needed for numeric stability. float16 is not enough.\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # The variance of the hidden_states is computed along the last dimension using the pow(2).\n",
    "        # mean(-1, keepdim=True) operations, which square the values, compute the mean, and \n",
    "        # retain the dimensions for broadcasting.\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        \n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.gamma * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "\n",
    "Rotary position embedding consists of pre-computing cosine, sine at different frequences (from 0 to 1/(10000)) and different position ids (from 0 to max_seq_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        self.original_max_seq_len = max_position_embeddings\n",
    "\n",
    "\n",
    "        #inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        \n",
    "        # inv freq is a tensor of shape (dim // 2)\n",
    "        # (0, 1/10000^(2/dim),..., 1/10000^((dim-2)/dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        # Use None to add two new dimensions to the inv_freq\n",
    "        # use expand to repeat the inv_freq along the batch dimension\n",
    "        # inv_freq_expanded has shape (batch_size, dim // 2, 1), dim // 2 is the number of frequencies\n",
    "        # position_ids_expanded has shape (batch_size, 1, seq_len)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        # inv_freq_expanded.float() @ position_ids_expanded.float() gives shape (batch_size, dim // 2, seq_len)\n",
    "        # after transpose, we get (batch_size, seq_len, dim // 2)\n",
    "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "        # emb has shape (batch_size, seq_len, dim), the concat is on the frequency dimension\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # x1 is the first half of the hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # x2 is the second half of the hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# q (`torch.Tensor`): The query tensor, which has shape [batch_size, heads, seq_len, head_dim].\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    # add a dimension to the cos and sin tensors to account for the number of heads\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    # Here has a different order in the frequency dimension, as described in the paper https://arxiv.org/pdf/2104.09864 page 7\n",
    "    # in the paper, the order is \n",
    "    # [cos m theta 1, cos m theta 1, ..., cos m theta (d//2), cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 1, ..., sin m theta (d//2), sin m theta (d//2)]\n",
    "    # here the order is\n",
    "    # [cos m theta 1, cos m theta 2, ...cos m theta (d//2), cos m theta 1, cos m theta 2, ...cos m theta (d//2)]\n",
    "    # and [sin m theta 1, sin m theta 2, ...sin m theta (d//2), sin m theta 1, sin m theta 2, ...sin m theta (d//2)]\n",
    "    # that is, the frequency order is permuted\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "Attention layer implements the grouped query attention; Note that the rotary position encoding are implemented by rotating the query encoding and key encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for Group query attention\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, seqlen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, seqlen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, seqlen, head_dim)\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # Here supports GQA, which specifies the number of key value heads << num_heads\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.o_bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  \n",
    "    ):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "\n",
    "        # projetion of the hidden states into query, key and value\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Get the rotary embeddings cosines and sines functions\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # apply the rotary embeddings to the query and key states\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # Copy kv for matching the number of heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        # applied scaled dot product attention\n",
    "        # attn_weights has shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        # attn_output has shape (batch_size,  seq_len, num_heads, head_dim) after transpose\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # attn_output output has shape (batch_size, seq_len, num_heads * head_dim) after reshape\n",
    "        # which is equivalent to concatenating the heads\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        # apply the output projection\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN Layer\n",
    "\n",
    "Llama uses Swish function in the GLU, we can obtain the following variations:\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}_{SwiGLU} = (\\text{Swish}_1(\\underbrace{xW_1}_{\\text{Gate Projection}})\\otimes \\underbrace{xV}_{\\text{Up Projection}} ) \\underbrace{W_2}_{\\text{Down Projection}}\n",
    "$$\n",
    "\n",
    "with $\\operatorname{Swish}_1(x)=x \\cdot \\sigma(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        # silu is the same as swish\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama Decoder Layer\n",
    "\n",
    "Each decoder layer has\n",
    "* Two Pre-RMSNorm layers, one before the self-attention sublayer and one before the FFN layer\n",
    "* GQA attention layer\n",
    "* FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "        # FFN layer\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        # pre layer norm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        # pre layer norm before FFN layer\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Decoder layers\n",
    "\n",
    "In the stacked decoder layer, \n",
    "* There are L decoder layers\n",
    "* Rotary embeddings (i.e., elements in the rotation matrices) are shared across layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        # apply to last layer hidden state\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # rotary embedding matrices are shared across the decoder layers\n",
    "        self.rotary_emb = LlamaRotaryEmbedding( dim=config.hidden_size // config.num_attention_heads,\n",
    "                                                max_position_embeddings=config.max_position_embeddings,\n",
    "                                                base=config.rope_theta,)\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ):\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.shape[1], dtype=torch.int64, device=hidden_states.device)\n",
    "            position_ids = position_ids.expand(input_ids.shape[0], -1)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder for language modeling\n",
    "\n",
    "Decoder with language modeling is the previous stacked decoder layer plus a linear layer as language prediction head. The langauge prediciton head linearly transforms the hidden state into the logits distributed over the vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    #_tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "    ):\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  from omegaconf import OmegaConf\n",
    "  model_config = {\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"bos_token_id\": 151643,\n",
    "    \"eos_token_id\": 151643,\n",
    "    \"pad_token_id\": 151643,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 896,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4864,\n",
    "    \"max_position_embeddings\": 32768,\n",
    "    \"max_window_layers\": 24,\n",
    "    \"model_type\": \"qwen2\",\n",
    "    \"num_attention_heads\": 14,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"rope_theta\": 1000000.0,\n",
    "    \"tie_word_embeddings\": True,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"transformers_version\": \"4.47.1\",\n",
    "    \"use_cache\": True,\n",
    "    \"use_mrope\": False,\n",
    "    \"vocab_size\": 151936,\n",
    "    \"qkv_bias\": True,\n",
    "    \"o_bias\": False,\n",
    "    \"mlp_bias\": False\n",
    "  }\n",
    "\n",
    "  model_config = OmegaConf.create(model_config)\n",
    "  custom_model = LlamaForCausalLM(model_config)\n",
    "\n",
    "  # load model weight from Huggingface\n",
    "  import transformers\n",
    "  from transformers import AutoModelForCausalLM\n",
    "\n",
    "  model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  custom_model.load_state_dict(model.state_dict(), strict=False)\n",
    "\n",
    "  # test input\n",
    "  input_ids = torch.LongTensor([[1, 2, 3]])\n",
    "  custom_model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text + \"\\n\\n### Response:\\n\"\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.prompt_with_completions = []\n",
    "        self.completions = []\n",
    "        \n",
    "        for entry in data:\n",
    "            \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            completion = entry['output']\n",
    "            \n",
    "            self.prompt_with_completions.append(instruction_plus_input + completion)\n",
    "            self.completions.append(completion)\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_with_completions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompt_with_completions[idx], self.completions[idx]\n",
    "\n",
    "def custom_collate_fn(batch, tokenizer, ignore_idx=-100):\n",
    "    \n",
    "    prompt_with_completions, completions = zip(*batch)\n",
    "    \n",
    "    padded = tokenizer(list(prompt_with_completions), padding='longest', truncation=True, return_tensors='pt')\n",
    "    padded = padded['input_ids']\n",
    "    inputs = padded[:,:-1]\n",
    "    targets = padded[:,1:]\n",
    "    \n",
    "    mask = (targets == tokenizer.pad_token_id)\n",
    "    \n",
    "    targets = targets.masked_fill(mask, ignore_idx)\n",
    "    \n",
    "    return inputs, targets\n",
    "    \n",
    "     \n",
    "def create_data_loader(data, tokenizer, batch_size=4, max_length=256, \n",
    "                       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    dataset = InstructionDataset(data=data)\n",
    "    \n",
    "    collate_fn = partial(custom_collate_fn, tokenizer=tokenizer, ignore_idx=-100)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=drop_last,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_data(file_path, url):\n",
    "    import urllib\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  31115,    264,  15908,  40158,   6524,    382,\n",
      "          14374,   5571,    510,     40,   1079,  14589,    369,    537,  33094,\n",
      "            847,  16319,    389,    882,    382,  14374,   5949,    510,  30665,\n",
      "            508,    675,  49088,     40,  36879,    369,  33094,    847,  16319,\n",
      "           3309,     13,    358,   3535,    429,  33094,    975,    389,    882,\n",
      "            374,   1376,    311,    697,   6950,    304,    752,     13,    358,\n",
      "           1079,   8480,    369,    279,   7626,    323,    358,   1896,   2480,\n",
      "          38142,    369,    432,     13,    358,  58283,  22231,    894,  60009,\n",
      "           8881,    553,    847,   6168,    382,     40,  15440,    429,    358,\n",
      "            686,   1896,    678,   5871,   7354,    311,   5978,    429,   1741,\n",
      "            458,  10455,   1558,    537,   3537,   1549,     13,   9063,  27378,\n",
      "             11,    358,    686,    653,    847,   1850,    311,   1281,    705,\n",
      "            369,    847,  20643,    382,     40,   1401,   4637,    311,    279,\n",
      "           6638,    315,  49103,    697,   6950,    304,    752,    382,     50,\n",
      "          86091,    345,     58,   7771,   3988],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  60424,   2326,  10295,    315,  65060,    315,\n",
      "            220,     21,    382,  14374,   5949,    510,  19641,  10295,    315,\n",
      "          65060,    315,    220,     21,    525,    220,     21,     11,    220,\n",
      "             16,     17,     11,    323,    220,     16,     23,     13, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  22550,   1493,   5837,   4092,    311,    862,\n",
      "          42807,  51749,   5643,    271,  14374,   5571,    510,  24347,     11,\n",
      "           9856,     11,   9625,     11,   6747,    271,  14374,   5949,    510,\n",
      "          24347,     11,   9856,     11,   6747,     11,   9625, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,   4340,   2310,    374,    279,  88575,    315,\n",
      "          31392,   1939,  14374,   5949,    510,    785,  88575,    315,  31392,\n",
      "            374,    916,    220,     16,     18,     19,   1635,   2310,     11,\n",
      "           3432,   1012,   8145,    304,   6527,    220,     16,     23,     23,\n",
      "             21,     13, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643]]), tensor([[  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         31115,   264, 15908, 40158,  6524,   382, 14374,  5571,   510,    40,\n",
      "          1079, 14589,   369,   537, 33094,   847, 16319,   389,   882,   382,\n",
      "         14374,  5949,   510, 30665,   508,   675, 49088,    40, 36879,   369,\n",
      "         33094,   847, 16319,  3309,    13,   358,  3535,   429, 33094,   975,\n",
      "           389,   882,   374,  1376,   311,   697,  6950,   304,   752,    13,\n",
      "           358,  1079,  8480,   369,   279,  7626,   323,   358,  1896,  2480,\n",
      "         38142,   369,   432,    13,   358, 58283, 22231,   894, 60009,  8881,\n",
      "           553,   847,  6168,   382,    40, 15440,   429,   358,   686,  1896,\n",
      "           678,  5871,  7354,   311,  5978,   429,  1741,   458, 10455,  1558,\n",
      "           537,  3537,  1549,    13,  9063, 27378,    11,   358,   686,   653,\n",
      "           847,  1850,   311,  1281,   705,   369,   847, 20643,   382,    40,\n",
      "          1401,  4637,   311,   279,  6638,   315, 49103,   697,  6950,   304,\n",
      "           752,   382,    50, 86091,   345,    58,  7771,  3988,    60],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         60424,  2326, 10295,   315, 65060,   315,   220,    21,   382, 14374,\n",
      "          5949,   510, 19641, 10295,   315, 65060,   315,   220,    21,   525,\n",
      "           220,    21,    11,   220,    16,    17,    11,   323,   220,    16,\n",
      "            23,    13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         22550,  1493,  5837,  4092,   311,   862, 42807, 51749,  5643,   271,\n",
      "         14374,  5571,   510, 24347,    11,  9856,    11,  9625,    11,  6747,\n",
      "           271, 14374,  5949,   510, 24347,    11,  9856,    11,  6747,    11,\n",
      "          9625,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "          4340,  2310,   374,   279, 88575,   315, 31392,  1939, 14374,  5949,\n",
      "           510,   785, 88575,   315, 31392,   374,   916,   220,    16,    18,\n",
      "            19,  1635,  2310,    11,  3432,  1012,  8145,   304,  6527,   220,\n",
      "            16,    23,    23,    21,    13,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]]))\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "def test_data_component():\n",
    "    file_path = \"instruction-data2.json\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"        \n",
    "    )\n",
    "\n",
    "    text_data = read_text_data(file_path, url)\n",
    "    tokenizer_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    #tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    for batch in train_loader:\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "test_data_component()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(\n",
    "    policy_chosen_logps: torch.FloatTensor, \n",
    "    policy_rejected_logps: torch.FloatTensor,\n",
    "    reference_chosen_logps: torch.FloatTensor,\n",
    "    reference_rejected_logps: torch.FloatTensor,\n",
    "    beta: float,\n",
    "    label_smoothing: float = 0.0,\n",
    "    ipo: bool = False,\n",
    "    reference_free: bool = False,\n",
    ") -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy_chosen_logps: log probabilities of the policy model for the chosen responses, shape: (batch_size,)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "    ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "    \n",
    "    logits = pi_logratios - ref_logratios\n",
    "    \n",
    "    if reference_free:\n",
    "        ref_logratios = 0.0\n",
    "    \n",
    "    if ipo:\n",
    "        losses = (logits - 1 / (2 * beta)) ** 2\n",
    "    else:\n",
    "        losses = - F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(-beta * logits) * label_smoothing\n",
    "        \n",
    "    \n",
    "    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "def _get_batch_logps(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    loss_mask: torch.LongTensor,\n",
    "    average_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: logits of the model output. Shape: (batch_size, seq_length, vocab_size)\n",
    "        labels: labels for which token's log probability; label = -100 indicates ignore. Shape (batch_size, seq_length)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    assert logits.shape[:-1] == labels.shape\n",
    "    # let the sequence be A, B, C, D\n",
    "    # labels[:,1：] are B, C, D\n",
    "    # logits corresponds to B, C, D, X\n",
    "    # logits[:,:-1,:] corresponds to B, C, D\n",
    "    labels = labels[:,1:].clone() # labels \n",
    "    logits = logits[:,:-1,:]\n",
    "    \n",
    "    loss_mark = loss_mask[:, 1:]\n",
    "    \n",
    "    # shape (batch_size, seq_len - 1)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsequeeze(2)).squeeze(2)\n",
    "    \n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * loss_mark).sum(-1) / loss_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * loss_mask).sum(-1)\n",
    "    \n",
    "def get_logps(outputs, labels, input_mask) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "    \n",
    "    all_logits = outputs.logits.to(torch.float32)\n",
    "    all_logps = _get_batch_logps(all_logits, labels, input_mask, average_log_prob=False)\n",
    "    \n",
    "    batch_size = all_logps.shape[0]\n",
    "    \n",
    "    chosen_logps = all_logps[: batch_size // 2]\n",
    "    rejected_logps = all_logps[batch_size//2:]\n",
    "    \n",
    "    return chosen_logps, rejected_logps\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    flat_targets = target_batch.flatten() \n",
    "    flat_logits = logits.flatten(0, 1)# flatten the first two dimensions \n",
    "    loss = F.cross_entropy(flat_logits, flat_targets) # tokens with ignore idx will not contribute to loss \n",
    "    return loss\n",
    "\n",
    "def train_model_epoch(model, \n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                num_epochs):\n",
    "    \n",
    "    train_losses, val_losses, track_token_seen = [],[],[]\n",
    "    tokens_seen = 0\n",
    "    global_steps = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_steps += 1\n",
    "            train_losses.append(loss.detach().item())\n",
    "            print(train_losses[-1])\n",
    "        \n",
    "    return train_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dpo(model, ref_model, optimizer, train_loader, train_settings):\n",
    "    \n",
    "    for epoch in range(train_settings.num_epochs):\n",
    "        \n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            \n",
    "            model_outputs = model(batch)\n",
    "            \n",
    "            reference_outputs = ref_model(batch)\n",
    "            \n",
    "            policy_chosen_logps, policy_rejected_logps = get_logps(model_outputs, batch['labels'])\n",
    "            \n",
    "            reference_chosen_logps, reference_rejected_logps = get_logps(reference_outputs, batch['labels'])\n",
    "            \n",
    "            loss_kwargs = {\"beta\": 0.1, \"reference_free\": False}\n",
    "            \n",
    "            losses, chosen_rewards, rejected_rewards = preference_loss(\n",
    "                policy_chosen_logps=policy_chosen_logps,\n",
    "                policy_rejected_logps=policy_rejected_logps,\n",
    "                reference_chosen_logps=reference_chosen_logps,\n",
    "                reference_rejected_logps=reference_rejected_logps,\n",
    "                **loss_kwargs\n",
    "            )\n",
    "    \n",
    "            loss = losses.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model_config, train_settings):\n",
    "    \n",
    "    torch.manual_seed(train_settings.seed)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    text_data = read_text_data(train_settings.file_path, train_settings.url)\n",
    "            \n",
    "    model = LlamaForCausalLM(config=model_config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=train_settings.learning_rate,\n",
    "                                  weight_decay=train_settings.weight_decay)\n",
    "    \n",
    "    # set up dataloader\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(train_settings.pretrained_model_name)\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    \n",
    "    \n",
    "    train_loader = create_data_loader(data=text_data,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      drop_last=True,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0\n",
    "    )\n",
    "        \n",
    "    train_losses, model = train_model_epoch(model=model,\n",
    "                train_loader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=train_settings.num_epochs,\n",
    "                device=device)\n",
    "    \n",
    "    return train_losses, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.114983558654785\n",
      "11.961651802062988\n",
      "11.927282333374023\n",
      "11.826778411865234\n",
      "11.707758903503418\n",
      "11.793996810913086\n",
      "11.565061569213867\n",
      "11.341635704040527\n",
      "11.44568920135498\n",
      "11.51699161529541\n",
      "11.538795471191406\n",
      "11.205238342285156\n",
      "11.476774215698242\n",
      "11.2799654006958\n",
      "10.608583450317383\n",
      "10.964029312133789\n",
      "11.31577205657959\n",
      "10.75229549407959\n",
      "10.765582084655762\n",
      "10.845158576965332\n",
      "10.3082857131958\n",
      "10.781938552856445\n",
      "11.183990478515625\n",
      "10.394356727600098\n",
      "10.934983253479004\n",
      "9.947456359863281\n",
      "10.557378768920898\n",
      "10.630148887634277\n",
      "10.455920219421387\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_config = {\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"bos_token_id\": 151643,\n",
    "        \"eos_token_id\": 151643,\n",
    "        \"pad_token_id\": 151643,\n",
    "        \"hidden_act\": \"silu\",\n",
    "        \"hidden_size\": 896,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 4864,\n",
    "        \"max_position_embeddings\": 32768,\n",
    "        \"max_window_layers\": 24,\n",
    "        \"model_type\": \"qwen2\",\n",
    "        \"num_attention_heads\": 14,\n",
    "        \"num_hidden_layers\": 24,\n",
    "        \"num_key_value_heads\": 2,\n",
    "        \"rms_norm_eps\": 1e-06,\n",
    "        \"rope_theta\": 1000000.0,\n",
    "        \"tie_word_embeddings\": True,\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"transformers_version\": \"4.47.1\",\n",
    "        \"use_cache\": True,\n",
    "        \"use_mrope\": False,\n",
    "        \"vocab_size\": 151936,\n",
    "        \"qkv_bias\": True,\n",
    "        \"o_bias\": False,\n",
    "        \"mlp_bias\": False\n",
    "    }\n",
    "\n",
    "    model_config = OmegaConf.create(model_config)\n",
    "    train_settings = {\n",
    "        \"pretrained_model_name\": \"Qwen/Qwen2.5-0.5B\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"stride\": 128,\n",
    "        \"seed\": 1,\n",
    "        \"file_path\":\"./instruction_data/instruction-data2.json\",\n",
    "        \"url\":\"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
    "    }\n",
    "    \n",
    "    train_settings = OmegaConf.create(train_settings)\n",
    "    \n",
    "    # train model\n",
    "    train_losses, model = train_main(model_config=model_config,\n",
    "                       train_settings=train_settings)\n",
    "    \n",
    "    print(train_losses)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
