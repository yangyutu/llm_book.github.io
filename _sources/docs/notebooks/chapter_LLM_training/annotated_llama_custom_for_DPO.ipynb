{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: DPO Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "from llm_lab.utils.common_utils import move_to_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Oh, you like [insert interest]? Me too! What do you love about it?',\n",
       " 'chosen': \"Yeah! I'm super passionate about music! üéµ There's just something about how a good song can evoke emotions and transport you to a different time and place, you know? üï∞Ô∏è I love how it can bring people together, too. What about you? What kind of music are you into? üé∂ Do you have a favorite artist or genre? ü§î\",\n",
       " 'rejected': \"Good day. As a digital entity, I don't have a physical presence or a circadian rhythm, so I neither wake up early nor stay up late. I am designed to operate 24/7, providing assistance and responding to inquiries at any time. My purpose is to provide accurate and helpful information, and I do not have personal preferences or experiences.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"MiniLLM/MiniLLM-gpt2-120M\" # a tiny model for fast debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e1626c3f254997adf159f8b5fe562a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debb593b69a54de8991b3dec0bcb3742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86a0a0c341a4c0bbcc663506ff93cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce293f6af9264965a4e625ab29c18c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c9c36ec8b84b45a9cc836ad8d5fa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    result_dict = {}\n",
    "    prompt_tokenized = tokenizer(example['prompt'])\n",
    "    chosen_encoded = tokenizer(example['chosen'])\n",
    "    rejected_encoded = tokenizer(example['rejected'])\n",
    "    \n",
    "    result_dict['chosen_encoded'] = {'input_ids': prompt_tokenized['input_ids'] + chosen_encoded['input_ids'],\n",
    "                                   'attention_mask': prompt_tokenized['attention_mask'] + chosen_encoded['attention_mask'],\n",
    "                                   'loss_mask': [0] * len(prompt_tokenized['input_ids']) + [1] * len(chosen_encoded['input_ids']),\n",
    "                                   }\n",
    "    result_dict['rejected_encoded'] = {'input_ids': prompt_tokenized['input_ids'] + rejected_encoded['input_ids'],\n",
    "                                   'attention_mask': prompt_tokenized['attention_mask'] + rejected_encoded['attention_mask'],\n",
    "                                   'loss_mask': [0] * len(prompt_tokenized['input_ids']) + [1] * len(rejected_encoded['input_ids']),\n",
    "                                   }\n",
    "    example.update(result_dict)\n",
    "    return example\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb20d042f164a0f874385075d52900f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize, remove_columns=['prompt','chosen','rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer, ignore_idx=-100):\n",
    "    \n",
    "    max_len = max([len(e[type]['input_ids']) for type in ['chosen_encoded','rejected_encoded'] for e in batch])\n",
    "    result_dict = {}\n",
    "    \n",
    "    for type in ['chosen_encoded','rejected_encoded']:\n",
    "        if type not in result_dict:\n",
    "            result_dict[type] = collections.defaultdict(list)\n",
    "        for e in batch:\n",
    "            needed = max_len - len(e[type]['input_ids'])\n",
    "            e[type]['input_ids'] += [tokenizer.pad_token_id] * needed\n",
    "            e[type]['attention_mask'] += [0] * needed\n",
    "            e[type]['loss_mask'] += [0] * needed\n",
    "            result_dict[type]['input_ids'].append(e[type]['input_ids'])\n",
    "            result_dict[type]['attention_mask'].append(e[type]['attention_mask'])\n",
    "            result_dict[type]['loss_mask'].append(e[type]['loss_mask'])\n",
    "    \n",
    "    for type in ['chosen_encoded','rejected_encoded']:\n",
    "        for key in result_dict[type]:\n",
    "            result_dict[type][key] = torch.LongTensor(result_dict[type][key])\n",
    "            \n",
    "    return result_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenized_dataset['train'].train_test_split(test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     #print(batch)\n",
    "#     chosen_batch = move_to_device(batch['chosen_encoded'], device)\n",
    "    \n",
    "#     chosen_logits = model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits    \n",
    "#     print(chosen_logits)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Learning utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(\n",
    "    chosen_log_probs: torch.FloatTensor, \n",
    "    rejected_log_probs: torch.FloatTensor,\n",
    "    reference_chosen_log_probs: torch.FloatTensor,\n",
    "    reference_rejected_log_probs: torch.FloatTensor,\n",
    "    beta: float = 0.1, # suggested value in the DPO paper\n",
    ") -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        chosen_log_probs: log probabilities of the policy model for the chosen responses, shape: (batch_size,)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    pi_logratios = chosen_log_probs - rejected_log_probs\n",
    "    ref_logratios = reference_chosen_log_probs - reference_rejected_log_probs\n",
    "    \n",
    "    logratios_difference = pi_logratios - ref_logratios\n",
    "    \n",
    "    losses = - F.logsigmoid(beta * logratios_difference)\n",
    "        \n",
    "    chosen_rewards = beta * (chosen_log_probs - reference_chosen_log_probs).detach()\n",
    "    rejected_rewards = beta * (rejected_log_probs - reference_rejected_log_probs).detach()\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "def _get_squence_log_probs(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    loss_mask: torch.LongTensor,\n",
    "    average_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: logits of the model output. Shape: (batch_size, seq_length, vocab_size)\n",
    "        labels: labels for which token's log probability; label = -100 indicates ignore. Shape (batch_size, seq_length)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert logits.shape[:-1] == labels.shape\n",
    "    #assert labels.shape == loss_mask.shape\n",
    "    # let the sequence be A, B, C, D\n",
    "    # labels[:,1Ôºö] are B, C, D\n",
    "    # logits corresponds to B, C, D, X\n",
    "    # logits[:,:-1,:] corresponds to B, C, D\n",
    "    labels = labels[:,1:].clone() # labels \n",
    "    logits = logits[:,:-1,:]\n",
    "    loss_mask = loss_mask[:, 1:]\n",
    "    \n",
    "    \n",
    "    # log_probs shape (batch_size, seq_len - 1, vocab_size)\n",
    "    # label shape before unsqueeze - (batch_size, seq_len - 1), after - (batch_size, seq_len - 1, vocab_size)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    # per_token_logps shape (batch_size, seq_len - 1)\n",
    "    per_token_logps = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(2) # squeeze on the last dim\n",
    "    \n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * loss_mask).sum(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, model, ref_model, device):\n",
    "    \n",
    "    assert model.training == True\n",
    "    assert ref_model.training == False\n",
    "    \n",
    "    chosen_batch = batch['chosen_encoded']\n",
    "    rejected_batch = batch['rejected_encoded']\n",
    "    \n",
    "    chosen_batch = move_to_device(chosen_batch, device)\n",
    "    rejected_batch = move_to_device(rejected_batch, device)\n",
    "    \n",
    "    chosen_logits = model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits\n",
    "    rejected_logits = model(input_ids=rejected_batch['input_ids'], attention_mask=rejected_batch['attention_mask']).logits\n",
    "    \n",
    "    chosen_sequence_log_probs = _get_squence_log_probs(chosen_logits, labels = chosen_batch['input_ids'], loss_mask = chosen_batch['loss_mask'])\n",
    "    rejected_sequence_log_probs = _get_squence_log_probs(rejected_logits, labels = rejected_batch['input_ids'], loss_mask = rejected_batch['loss_mask'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        chosen_logits_ref = ref_model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits\n",
    "        rejected_logits_ref = ref_model(input_ids=rejected_batch['input_ids'], attention_mask=rejected_batch['attention_mask']).logits\n",
    "    \n",
    "        chosen_sequence_log_probs_ref = _get_squence_log_probs(chosen_logits_ref, labels = chosen_batch['input_ids'], loss_mask = chosen_batch['loss_mask'])\n",
    "        rejected_sequence_log_probs_ref = _get_squence_log_probs(rejected_logits_ref, labels = rejected_batch['input_ids'], loss_mask = rejected_batch['loss_mask'])\n",
    "    \n",
    "    losses, chosen_rewards, rejected_rewards = preference_loss(chosen_sequence_log_probs, rejected_sequence_log_probs, chosen_sequence_log_probs_ref, rejected_sequence_log_probs_ref)\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dpo(model, ref_model, optimizer, train_loader, train_settings, device):\n",
    "    \n",
    "    global_steps = 0\n",
    "    record_list = []\n",
    "    model = model.to(device)\n",
    "    ref_model = ref_model.to(device)\n",
    "    for epoch in range(train_settings.num_epochs):\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            #print(global_steps)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses, chosen_rewards, rejected_rewards = compute_batch_loss(batch, model, ref_model, device)\n",
    "    \n",
    "            loss = losses.mean()\n",
    "            chosen_reward = chosen_rewards.mean()\n",
    "            rejected_reward = rejected_rewards.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            global_steps += 1\n",
    "            if global_steps % train_settings.log_freq == 0:\n",
    "                #model.eval()\n",
    "                record = {\"epoch\": epoch,\n",
    "                          \"step\": global_steps,\n",
    "                          \"train_loss\": loss.detach().item(),\n",
    "                          \"chosen_reward\": chosen_reward.item(),\n",
    "                          \"rejected_reward\": rejected_reward.item()\n",
    "                          }\n",
    "                print(record)\n",
    "                record_list.append(record)\n",
    "                \n",
    "    return record_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1437e27c332b4ffc8b6c94881cd6d22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/980 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32458cb7fe4f1c860c3b98477e0458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3cab14a7cb426286c987b17dcc59e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-6\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain_dpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_settings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [13], line 14\u001b[0m, in \u001b[0;36mtrain_dpo\u001b[1;34m(model, ref_model, optimizer, train_loader, train_settings, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m losses, chosen_rewards, rejected_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_batch_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     17\u001b[0m chosen_reward \u001b[38;5;241m=\u001b[39m chosen_rewards\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[1;32mIn [12], line 22\u001b[0m, in \u001b[0;36mcompute_batch_loss\u001b[1;34m(batch, model, ref_model, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m     chosen_logits_ref \u001b[38;5;241m=\u001b[39m ref_model(input_ids\u001b[38;5;241m=\u001b[39mchosen_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mchosen_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     20\u001b[0m     rejected_logits_ref \u001b[38;5;241m=\u001b[39m ref_model(input_ids\u001b[38;5;241m=\u001b[39mrejected_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mrejected_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m---> 22\u001b[0m     chosen_sequence_log_probs_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_get_squence_log_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchosen_logits_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchosen_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchosen_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     rejected_sequence_log_probs_ref \u001b[38;5;241m=\u001b[39m _get_squence_log_probs(rejected_logits_ref, labels \u001b[38;5;241m=\u001b[39m rejected_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], loss_mask \u001b[38;5;241m=\u001b[39m rejected_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m losses, chosen_rewards, rejected_rewards \u001b[38;5;241m=\u001b[39m preference_loss(chosen_sequence_log_probs, rejected_sequence_log_probs, chosen_sequence_log_probs_ref, rejected_sequence_log_probs_ref)\n",
      "Cell \u001b[1;32mIn [11], line 46\u001b[0m, in \u001b[0;36m_get_squence_log_probs\u001b[1;34m(logits, labels, loss_mask, average_log_prob)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#assert labels.shape == loss_mask.shape\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# let the sequence be A, B, C, D\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# labels[:,1Ôºö] are B, C, D\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# logits corresponds to B, C, D, X\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# logits[:,:-1,:] corresponds to B, C, D\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# labels \u001b[39;00m\n\u001b[0;32m     47\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[0;32m     48\u001b[0m loss_mask \u001b[38;5;241m=\u001b[39m loss_mask[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "train_settings = {\n",
    "    \"pretrained_model_name\": \"Qwen/Qwen2.5-0.5B\",\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"seed\": 1,\n",
    "    \"log_freq\": 50\n",
    "}\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(dataset['train'], \n",
    "                              batch_size= batch_size, \n",
    "                              #num_workers=num_workers, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=partial(custom_collate_fn, tokenizer=tokenizer))\n",
    "\n",
    "device = 'cuda'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# ref_model.load_state_dict(model.state_dict())\n",
    "for param in ref_model.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "# train model\n",
    "train_dpo(model, ref_model, optimizer, train_dataloader, OmegaConf.create(train_settings), device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
