{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: DPO Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text + \"\\n\\n### Response:\\n\"\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.prompt_with_completions = []\n",
    "        self.completions = []\n",
    "        \n",
    "        for entry in data:\n",
    "            \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            completion = entry['output']\n",
    "            \n",
    "            self.prompt_with_completions.append(instruction_plus_input + completion)\n",
    "            self.completions.append(completion)\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_with_completions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompt_with_completions[idx], self.completions[idx]\n",
    "\n",
    "def custom_collate_fn(batch, tokenizer, ignore_idx=-100):\n",
    "    \n",
    "    prompt_with_completions, completions = zip(*batch)\n",
    "    \n",
    "    padded = tokenizer(list(prompt_with_completions), padding='longest', truncation=True, return_tensors='pt')\n",
    "    padded = padded['input_ids']\n",
    "    inputs = padded[:,:-1]\n",
    "    targets = padded[:,1:]\n",
    "    \n",
    "    mask = (targets == tokenizer.pad_token_id)\n",
    "    \n",
    "    targets = targets.masked_fill(mask, ignore_idx)\n",
    "    \n",
    "    return inputs, targets\n",
    "    \n",
    "     \n",
    "def create_data_loader(data, tokenizer, batch_size=4, max_length=256, \n",
    "                       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    dataset = InstructionDataset(data=data)\n",
    "    \n",
    "    collate_fn = partial(custom_collate_fn, tokenizer=tokenizer, ignore_idx=-100)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=drop_last,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_data(file_path, url):\n",
    "    import urllib\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  31115,    264,  15908,  40158,   6524,    382,\n",
      "          14374,   5571,    510,     40,   1079,  14589,    369,    537,  33094,\n",
      "            847,  16319,    389,    882,    382,  14374,   5949,    510,  30665,\n",
      "            508,    675,  49088,     40,  36879,    369,  33094,    847,  16319,\n",
      "           3309,     13,    358,   3535,    429,  33094,    975,    389,    882,\n",
      "            374,   1376,    311,    697,   6950,    304,    752,     13,    358,\n",
      "           1079,   8480,    369,    279,   7626,    323,    358,   1896,   2480,\n",
      "          38142,    369,    432,     13,    358,  58283,  22231,    894,  60009,\n",
      "           8881,    553,    847,   6168,    382,     40,  15440,    429,    358,\n",
      "            686,   1896,    678,   5871,   7354,    311,   5978,    429,   1741,\n",
      "            458,  10455,   1558,    537,   3537,   1549,     13,   9063,  27378,\n",
      "             11,    358,    686,    653,    847,   1850,    311,   1281,    705,\n",
      "            369,    847,  20643,    382,     40,   1401,   4637,    311,    279,\n",
      "           6638,    315,  49103,    697,   6950,    304,    752,    382,     50,\n",
      "          86091,    345,     58,   7771,   3988],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  60424,   2326,  10295,    315,  65060,    315,\n",
      "            220,     21,    382,  14374,   5949,    510,  19641,  10295,    315,\n",
      "          65060,    315,    220,     21,    525,    220,     21,     11,    220,\n",
      "             16,     17,     11,    323,    220,     16,     23,     13, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  22550,   1493,   5837,   4092,    311,    862,\n",
      "          42807,  51749,   5643,    271,  14374,   5571,    510,  24347,     11,\n",
      "           9856,     11,   9625,     11,   6747,    271,  14374,   5949,    510,\n",
      "          24347,     11,   9856,     11,   6747,     11,   9625, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643],\n",
      "        [ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,   4340,   2310,    374,    279,  88575,    315,\n",
      "          31392,   1939,  14374,   5949,    510,    785,  88575,    315,  31392,\n",
      "            374,    916,    220,     16,     18,     19,   1635,   2310,     11,\n",
      "           3432,   1012,   8145,    304,   6527,    220,     16,     23,     23,\n",
      "             21,     13, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643]]), tensor([[  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         31115,   264, 15908, 40158,  6524,   382, 14374,  5571,   510,    40,\n",
      "          1079, 14589,   369,   537, 33094,   847, 16319,   389,   882,   382,\n",
      "         14374,  5949,   510, 30665,   508,   675, 49088,    40, 36879,   369,\n",
      "         33094,   847, 16319,  3309,    13,   358,  3535,   429, 33094,   975,\n",
      "           389,   882,   374,  1376,   311,   697,  6950,   304,   752,    13,\n",
      "           358,  1079,  8480,   369,   279,  7626,   323,   358,  1896,  2480,\n",
      "         38142,   369,   432,    13,   358, 58283, 22231,   894, 60009,  8881,\n",
      "           553,   847,  6168,   382,    40, 15440,   429,   358,   686,  1896,\n",
      "           678,  5871,  7354,   311,  5978,   429,  1741,   458, 10455,  1558,\n",
      "           537,  3537,  1549,    13,  9063, 27378,    11,   358,   686,   653,\n",
      "           847,  1850,   311,  1281,   705,   369,   847, 20643,   382,    40,\n",
      "          1401,  4637,   311,   279,  6638,   315, 49103,   697,  6950,   304,\n",
      "           752,   382,    50, 86091,   345,    58,  7771,  3988,    60],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         60424,  2326, 10295,   315, 65060,   315,   220,    21,   382, 14374,\n",
      "          5949,   510, 19641, 10295,   315, 65060,   315,   220,    21,   525,\n",
      "           220,    21,    11,   220,    16,    17,    11,   323,   220,    16,\n",
      "            23,    13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "         22550,  1493,  5837,  4092,   311,   862, 42807, 51749,  5643,   271,\n",
      "         14374,  5571,   510, 24347,    11,  9856,    11,  9625,    11,  6747,\n",
      "           271, 14374,  5949,   510, 24347,    11,  9856,    11,  6747,    11,\n",
      "          9625,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  374,   458,  7600,   429, 16555,   264,  3383,    13,  9645,   264,\n",
      "          2033,   429, 34901, 44595,   279,  1681,   382, 14374, 29051,   510,\n",
      "          4340,  2310,   374,   279, 88575,   315, 31392,  1939, 14374,  5949,\n",
      "           510,   785, 88575,   315, 31392,   374,   916,   220,    16,    18,\n",
      "            19,  1635,  2310,    11,  3432,  1012,  8145,   304,  6527,   220,\n",
      "            16,    23,    23,    21,    13,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]]))\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "def test_data_component():\n",
    "    file_path = \"instruction-data2.json\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"        \n",
    "    )\n",
    "\n",
    "    text_data = read_text_data(file_path, url)\n",
    "    tokenizer_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    #tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    for batch in train_loader:\n",
    "        print(batch)\n",
    "        break\n",
    "    \n",
    "test_data_component()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(\n",
    "    policy_chosen_logps: torch.FloatTensor, \n",
    "    policy_rejected_logps: torch.FloatTensor,\n",
    "    reference_chosen_logps: torch.FloatTensor,\n",
    "    reference_rejected_logps: torch.FloatTensor,\n",
    "    beta: float,\n",
    "    label_smoothing: float = 0.0,\n",
    "    ipo: bool = False,\n",
    "    reference_free: bool = False,\n",
    ") -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy_chosen_logps: log probabilities of the policy model for the chosen responses, shape: (batch_size,)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "    ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "    \n",
    "    logits = pi_logratios - ref_logratios\n",
    "    \n",
    "    if reference_free:\n",
    "        ref_logratios = 0.0\n",
    "    \n",
    "    if ipo:\n",
    "        losses = (logits - 1 / (2 * beta)) ** 2\n",
    "    else:\n",
    "        losses = - F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(-beta * logits) * label_smoothing\n",
    "        \n",
    "    \n",
    "    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "def _get_batch_logps(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    loss_mask: torch.LongTensor,\n",
    "    average_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: logits of the model output. Shape: (batch_size, seq_length, vocab_size)\n",
    "        labels: labels for which token's log probability; label = -100 indicates ignore. Shape (batch_size, seq_length)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    assert logits.shape[:-1] == labels.shape\n",
    "    # let the sequence be A, B, C, D\n",
    "    # labels[:,1：] are B, C, D\n",
    "    # logits corresponds to B, C, D, X\n",
    "    # logits[:,:-1,:] corresponds to B, C, D\n",
    "    labels = labels[:,1:].clone() # labels \n",
    "    logits = logits[:,:-1,:]\n",
    "    \n",
    "    loss_mark = loss_mask[:, 1:]\n",
    "    \n",
    "    # shape (batch_size, seq_len - 1)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsequeeze(2)).squeeze(2)\n",
    "    \n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * loss_mark).sum(-1) / loss_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * loss_mask).sum(-1)\n",
    "    \n",
    "def get_logps(outputs, labels, input_mask) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "    \n",
    "    all_logits = outputs.logits.to(torch.float32)\n",
    "    all_logps = _get_batch_logps(all_logits, labels, input_mask, average_log_prob=False)\n",
    "    \n",
    "    batch_size = all_logps.shape[0]\n",
    "    \n",
    "    chosen_logps = all_logps[: batch_size // 2]\n",
    "    rejected_logps = all_logps[batch_size//2:]\n",
    "    \n",
    "    return chosen_logps, rejected_logps\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    flat_targets = target_batch.flatten() \n",
    "    flat_logits = logits.flatten(0, 1)# flatten the first two dimensions \n",
    "    loss = F.cross_entropy(flat_logits, flat_targets) # tokens with ignore idx will not contribute to loss \n",
    "    return loss\n",
    "\n",
    "def train_model_epoch(model, \n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                num_epochs):\n",
    "    \n",
    "    train_losses, val_losses, track_token_seen = [],[],[]\n",
    "    tokens_seen = 0\n",
    "    global_steps = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_steps += 1\n",
    "            train_losses.append(loss.detach().item())\n",
    "            print(train_losses[-1])\n",
    "        \n",
    "    return train_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dpo(model, ref_model, optimizer, train_loader, train_settings):\n",
    "    \n",
    "    for epoch in range(train_settings.num_epochs):\n",
    "        \n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            \n",
    "            model_outputs = model(batch)\n",
    "            \n",
    "            reference_outputs = ref_model(batch)\n",
    "            \n",
    "            policy_chosen_logps, policy_rejected_logps = get_logps(model_outputs, batch['labels'])\n",
    "            \n",
    "            reference_chosen_logps, reference_rejected_logps = get_logps(reference_outputs, batch['labels'])\n",
    "            \n",
    "            loss_kwargs = {\"beta\": 0.1, \"reference_free\": False}\n",
    "            \n",
    "            losses, chosen_rewards, rejected_rewards = preference_loss(\n",
    "                policy_chosen_logps=policy_chosen_logps,\n",
    "                policy_rejected_logps=policy_rejected_logps,\n",
    "                reference_chosen_logps=reference_chosen_logps,\n",
    "                reference_rejected_logps=reference_rejected_logps,\n",
    "                **loss_kwargs\n",
    "            )\n",
    "    \n",
    "            loss = losses.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model_config, train_settings):\n",
    "    \n",
    "    torch.manual_seed(train_settings.seed)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    text_data = read_text_data(train_settings.file_path, train_settings.url)\n",
    "            \n",
    "    model = LlamaForCausalLM(config=model_config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=train_settings.learning_rate,\n",
    "                                  weight_decay=train_settings.weight_decay)\n",
    "    \n",
    "    # set up dataloader\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(train_settings.pretrained_model_name)\n",
    "    train_loader = create_data_loader(data=text_data, \n",
    "                                      tokenizer=tokenizer)\n",
    "    \n",
    "    \n",
    "    train_loader = create_data_loader(data=text_data,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      drop_last=True,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0\n",
    "    )\n",
    "        \n",
    "    train_losses, model = train_model_epoch(model=model,\n",
    "                train_loader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=train_settings.num_epochs,\n",
    "                device=device)\n",
    "    \n",
    "    return train_losses, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.114983558654785\n",
      "11.961651802062988\n",
      "11.927282333374023\n",
      "11.826778411865234\n",
      "11.707758903503418\n",
      "11.793996810913086\n",
      "11.565061569213867\n",
      "11.341635704040527\n",
      "11.44568920135498\n",
      "11.51699161529541\n",
      "11.538795471191406\n",
      "11.205238342285156\n",
      "11.476774215698242\n",
      "11.2799654006958\n",
      "10.608583450317383\n",
      "10.964029312133789\n",
      "11.31577205657959\n",
      "10.75229549407959\n",
      "10.765582084655762\n",
      "10.845158576965332\n",
      "10.3082857131958\n",
      "10.781938552856445\n",
      "11.183990478515625\n",
      "10.394356727600098\n",
      "10.934983253479004\n",
      "9.947456359863281\n",
      "10.557378768920898\n",
      "10.630148887634277\n",
      "10.455920219421387\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_config = {\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"bos_token_id\": 151643,\n",
    "        \"eos_token_id\": 151643,\n",
    "        \"pad_token_id\": 151643,\n",
    "        \"hidden_act\": \"silu\",\n",
    "        \"hidden_size\": 896,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 4864,\n",
    "        \"max_position_embeddings\": 32768,\n",
    "        \"max_window_layers\": 24,\n",
    "        \"model_type\": \"qwen2\",\n",
    "        \"num_attention_heads\": 14,\n",
    "        \"num_hidden_layers\": 24,\n",
    "        \"num_key_value_heads\": 2,\n",
    "        \"rms_norm_eps\": 1e-06,\n",
    "        \"rope_theta\": 1000000.0,\n",
    "        \"tie_word_embeddings\": True,\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "        \"transformers_version\": \"4.47.1\",\n",
    "        \"use_cache\": True,\n",
    "        \"use_mrope\": False,\n",
    "        \"vocab_size\": 151936,\n",
    "        \"qkv_bias\": True,\n",
    "        \"o_bias\": False,\n",
    "        \"mlp_bias\": False\n",
    "    }\n",
    "\n",
    "    model_config = OmegaConf.create(model_config)\n",
    "    train_settings = {\n",
    "        \"pretrained_model_name\": \"Qwen/Qwen2.5-0.5B\",\n",
    "        \"learning_rate\": 5e-6,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"stride\": 128,\n",
    "        \"seed\": 1,\n",
    "        \"file_path\":\"./instruction_data/instruction-data2.json\",\n",
    "        \"url\":\"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
    "    }\n",
    "    \n",
    "    train_settings = OmegaConf.create(train_settings)\n",
    "    \n",
    "    # train model\n",
    "    train_losses, model = train_main(model_config=model_config,\n",
    "                       train_settings=train_settings)\n",
    "    \n",
    "    print(train_losses)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
