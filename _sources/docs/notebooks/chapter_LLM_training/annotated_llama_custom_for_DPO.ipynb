{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: DPO Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "from llm_lab.utils.common_utils import move_to_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Oh, you like [insert interest]? Me too! What do you love about it?',\n",
       " 'chosen': \"Yeah! I'm super passionate about music! üéµ There's just something about how a good song can evoke emotions and transport you to a different time and place, you know? üï∞Ô∏è I love how it can bring people together, too. What about you? What kind of music are you into? üé∂ Do you have a favorite artist or genre? ü§î\",\n",
       " 'rejected': \"Good day. As a digital entity, I don't have a physical presence or a circadian rhythm, so I neither wake up early nor stay up late. I am designed to operate 24/7, providing assistance and responding to inquiries at any time. My purpose is to provide accurate and helpful information, and I do not have personal preferences or experiences.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"MiniLLM/MiniLLM-gpt2-120M\" # a tiny model for fast debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e1626c3f254997adf159f8b5fe562a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debb593b69a54de8991b3dec0bcb3742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86a0a0c341a4c0bbcc663506ff93cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce293f6af9264965a4e625ab29c18c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c9c36ec8b84b45a9cc836ad8d5fa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    result_dict = {}\n",
    "    prompt_tokenized = tokenizer(example['prompt'])\n",
    "    chosen_encoded = tokenizer(example['chosen'])\n",
    "    rejected_encoded = tokenizer(example['rejected'])\n",
    "    \n",
    "    result_dict['chosen_encoded'] = {'input_ids': prompt_tokenized['input_ids'] + chosen_encoded['input_ids'],\n",
    "                                   'attention_mask': prompt_tokenized['attention_mask'] + chosen_encoded['attention_mask'],\n",
    "                                   'loss_mask': [0] * len(prompt_tokenized['input_ids']) + [1] * len(chosen_encoded['input_ids']),\n",
    "                                   }\n",
    "    result_dict['rejected_encoded'] = {'input_ids': prompt_tokenized['input_ids'] + rejected_encoded['input_ids'],\n",
    "                                   'attention_mask': prompt_tokenized['attention_mask'] + rejected_encoded['attention_mask'],\n",
    "                                   'loss_mask': [0] * len(prompt_tokenized['input_ids']) + [1] * len(rejected_encoded['input_ids']),\n",
    "                                   }\n",
    "    example.update(result_dict)\n",
    "    return example\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb20d042f164a0f874385075d52900f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize, remove_columns=['prompt','chosen','rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer, ignore_idx=-100):\n",
    "    \n",
    "    max_len = max([len(e[type]['input_ids']) for type in ['chosen_encoded','rejected_encoded'] for e in batch])\n",
    "    result_dict = {}\n",
    "    \n",
    "    for type in ['chosen_encoded','rejected_encoded']:\n",
    "        if type not in result_dict:\n",
    "            result_dict[type] = collections.defaultdict(list)\n",
    "        for e in batch:\n",
    "            needed = max_len - len(e[type]['input_ids'])\n",
    "            e[type]['input_ids'] += [tokenizer.pad_token_id] * needed\n",
    "            e[type]['attention_mask'] += [0] * needed\n",
    "            e[type]['loss_mask'] += [0] * needed\n",
    "            result_dict[type]['input_ids'].append(e[type]['input_ids'])\n",
    "            result_dict[type]['attention_mask'].append(e[type]['attention_mask'])\n",
    "            result_dict[type]['loss_mask'].append(e[type]['loss_mask'])\n",
    "    \n",
    "    for type in ['chosen_encoded','rejected_encoded']:\n",
    "        for key in result_dict[type]:\n",
    "            result_dict[type][key] = torch.LongTensor(result_dict[type][key])\n",
    "            \n",
    "    return result_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenized_dataset['train'].train_test_split(test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     #print(batch)\n",
    "#     chosen_batch = move_to_device(batch['chosen_encoded'], device)\n",
    "    \n",
    "#     chosen_logits = model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits    \n",
    "#     print(chosen_logits)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Learning utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_loss(\n",
    "    chosen_log_probs: torch.FloatTensor, \n",
    "    rejected_log_probs: torch.FloatTensor,\n",
    "    reference_chosen_log_probs: torch.FloatTensor,\n",
    "    reference_rejected_log_probs: torch.FloatTensor,\n",
    "    beta: float = 0.1, # suggested value in the DPO paper\n",
    ") -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        chosen_log_probs: log probabilities of the policy model for the chosen responses, shape: (batch_size,)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    pi_logratios = chosen_log_probs - rejected_log_probs\n",
    "    ref_logratios = reference_chosen_log_probs - reference_rejected_log_probs\n",
    "    \n",
    "    logratios_difference = pi_logratios - ref_logratios\n",
    "    \n",
    "    losses = - F.logsigmoid(beta * logratios_difference)\n",
    "        \n",
    "    chosen_rewards = beta * (chosen_log_probs - reference_chosen_log_probs).detach()\n",
    "    rejected_rewards = beta * (rejected_log_probs - reference_rejected_log_probs).detach()\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "def _get_squence_log_probs(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    loss_mask: torch.LongTensor,\n",
    "    average_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: logits of the model output. Shape: (batch_size, seq_length, vocab_size)\n",
    "        labels: labels for which token's log probability; label = -100 indicates ignore. Shape (batch_size, seq_length)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert logits.shape[:-1] == labels.shape\n",
    "    #assert labels.shape == loss_mask.shape\n",
    "    # let the sequence be A, B, C, D\n",
    "    # labels[:,1Ôºö] are B, C, D\n",
    "    # logits corresponds to B, C, D, X\n",
    "    # logits[:,:-1,:] corresponds to B, C, D\n",
    "    labels = labels[:,1:].clone() # labels \n",
    "    logits = logits[:,:-1,:]\n",
    "    loss_mask = loss_mask[:, 1:]\n",
    "    \n",
    "    \n",
    "    # log_probs shape (batch_size, seq_len - 1, vocab_size)\n",
    "    # label shape before unsqueeze - (batch_size, seq_len - 1), after - (batch_size, seq_len - 1, vocab_size)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    # per_token_logps shape (batch_size, seq_len - 1)\n",
    "    per_token_logps = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(2) # squeeze on the last dim\n",
    "    \n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * loss_mask).sum(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, model, ref_model, device):\n",
    "    \n",
    "    assert model.training == True\n",
    "    assert ref_model.training == False\n",
    "    \n",
    "    chosen_batch = batch['chosen_encoded']\n",
    "    rejected_batch = batch['rejected_encoded']\n",
    "    \n",
    "    chosen_batch = move_to_device(chosen_batch, device)\n",
    "    rejected_batch = move_to_device(rejected_batch, device)\n",
    "    \n",
    "    chosen_logits = model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits\n",
    "    rejected_logits = model(input_ids=rejected_batch['input_ids'], attention_mask=rejected_batch['attention_mask']).logits\n",
    "    \n",
    "    chosen_sequence_log_probs = _get_squence_log_probs(chosen_logits, labels = chosen_batch['input_ids'], loss_mask = chosen_batch['loss_mask'])\n",
    "    rejected_sequence_log_probs = _get_squence_log_probs(rejected_logits, labels = rejected_batch['input_ids'], loss_mask = rejected_batch['loss_mask'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        chosen_logits_ref = ref_model(input_ids=chosen_batch['input_ids'], attention_mask=chosen_batch['attention_mask']).logits\n",
    "        rejected_logits_ref = ref_model(input_ids=rejected_batch['input_ids'], attention_mask=rejected_batch['attention_mask']).logits\n",
    "    \n",
    "        chosen_sequence_log_probs_ref = _get_squence_log_probs(chosen_logits_ref, labels = chosen_batch['input_ids'], loss_mask = chosen_batch['loss_mask'])\n",
    "        rejected_sequence_log_probs_ref = _get_squence_log_probs(rejected_logits_ref, labels = rejected_batch['input_ids'], loss_mask = rejected_batch['loss_mask'])\n",
    "    \n",
    "    losses, chosen_rewards, rejected_rewards = preference_loss(chosen_sequence_log_probs, rejected_sequence_log_probs, chosen_sequence_log_probs_ref, rejected_sequence_log_probs_ref)\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dpo(model, ref_model, optimizer, train_loader, train_settings, device):\n",
    "    \n",
    "    global_steps = 0\n",
    "    record_list = []\n",
    "    model = model.to(device)\n",
    "    ref_model = ref_model.to(device)\n",
    "    for epoch in range(train_settings.num_epochs):\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            #print(global_steps)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses, chosen_rewards, rejected_rewards = compute_batch_loss(batch, model, ref_model, device)\n",
    "    \n",
    "            loss = losses.mean()\n",
    "            chosen_reward = chosen_rewards.mean()\n",
    "            rejected_reward = rejected_rewards.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            global_steps += 1\n",
    "            if global_steps % train_settings.log_freq == 0:\n",
    "                #model.eval()\n",
    "                record = {\"epoch\": epoch,\n",
    "                          \"step\": global_steps,\n",
    "                          \"train_loss\": loss.detach().item(),\n",
    "                          \"chosen_reward\": chosen_reward.item(),\n",
    "                          \"rejected_reward\": rejected_reward.item()\n",
    "                          }\n",
    "                print(record)\n",
    "                record_list.append(record)\n",
    "                \n",
    "    return record_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1437e27c332b4ffc8b6c94881cd6d22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/980 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32458cb7fe4f1c860c3b98477e0458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3cab14a7cb426286c987b17dcc59e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "train_settings = {\n",
    "    \"pretrained_model_name\": \"Qwen/Qwen2.5-0.5B\",\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"seed\": 1,\n",
    "    \"log_freq\": 50\n",
    "}\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(dataset['train'], \n",
    "                              batch_size= batch_size, \n",
    "                              #num_workers=num_workers, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=partial(custom_collate_fn, tokenizer=tokenizer))\n",
    "\n",
    "device = 'cuda'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# ref_model.load_state_dict(model.state_dict())\n",
    "for param in ref_model.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "# train model\n",
    "train_dpo(model, ref_model, optimizer, train_dataloader, OmegaConf.create(train_settings), device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
