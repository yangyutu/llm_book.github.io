{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lab: LLM Pretraining\n",
    "\n",
    "Here we directly leverage the decoder architecture we made from previous sections. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from llm_lab.model.rotary_decoder import RotaryDecoderModel\n",
    "from llm_lab.utils.collate_utils import default_data_collator\n",
    "from llm_lab.utils.common_utils import move_to_device\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name =  \"wikitext\"\n",
    "data_config = \"wikitext-2-raw-v1\"\n",
    "text_column_name = \"text\"\n",
    "\n",
    "# model parameters\n",
    "model_name_or_path=\"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name, data_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[text_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_chunk(tokenized_examples, chunk_size=1024, chunk_key='input_ids'):\n",
    "    keys = list(tokenized_examples.keys())\n",
    "    # use chain to flatten list\n",
    "    concat_examples = {k: list(chain(*tokenized_examples[k])) for k in keys}\n",
    "    total_length = len(concat_examples[chunk_key])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    \n",
    "    result_dict = {\n",
    "        k: [v[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, v in concat_examples.items()\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_datasets.map(\n",
    "                    tokenize, \n",
    "                    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data = tokenized_dataset.map(\n",
    "                                    partial(group_and_chunk, \n",
    "                                            chunk_size=256),\n",
    "                                        #chunk_size=tokenizer.model_max_length),\n",
    "                                    batched=True,\n",
    "                                    remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1104\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9327\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 964\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = RotaryDecoderModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        hidden_states = self.decoder(input_ids=batch['input_ids'])\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, model, device):\n",
    "    assert model.training\n",
    "    move_to_device(batch, device)\n",
    "    model_input = {'input_ids':batch['input_ids'],'attention_mask': batch['attention_mask']}\n",
    "    logits = model(model_input)[:,:-1,:].contiguous()\n",
    "    labels = batch['input_ids'][:,1:].contiguous()\n",
    "    flat_labels = labels.view(-1)\n",
    "    flat_logits = logits.view(-1, logits.shape[-1])\n",
    "    loss = F.cross_entropy(flat_logits, flat_labels)\n",
    "    return loss\n",
    "\n",
    "def compute_eval_loss(eval_dataloader, model, device):\n",
    "    assert not model.training\n",
    "    all_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            move_to_device(batch, device)\n",
    "            model_input = {'input_ids':batch['input_ids'],'attention_mask': batch['attention_mask']}\n",
    "            logits = model(model_input)[:,:-1,:].contiguous()\n",
    "            labels = batch['input_ids'][:,1:].contiguous()\n",
    "            flat_labels = labels.view(-1)\n",
    "            flat_logits = logits.view(-1, logits.shape[-1])\n",
    "            losses = F.cross_entropy(flat_logits, flat_labels, reduction='none').tolist()\n",
    "            all_losses.extend(losses)\n",
    "    \n",
    "    mean_loss = np.mean(all_losses)\n",
    "    return mean_loss\n",
    "\n",
    "def train_model_epoch(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                optimizer,\n",
    "                device,\n",
    "                train_config):\n",
    "    \n",
    "    global_steps = 0\n",
    "    record_list = []\n",
    "    model = model.to(device)\n",
    "    for epoch in range(train_config.num_epochs):\n",
    "        \n",
    "        \n",
    "        for batch in train_loader:\n",
    "            model.train()\n",
    "            loss = compute_batch_loss(batch, model, device)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_steps += 1\n",
    "            if global_steps % train_config.log_freq == 0:\n",
    "                model.eval()\n",
    "                val_loss = compute_eval_loss(val_loader, model, device)\n",
    "                record = {\"epoch\": epoch,\n",
    "                          \"step\": global_steps,\n",
    "                          \"train_loss\": loss.detach().item(),\n",
    "                          \"val_loss\": val_loss}\n",
    "                print(record)\n",
    "                record_list.append(record)\n",
    "        \n",
    "    return record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model_config, train_settings, chunk_data):\n",
    "    \n",
    "    torch.manual_seed(train_settings.seed)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            \n",
    "    model = DecoderCausalLM(config=model_config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=train_settings.learning_rate,\n",
    "                                  weight_decay=train_settings.weight_decay)\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(chunk_data['train'],\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                        num_workers=0,\n",
    "                                        collate_fn=default_data_collator\n",
    "    )\n",
    "    \n",
    "    val_loader =  DataLoader(chunk_data['validation'],\n",
    "                                      batch_size=train_settings.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                        num_workers=0,\n",
    "                                        collate_fn=default_data_collator\n",
    "    )\n",
    "    \n",
    "    train_model_epoch(model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                train_config=train_settings,\n",
    "                device=device)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'step': 50, 'train_loss': 5.934116363525391, 'val_loss': 5.95978886500486}\n",
      "{'epoch': 0, 'step': 100, 'train_loss': 4.6610188484191895, 'val_loss': 4.4738071170653555}\n",
      "{'epoch': 0, 'step': 150, 'train_loss': 2.971130132675171, 'val_loss': 3.394472318179944}\n",
      "{'epoch': 0, 'step': 200, 'train_loss': 2.0693352222442627, 'val_loss': 2.6074926472101367}\n",
      "{'epoch': 0, 'step': 250, 'train_loss': 1.3895280361175537, 'val_loss': 2.1285733113386835}\n",
      "{'epoch': 0, 'step': 300, 'train_loss': 1.6931740045547485, 'val_loss': 1.758039969718746}\n",
      "{'epoch': 0, 'step': 350, 'train_loss': 1.4765475988388062, 'val_loss': 1.500211113480706}\n",
      "{'epoch': 0, 'step': 400, 'train_loss': 1.3626762628555298, 'val_loss': 1.3146632623914765}\n",
      "{'epoch': 0, 'step': 450, 'train_loss': 1.3208198547363281, 'val_loss': 1.1729037596728389}\n",
      "{'epoch': 0, 'step': 500, 'train_loss': 1.2643789052963257, 'val_loss': 1.0483412505480632}\n",
      "{'epoch': 0, 'step': 550, 'train_loss': 1.347705364227295, 'val_loss': 0.942060433071201}\n",
      "{'epoch': 0, 'step': 600, 'train_loss': 0.37800800800323486, 'val_loss': 0.8477879156293748}\n",
      "{'epoch': 0, 'step': 650, 'train_loss': 0.7735574245452881, 'val_loss': 0.7757829034613258}\n",
      "{'epoch': 0, 'step': 700, 'train_loss': 0.4930685758590698, 'val_loss': 0.7158518082294832}\n",
      "{'epoch': 0, 'step': 750, 'train_loss': 0.7035064101219177, 'val_loss': 0.6531763765318296}\n",
      "{'epoch': 0, 'step': 800, 'train_loss': 0.4497267007827759, 'val_loss': 0.5995545712060928}\n",
      "{'epoch': 0, 'step': 850, 'train_loss': 1.0339795351028442, 'val_loss': 0.555557148217498}\n",
      "{'epoch': 0, 'step': 900, 'train_loss': 0.2743229269981384, 'val_loss': 0.5206479844022962}\n",
      "{'epoch': 0, 'step': 950, 'train_loss': 0.3691011965274811, 'val_loss': 0.4959029606489849}\n",
      "{'epoch': 0, 'step': 1000, 'train_loss': 0.4940517842769623, 'val_loss': 0.4663128215367203}\n",
      "{'epoch': 0, 'step': 1050, 'train_loss': 0.8797768950462341, 'val_loss': 0.4402653791785611}\n",
      "{'epoch': 0, 'step': 1100, 'train_loss': 0.34599336981773376, 'val_loss': 0.41212919295760314}\n",
      "{'epoch': 0, 'step': 1150, 'train_loss': 0.3531911373138428, 'val_loss': 0.4092062050130844}\n",
      "{'epoch': 0, 'step': 1200, 'train_loss': 0.4641529619693756, 'val_loss': 0.38234950190919664}\n",
      "{'epoch': 0, 'step': 1250, 'train_loss': 0.22967249155044556, 'val_loss': 0.3607293127420803}\n",
      "{'epoch': 0, 'step': 1300, 'train_loss': 0.3634558618068695, 'val_loss': 0.3436481123064947}\n",
      "{'epoch': 0, 'step': 1350, 'train_loss': 0.35325485467910767, 'val_loss': 0.3274566013152589}\n",
      "{'epoch': 0, 'step': 1400, 'train_loss': 0.09018289297819138, 'val_loss': 0.3139857236895701}\n",
      "{'epoch': 0, 'step': 1450, 'train_loss': 0.13889189064502716, 'val_loss': 0.3028809143600657}\n",
      "{'epoch': 0, 'step': 1500, 'train_loss': 0.3882494270801544, 'val_loss': 0.29457289319999}\n",
      "{'epoch': 0, 'step': 1550, 'train_loss': 0.18418020009994507, 'val_loss': 0.28344289717003707}\n",
      "{'epoch': 0, 'step': 1600, 'train_loss': 0.30477046966552734, 'val_loss': 0.2698668811876248}\n",
      "{'epoch': 0, 'step': 1650, 'train_loss': 0.220538929104805, 'val_loss': 0.2589764347443116}\n",
      "{'epoch': 0, 'step': 1700, 'train_loss': 0.1040421724319458, 'val_loss': 0.250100136061844}\n",
      "{'epoch': 0, 'step': 1750, 'train_loss': 0.14432775974273682, 'val_loss': 0.23775900081025814}\n",
      "{'epoch': 0, 'step': 1800, 'train_loss': 0.11945632100105286, 'val_loss': 0.23192028569184286}\n",
      "{'epoch': 0, 'step': 1850, 'train_loss': 0.16128183901309967, 'val_loss': 0.22432074770765273}\n",
      "{'epoch': 0, 'step': 1900, 'train_loss': 8.705660820007324, 'val_loss': 8.350389710870076}\n",
      "{'epoch': 0, 'step': 1950, 'train_loss': 8.342676162719727, 'val_loss': 8.020229710955313}\n",
      "{'epoch': 0, 'step': 2000, 'train_loss': 7.809548377990723, 'val_loss': 7.690571196670064}\n",
      "{'epoch': 0, 'step': 2050, 'train_loss': 7.34458065032959, 'val_loss': 7.149815051465409}\n",
      "{'epoch': 0, 'step': 2100, 'train_loss': 6.5025153160095215, 'val_loss': 6.700878530984053}\n",
      "{'epoch': 0, 'step': 2150, 'train_loss': 6.191983222961426, 'val_loss': 5.982873797151425}\n",
      "{'epoch': 0, 'step': 2200, 'train_loss': 5.310995578765869, 'val_loss': 5.394032826321432}\n",
      "{'epoch': 0, 'step': 2250, 'train_loss': 5.210432529449463, 'val_loss': 5.1433515040811}\n",
      "{'epoch': 0, 'step': 2300, 'train_loss': 4.809581279754639, 'val_loss': 4.4858431711281295}\n",
      "{'epoch': 0, 'step': 2350, 'train_loss': 4.1014628410339355, 'val_loss': 4.106788948743525}\n",
      "{'epoch': 0, 'step': 2400, 'train_loss': 4.44239616394043, 'val_loss': 4.132312455478721}\n",
      "{'epoch': 0, 'step': 2450, 'train_loss': 3.250267744064331, 'val_loss': 3.8381655649834774}\n",
      "{'epoch': 0, 'step': 2500, 'train_loss': 3.5711305141448975, 'val_loss': 3.8768836422597044}\n",
      "{'epoch': 0, 'step': 2550, 'train_loss': 3.418431282043457, 'val_loss': 3.2884441027276625}\n",
      "{'epoch': 0, 'step': 2600, 'train_loss': 3.146705389022827, 'val_loss': 3.21507889968048}\n",
      "{'epoch': 0, 'step': 2650, 'train_loss': 2.2807860374450684, 'val_loss': 2.3467597127653406}\n",
      "{'epoch': 0, 'step': 2700, 'train_loss': 0.9968742728233337, 'val_loss': 1.0672410720783017}\n",
      "{'epoch': 0, 'step': 2750, 'train_loss': 0.6552670001983643, 'val_loss': 0.5211755128874394}\n",
      "{'epoch': 0, 'step': 2800, 'train_loss': 0.19717825949192047, 'val_loss': 0.43510968697448005}\n",
      "{'epoch': 0, 'step': 2850, 'train_loss': 0.20189234614372253, 'val_loss': 0.3295214429391804}\n",
      "{'epoch': 0, 'step': 2900, 'train_loss': 0.18330155313014984, 'val_loss': 0.2813216200457986}\n",
      "{'epoch': 0, 'step': 2950, 'train_loss': 0.15684756636619568, 'val_loss': 0.2595366069104909}\n",
      "{'epoch': 0, 'step': 3000, 'train_loss': 0.18667520582675934, 'val_loss': 0.24772822535568678}\n",
      "{'epoch': 0, 'step': 3050, 'train_loss': 0.1248273029923439, 'val_loss': 0.2435624335909632}\n",
      "{'epoch': 0, 'step': 3100, 'train_loss': 0.27407965064048767, 'val_loss': 0.324169851526482}\n",
      "{'epoch': 0, 'step': 3150, 'train_loss': 0.17817610502243042, 'val_loss': 0.23635927794352224}\n",
      "{'epoch': 0, 'step': 3200, 'train_loss': 0.05649694427847862, 'val_loss': 0.2180812220598306}\n",
      "{'epoch': 0, 'step': 3250, 'train_loss': 0.18310590088367462, 'val_loss': 0.20675155868711184}\n",
      "{'epoch': 0, 'step': 3300, 'train_loss': 0.07765556126832962, 'val_loss': 0.1976093819610651}\n",
      "{'epoch': 0, 'step': 3350, 'train_loss': 0.08958636224269867, 'val_loss': 0.1902608141362801}\n",
      "{'epoch': 0, 'step': 3400, 'train_loss': 0.22411604225635529, 'val_loss': 0.18098746515097255}\n",
      "{'epoch': 0, 'step': 3450, 'train_loss': 0.07328592240810394, 'val_loss': 0.17539710472750406}\n",
      "{'epoch': 0, 'step': 3500, 'train_loss': 0.1396494358778, 'val_loss': 0.17016740589497703}\n",
      "{'epoch': 0, 'step': 3550, 'train_loss': 0.11717287451028824, 'val_loss': 0.16630381678691808}\n",
      "{'epoch': 0, 'step': 3600, 'train_loss': 0.08338280767202377, 'val_loss': 0.16008293923810207}\n",
      "{'epoch': 0, 'step': 3650, 'train_loss': 0.06099998578429222, 'val_loss': 0.15845389405402713}\n",
      "{'epoch': 0, 'step': 3700, 'train_loss': 0.04754674434661865, 'val_loss': 0.14847814030488105}\n",
      "{'epoch': 0, 'step': 3750, 'train_loss': 0.07633408159017563, 'val_loss': 0.14715632141067736}\n",
      "{'epoch': 0, 'step': 3800, 'train_loss': 0.11344993114471436, 'val_loss': 0.14168298818636121}\n",
      "{'epoch': 0, 'step': 3850, 'train_loss': 0.1232042983174324, 'val_loss': 0.13895043907426818}\n",
      "{'epoch': 0, 'step': 3900, 'train_loss': 0.1343431919813156, 'val_loss': 0.1355173505273704}\n",
      "{'epoch': 0, 'step': 3950, 'train_loss': 0.026931755244731903, 'val_loss': 0.1329402860087926}\n",
      "{'epoch': 0, 'step': 4000, 'train_loss': 0.012349644675850868, 'val_loss': 0.13514639128325923}\n",
      "{'epoch': 0, 'step': 4050, 'train_loss': 0.09015399217605591, 'val_loss': 0.12927584716122092}\n",
      "{'epoch': 0, 'step': 4100, 'train_loss': 0.08261846750974655, 'val_loss': 0.1252295247635492}\n",
      "{'epoch': 0, 'step': 4150, 'train_loss': 0.10221000760793686, 'val_loss': 0.12577679825099244}\n",
      "{'epoch': 0, 'step': 4200, 'train_loss': 0.029511943459510803, 'val_loss': 0.12120829767329572}\n",
      "{'epoch': 0, 'step': 4250, 'train_loss': 0.12373088300228119, 'val_loss': 0.11799028416805349}\n",
      "{'epoch': 0, 'step': 4300, 'train_loss': 0.097576804459095, 'val_loss': 0.11484288345580376}\n",
      "{'epoch': 0, 'step': 4350, 'train_loss': 0.16657713055610657, 'val_loss': 0.10905103474105166}\n",
      "{'epoch': 0, 'step': 4400, 'train_loss': 0.04857831075787544, 'val_loss': 0.10786582617982032}\n",
      "{'epoch': 0, 'step': 4450, 'train_loss': 0.04279662296175957, 'val_loss': 0.10175716297866659}\n",
      "{'epoch': 0, 'step': 4500, 'train_loss': 0.07119077444076538, 'val_loss': 0.09992072919463102}\n",
      "{'epoch': 0, 'step': 4550, 'train_loss': 0.010768338106572628, 'val_loss': 0.0980504394975618}\n",
      "{'epoch': 0, 'step': 4600, 'train_loss': 0.025507673621177673, 'val_loss': 0.09697091216987117}\n",
      "{'epoch': 0, 'step': 4650, 'train_loss': 0.10472672432661057, 'val_loss': 0.0959155055818307}\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"max_position_embeddings\": 1024,\n",
    "    \"hidden_size\": 768,         # model dimension\n",
    "    \"intermediate_size\": 768*4,\n",
    "    \"num_key_value_heads\": 2,\n",
    "    \"num_heads\": 4,          # Number of attention heads\n",
    "    \"num_layers\": 6,         # Number of layers\n",
    "    \"attention_dropout\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False,       # Query-key-value bias\n",
    "    \"o_bias\": True,\n",
    "    \"mlp_bias\": True,\n",
    "    \"rms_norm_eps\": 1e-6,\n",
    "    \"dropout\": 0.1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"causal_attention\": True\n",
    "}\n",
    "\n",
    "model_config = OmegaConf.create(model_config)\n",
    "train_settings = {\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size\": 2,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"seed\": 1,\n",
    "    \"log_freq\": 50\n",
    "}\n",
    "\n",
    "train_settings = OmegaConf.create(train_settings)\n",
    "\n",
    "# train model\n",
    "train_main(model_config=model_config, train_settings=train_settings, chunk_data=chunk_data)\n",
    "    \n",
    "\n",
    "# save model\n",
    "#torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# training process\n",
    "# {'epoch': 0, 'step': 950, 'train_loss': 0.3691011965274811, 'val_loss': 0.4959029606489849}\n",
    "# {'epoch': 0, 'step': 1000, 'train_loss': 0.4940517842769623, 'val_loss': 0.4663128215367203}\n",
    "# {'epoch': 0, 'step': 1050, 'train_loss': 0.8797768950462341, 'val_loss': 0.4402653791785611}\n",
    "# {'epoch': 0, 'step': 1100, 'train_loss': 0.34599336981773376, 'val_loss': 0.41212919295760314}\n",
    "# {'epoch': 0, 'step': 1150, 'train_loss': 0.3531911373138428, 'val_loss': 0.4092062050130844}\n",
    "# {'epoch': 0, 'step': 1200, 'train_loss': 0.4641529619693756, 'val_loss': 0.38234950190919664}\n",
    "# {'epoch': 0, 'step': 1250, 'train_loss': 0.22967249155044556, 'val_loss': 0.3607293127420803}\n",
    "# {'epoch': 0, 'step': 1300, 'train_loss': 0.3634558618068695, 'val_loss': 0.3436481123064947}\n",
    "# {'epoch': 0, 'step': 1350, 'train_loss': 0.35325485467910767, 'val_loss': 0.3274566013152589}\n",
    "# {'epoch': 0, 'step': 1400, 'train_loss': 0.09018289297819138, 'val_loss': 0.3139857236895701}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
