
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9. LLM Architectures Fundamentals &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_LLM_arch/LLM_dense_architectures';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="10. MOE sparse models" href="LLM_moe_sparse_architectures.html" />
    <link rel="prev" title="8. GPT Series" href="../chapter_foundation/GPT_series.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Table of Contents
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: Unveiling the Power of Language Models in the Age of Artificial Intelligence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="LLM_moe_sparse_architectures.html">10. MOE sparse models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM alignement and preference learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">14. LLM Training Acceleration</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. LLM Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference acceleration</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">20. Basic prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">21. Advanced prompt techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Retrieval-Augmented Generation (RAG)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">22. Basic RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">23. Advanced rag techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">24. Information Retrieval Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">25. Application of LLM in IR</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/chapter_LLM_arch/LLM_dense_architectures.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Architectures Fundamentals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">9.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-embeddings">9.2. Position Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-position">9.2.1. Absolute Position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotary-postion-embedding">9.2.2. Rotary Postion Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mechanism">9.2.2.1. The mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practival-implementations">9.2.2.2. Practival Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">9.3. Layer normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-basics">9.3.1. Layer normalization basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-norm-root-mean-square-norm">9.3.2. RMS Norm (Root Mean Square Norm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-position">9.3.3. Layer normalization position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-example-choices">9.3.4. Layer normalization example choices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-and-variants">9.4. Self-attention and Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-mha">9.4.1. Multi-Head Attention (MHA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-query-attention-mqa">9.4.2. Multi Query Attention (MQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention-gqa">9.4.3. Grouped Query Attention (GQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sliding-window-attention">9.4.4. Sliding Window Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-attention">9.4.5. Sparse Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">9.5. Activation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenziation-vocabulary-and-weight-tying">9.6. Tokenziation, vocabulary, and weight tying</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">9.6.1. BPE Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-bpe-to-bbpe">9.6.2. From BPE to BBPE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-composition-in-transformer-models">9.7. Parameter composition in Transformer models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">9.7.1. Input layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-layer">9.7.2. Attention layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-layer">9.7.3. Feed-forward layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">9.7.4. Output layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#total-weight">9.7.5. Total weight</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-architecture-examples">9.8. Dense Architecture Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">9.9. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-architectures">9.10. LLama architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-breadown-during-forward-pass">9.11. Computation breadown during forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">9.12. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-architectures-fundamentals">
<h1><span class="section-number">9. </span>LLM Architectures Fundamentals<a class="headerlink" href="#llm-architectures-fundamentals" title="Link to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">9.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<figure class="align-default" id="chapter-foundation-fig-pretrained-lm-transformer-arch">
<a class="reference internal image-reference" href="../../_images/large_langage_models_release_timeline.png"><img alt="../../_images/large_langage_models_release_timeline.png" src="../../_images/large_langage_models_release_timeline.png" style="width: 673.0px; height: 334.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.1 </span><span class="caption-text">A timeline of existing large language models (having a size larger than 10B) in recent years. We mark the open-source LLMs in yellow color. Image from <span id="id1">[<a class="reference internal" href="#id1500" title="Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.">ZZL+23</a>]</span>.</span><a class="headerlink" href="#chapter-foundation-fig-pretrained-lm-transformer-arch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Scale</strong>: LLMs are trained on enormous datasets, often containing hundreds of billions of words or tokens. This massive scale allows them to capture intricate patterns and nuances in language. For example, GPT-3 was trained on about 500 billion tokens, while some more recent models have used even larger datasets.</p></li>
<li><p><strong>Transformer architecture</strong>: Most modern LLMs use transformer architectures, which were introduced in the “Attention is All You Need” paper. These models can have billions of parameters - GPT-3 has 175 billion, for instance. The transformer architecture allows for efficient parallel processing and captures long-range dependencies in text.</p></li>
<li><p><strong>Few-shot learning via prompting</strong>: LLMs can often perform new tasks with just a few examples provided in the prompt. This “in-context learning” allows them to adapt to new tasks without changing their weights, demonstrating a form of meta-learning.</p></li>
<li><p><strong>Multi-task capability</strong>: A single LLM can perform various language tasks such as translation, summarization, question-answering, and text generation without needing separate models for each task. This versatility makes them powerful tools for a wide range of applications.</p></li>
<li><p><strong>Self-supervised learning</strong>: LLMs are typically pre-trained using self-supervised learning techniques. The most common approach is next-token prediction, where the model learns to predict the next word in a sequence given the previous words. This allows the model to learn from vast amounts of unlabeled text data.</p></li>
<li><p><strong>Emergent reasoning abilities</strong>: As LLMs grow in size and complexity, they often develop capabilities that weren’t explicitly trained for. For example, a) Arithmetic: Some LLMs can perform basic math operations despite not being trained specifically on mathematics. b) Logical reasoning: Models may show ability to follow simple logical arguments or solve puzzles.</p></li>
<li><p><strong>Transfer learning</strong>: The pre-trained LLM can be fine-tuned on specific tasks or domains with much smaller datasets. This transfer learning approach is powerful because the model can leverage its general language understanding for specialized tasks, often outperforming models trained from scratch on those tasks.</p></li>
<li><p><strong>Hallucination</strong>: LLMs can sometimes generate text that sounds plausible but is factually incorrect or nonsensical. This “hallucination” occurs because the models are optimizing for plausible text generation rather than strict factual accuracy. It’s a significant challenge in deploying LLMs for applications requiring high reliability.</p></li>
</ul>
</section>
<section id="position-embeddings">
<h2><span class="section-number">9.2. </span>Position Embeddings<a class="headerlink" href="#position-embeddings" title="Link to this heading">#</a></h2>
<section id="absolute-position">
<h3><span class="section-number">9.2.1. </span>Absolute Position<a class="headerlink" href="#absolute-position" title="Link to this heading">#</a></h3>
</section>
<section id="rotary-postion-embedding">
<h3><span class="section-number">9.2.2. </span>Rotary Postion Embedding<a class="headerlink" href="#rotary-postion-embedding" title="Link to this heading">#</a></h3>
<section id="the-mechanism">
<h4><span class="section-number">9.2.2.1. </span>The mechanism<a class="headerlink" href="#the-mechanism" title="Link to this heading">#</a></h4>
<p>The key idea of Rotary position embedding (Rope) is to multiply query vector <span class="math notranslate nohighlight">\(\boldsymbol{q}\)</span> (of a token) and key vector <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span> (of another token) by a rotational matrix <span class="math notranslate nohighlight">\(R(\theta_q)\)</span> and <span class="math notranslate nohighlight">\(R(\theta_k)\)</span>, where <span class="math notranslate nohighlight">\(\theta_q\)</span> and <span class="math notranslate nohighlight">\(\theta_k\)</span> are taking values to indicate the positions of query vector token and key vector token, respectively.</p>
<p>Specifically, let <span class="math notranslate nohighlight">\(\theta_q = m\theta\)</span> and <span class="math notranslate nohighlight">\(\theta_k = n\theta\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span> are integer positions of query vector token and key vector token.
Now we are showing that the rotated query-key inner product is a function of the relative position <span class="math notranslate nohighlight">\((m - n)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\left\langle R\left(\theta_q\right) \boldsymbol{q}, R\left(\theta_k\right) \boldsymbol{k}\right\rangle &amp; =\boldsymbol{q}^{\top} R\left(\theta_q\right)^{\top} R\left(\theta_k\right) \boldsymbol{k} \\
&amp; =\boldsymbol{q}^{\top} R\left(\theta_k-\theta_q\right) \boldsymbol{k} \\
&amp; =\left\langle R\left(\theta_q-\theta_k\right) \boldsymbol{q}, \boldsymbol{k}\right\rangle \\
&amp; =\left\langle R\left(\theta (m-n)\right) \boldsymbol{q}, \boldsymbol{k}\right\rangle 
\end{aligned}
\end{split}\]</div>
<p>We have used the following important properties of rotational matrix:</p>
<ol class="arabic simple">
<li><p>The transpose of a rotation matrix is equal to its inverse: <span class="math notranslate nohighlight">\(R(\theta)^{\top}=R(-\theta)\)</span>.</p></li>
<li><p>The matrix multiplication of rotational matrices satisfies: <span class="math notranslate nohighlight">\(R(\theta_x)\cdot R(\theta_y) = R(\theta_x + \theta_y)\)</span></p></li>
</ol>
<p>In other words, the inner product of two rotated vectors is equal to the inner product of one vector rotated by their angle difference and the other original vector.</p>
</section>
<section id="practival-implementations">
<h4><span class="section-number">9.2.2.2. </span>Practival Implementations<a class="headerlink" href="#practival-implementations" title="Link to this heading">#</a></h4>
<p>First,The rotation matrix is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R(\theta)=\left[\begin{array}{cc}
\cos (\theta) &amp; -\sin (\theta) \\
\sin (\theta) &amp; \cos (\theta)
\end{array}\right]
\end{split}\]</div>
</section>
</section>
</section>
<section id="layer-normalization">
<h2><span class="section-number">9.3. </span>Layer normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<section id="layer-normalization-basics">
<h3><span class="section-number">9.3.1. </span>Layer normalization basics<a class="headerlink" href="#layer-normalization-basics" title="Link to this heading">#</a></h3>
<p>The LayerNorm was originally proposed to overcome the  in combating the internal covariate shift issue <span id="id2">[<a class="reference internal" href="#id1510" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. 2015. URL: https://arxiv.org/abs/1502.03167, arXiv:1502.03167.">IS15</a>]</span>, where a layer’s input distribution changes as previous layers are updated, causing the difficulty of traning deep models.</p>
<p>The key idea in LayerNorm is to normalize the input to the neural network layer via</p>
<ul class="simple">
<li><p>re-centering by subtracting the mean</p></li>
<li><p>re-scaling by dividing the standard deviation.</p></li>
</ul>
<p>The calculation formula for an input vector <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(H\)</span> feature dimension is given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-llm-arch-layer-nomalization-formula">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-chapter-llm-arch-layer-nomalization-formula" title="Link to this equation">#</a></span>\[
\operatorname{LayerNorm}(x) =\frac{x-\mu}{\sqrt{\sigma+\epsilon}} \cdot \gamma+\beta
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the mean across feature dimensions, i.e., <span class="math notranslate nohighlight">\(\mu = \frac{1}{H} \sum_{i=1}^H x_i \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation across feature dimensions, i.e.,
<span class="math notranslate nohighlight">\(\sigma =\sqrt{\frac{1}{H} \sum_{i=1}^H\left(x_i-\mu\right)^2+\epsilon}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a small number acting as regularizer for division.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable scaling and shifting parameters</p></li>
</ul>
</section>
<section id="rms-norm-root-mean-square-norm">
<h3><span class="section-number">9.3.2. </span>RMS Norm (Root Mean Square Norm)<a class="headerlink" href="#rms-norm-root-mean-square-norm" title="Link to this heading">#</a></h3>
<p>A common hypothesis on why layer normalization can help stalize training and boost model convergence is the capability in  handling re-centering and re-scaling of both inputs and weight matrix. RMSNorm <span id="id3">[<a class="reference internal" href="../chapter_prompt/basic_prompt.html#id1502" title="Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: https://arxiv.org/abs/1910.07467, arXiv:1910.07467.">ZS19</a>]</span> is a technique aiming to achieve similar model training stablizing benefit with a reduced computational overhead compared to LayerNorm. RMSNorm hypothesizes that only the re-scaling component is necessary and proposes the following simplified normalization formula</p>
<div class="math notranslate nohighlight">
\[
\operatorname{RMSNorm}(x)=\frac{x}{\sqrt{\frac{1}{H} \sum_{i=1}^H x_i^2}} \cdot \gamma
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is learnable parameter. Experiments show that RMSNorm can achieve on-par performance with LayerNorm with much reduced training cost.</p>
<p>In the following, we summarize the differences between RMSNorm and LayerNorm</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Computational complexity</strong></span></p></div>
<p class="sd-card-text"><strong>LayerNorm</strong> involves both mean and variance calculation for each normalization layer, which brings sizable computational cost for high-dimensional inputs in LLM (e.g., GPT-3 <span class="math notranslate nohighlight">\(d_model = 12288\)</span>).
<strong>RMSNorm</strong>, on the other hand, only keeps the variance calculation, reducing the normalization cost by half and increses efficiency</p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Gradient propogation</strong></span></p></div>
<p class="sd-card-text"><strong>LayerNorm</strong> stablizes the input distribution between layers through normalization and benefits deep networks training by alleviating the problem of vanishing or exploding gradients. However, LayerNorm can also be affected by noise and input shifts when calculating the mean, potentially leading to unstable gradient propagation.
<strong>RMSNorm</strong>, by using only RMS for normalization, can provide a more robust, smoother gradient flow, especially in deeper networks. It reduces the impact of mean on gradient fluctuations, thereby improving the stability and speed of training.</p>
</div>
<div class="sd-card-footer docutils">
<p class="sd-card-text">See <span id="id4">[<a class="reference internal" href="../chapter_prompt/basic_prompt.html#id1502" title="Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: https://arxiv.org/abs/1910.07467, arXiv:1910.07467.">ZS19</a>]</span> for math derivation</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="layer-normalization-position">
<h3><span class="section-number">9.3.3. </span>Layer normalization position<a class="headerlink" href="#layer-normalization-position" title="Link to this heading">#</a></h3>
<p><span id="id5">[<a class="reference internal" href="#id392" title="Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, 10524–10533. PMLR, 2020.">XYH+20</a>]</span></p>
<p>The residual connection <span class="math notranslate nohighlight">\(x+F(x)\)</span> in the Transformer layer will modify the variance of input <span class="math notranslate nohighlight">\(x\)</span>. To see this, let the variance of <span class="math notranslate nohighlight">\(x\)</span> be <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> and the variance of <span class="math notranslate nohighlight">\(F(x)\)</span> be <span class="math notranslate nohighlight">\(\sigma_2^2\)</span>. Then the variance of <span class="math notranslate nohighlight">\(x + F(x)\)</span> will be given by</p>
<div class="math notranslate nohighlight">
\[ 
Var[x + F(x)] = \sigma_1^2 + \sigma_2^2 + \rho \sigma_1\sigma_2
\]</div>
<p>The Post-Norm thus can stablize the variance of the output by applying the LayerNorm after the residual connection, which is given by</p>
<div class="math notranslate nohighlight">
\[\operatorname{PostNorm Output} = \operatorname{LayerNorm}(X + \operatorname{SubLayer}(X))\]</div>
<p>Here the SubLayer could be the FeedForward Layer or the Attention Layer.</p>
<p>Clearly, the normalization will reduce the effect of identity mapping <span class="math notranslate nohighlight">\(I(x) = x\)</span> and therefore the gradient flow via the residual connection (aka high-way connection).
As a result, using Post-Norm in general will require carefully designed learning rate warm-up stage (the optimization starts with
a small/tiny learning rate, and then gradually increases it to a pre-defined maximum value within certain number of steps.) to speed up model training and covergence, including learning rate warm-up and other hyper-parameter tuning.</p>
<p>When it comes to training very deep models, Post-norm can lead to more unstable gradients during training, especially in very deep networks. This can lead to slower convergence and increased likelihood of training failure.</p>
<p>Pre-LN Transformers without the warm-up stage can reach comparable results
with baselines while requiring significantly less training time and hyper-parameter tuning on a
wide range of applications.</p>
<div class="math notranslate nohighlight">
\[\operatorname{PreNorm Output} = X + \operatorname{SubLayer}(\operatorname{LayerNorm}(X))\]</div>
<p>Intuitively, as the residual connection route <span class="math notranslate nohighlight">\(X\)</span> is not being normalized, the gradient flow via the idenity mapping is therefore not compromised. This help the training of very deep neural networks by mitigating the vanishing gradient issue.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Pre-Norm</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">In the Pre-Norm architecture, the normalization operation (RMS Norm or Layer Norm) is performed before the self-attention or feed-forward neural network (FFN) calculations. In other words, the input to each layer is first normalized before being passed to the attention or feed-forward layers.</p></li>
<li><p class="sd-card-text">Pre-Norm ensures that the magnitude of inputs remains within a stable range in deep networks, which is particularly beneficial for models with long-range dependencies. By performing normalization operations early, the model can learn from more stable inputs, thus helping to address the problem unstable gradients in deep models.</p></li>
<li><p class="sd-card-text">LLMs like GPT, LLama are using Pre-Norm design</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Post-Norm</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">In the Post-Norm architecture, the normalization operation is performed after the self-attention or FFN calculations. The model first goes through unnormalized operations, and finally, the results are normalized to ensure balanced model outputs.</p></li>
<li><p class="sd-card-text">Post-Norm can achieve good convergence effects in the early stages of training, performing particularly well in shallow models. However, in deep networks, the drawback of Post-Norm is that it may lead to gradient instability during the training process, especially as the network depth increases, gradients may become increasingly unstable during propagation.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="layer-normalization-example-choices">
<h3><span class="section-number">9.3.4. </span>Layer normalization example choices<a class="headerlink" href="#layer-normalization-example-choices" title="Link to this heading">#</a></h3>
<p>The core advantages of RMS Pre-Norm lie in its computational simplicity and gradient stability, making it an effective normalization choice for deep neural networks, especially large language models. This is exampified by the fact that LLaMa series started to use Pre-RMSNorm whereas GPT-3 model used Pre-LayerNorm.</p>
<ul class="simple">
<li><p>Improved computational efficiency: As RMS Norm omits mean calculation, it reduces the computational load for each layer, which is particularly important in deep networks. Compared to traditional Layer Norm, RMS Norm can process high-dimensional inputs more efficiently.</p></li>
<li><p>Enhanced gradient stability: RMS Pre-Norm can reduce instances of vanishing gradients, especially in deep networks. This normalization method improves training efficiency by smoothing gradient flow.</p></li>
<li><p>Suitable for large-scale models: For models like LLaMA, RMS Pre-Norm supports maintaining a relatively small model size while ensuring powerful performance. This allows the model to maintain good generalization capabilities without increasing complexity.</p></li>
</ul>
</section>
</section>
<section id="self-attention-and-variants">
<h2><span class="section-number">9.4. </span>Self-attention and Variants<a class="headerlink" href="#self-attention-and-variants" title="Link to this heading">#</a></h2>
<p>Certainly! I’ll provide a detailed summary of different attention modules used in Large Language Models (LLMs), including Multi-Head Attention (MHA), Grouped Query Attention (GQA), and others. I’ll explain their mechanisms, advantages, and use cases.</p>
<section id="multi-head-attention-mha">
<h3><span class="section-number">9.4.1. </span>Multi-Head Attention (MHA)<a class="headerlink" href="#multi-head-attention-mha" title="Link to this heading">#</a></h3>
<p>Multi-Head Attention is the foundation of many transformer-based models, including the original transformer architecture.</p>
<p>Computation of an MHA given input <span class="math notranslate nohighlight">\(X\)</span> matrix and project matrices <span class="math notranslate nohighlight">\(W^Q_i, W^K_i, W^V_i\)</span></p>
<div class="math notranslate nohighlight">
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\]</div>
<p>where each head is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{head}_i = \text{Attention}(XW^Q_i, XW^K_i, XW^V_i)
\]</div>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V
\]</div>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-mha">
<a class="reference internal image-reference" href="../../_images/MHA.png"><img alt="../../_images/MHA.png" src="../../_images/MHA.png" style="width: 267.5px; height: 262.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.2 </span><span class="caption-text">Multi-head attention has <span class="math notranslate nohighlight">\(H\)</span> query, key, and value heads for each token.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-mha" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Advantages</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Improves the model’s overall learning capacity</p></li>
<li><p class="sd-card-text">Different heads allow the model to jointly attend to information from different representation subspaces</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Drawbacks</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Computational complexity scales quadratically with sequence length (i.e., huge cost for long context applications)</p></li>
<li><p class="sd-card-text">During inference stage, each head has its own key and value to cache, bring additional memory burden to inference process.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="multi-query-attention-mqa">
<h3><span class="section-number">9.4.2. </span>Multi Query Attention (MQA)<a class="headerlink" href="#multi-query-attention-mqa" title="Link to this heading">#</a></h3>
<p><span id="id6">[<a class="reference internal" href="#id1521" title="Noam Shazeer. Fast transformer decoding: one write-head is all you need. 2019. URL: https://arxiv.org/abs/1911.02150, arXiv:1911.02150.">Sha19</a>]</span></p>
<p>MQA reduces <span class="math notranslate nohighlight">\(H\)</span> key and value heads in MHA to a single key and value head, reducing the size of the key-value cache by a factor of <span class="math notranslate nohighlight">\(H\)</span>. However,
larger models generally scale the number of heads (e.g., GPT-2 has 12 heads; GPT-3 has 96 heads), such that multi-query attention represents a more
aggressive cut in both memory bandwidth and capacity.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-mqa">
<a class="reference internal image-reference" href="../../_images/MHA.png"><img alt="../../_images/MHA.png" src="../../_images/MHA.png" style="width: 267.5px; height: 262.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.3 </span><span class="caption-text">Multi-head attention has <span class="math notranslate nohighlight">\(H\)</span> query, and one shared single key head and single value head for each token.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-mqa" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Advantages</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">During inference stage, each head has its own key and value to cache, bring additional memory burden to inference process.</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Drawbacks</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Computational complexity scales quadratically with sequence length (i.e., huge cost for long context applications)</p></li>
<li><p class="sd-card-text">Modeling capacity is largely compromised due to the usage of single head, leading to quality degradation.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="grouped-query-attention-gqa">
<h3><span class="section-number">9.4.3. </span>Grouped Query Attention (GQA)<a class="headerlink" href="#grouped-query-attention-gqa" title="Link to this heading">#</a></h3>
<p><span id="id7">[<a class="reference internal" href="#id1520" title="Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: training generalized multi-query transformer models from multi-head checkpoints. 2023. URL: https://arxiv.org/abs/2305.13245, arXiv:2305.13245.">ALTdJ+23</a>]</span>
GQA is an optimization of MHA and MQA that reduces computational complexity while maintaining performance.</p>
<p>A generalization of MQA which uses an intermediate (more than one, less than number of query heads) number of key-value heads.
GQA is shown to achieve quality close to MHA with comparable speed to MQA</p>
<p>Formula:</p>
<div class="math notranslate nohighlight">
\[
\text{GQA}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
\text{head}_i = \text{Attention}(QW^Q_i, KW^K_{g(i)}, VW^V_{g(i)})
\]</div>
<p><span class="math notranslate nohighlight">\(g(i)\)</span> is a function that maps head index to group index.</p>
<figure class="align-default" id="chapter-llm-arch-fig-fundamentals-attention-gqa">
<a class="reference internal image-reference" href="../../_images/GQA.png"><img alt="../../_images/GQA.png" src="../../_images/GQA.png" style="width: 448.5px; height: 340.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.4 </span><span class="caption-text">GQA divides the key and value heads into multiple groups. Within each group, a single shared
key and value heads are attended to by query heads. GQA interpolats between MHA and MQA.</span><a class="headerlink" href="#chapter-llm-arch-fig-fundamentals-attention-gqa" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Advantages:</p>
<ul class="simple">
<li><p>Reduces parameters and computation compared to MHA</p></li>
<li><p>Maintains most of the performance of MHA</p></li>
</ul>
<p>Use cases:</p>
<ul class="simple">
<li><p>Large-scale language models where efficiency is crucial</p></li>
</ul>
</section>
<section id="sliding-window-attention">
<h3><span class="section-number">9.4.4. </span>Sliding Window Attention<a class="headerlink" href="#sliding-window-attention" title="Link to this heading">#</a></h3>
<p>This method restricts attention to a local window around each token.</p>
<p>Key idea:</p>
<ul class="simple">
<li><p>Each token attends only to a fixed number of neighboring tokens</p></li>
</ul>
<p>Formula:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q_i, K_{i-w:i+w}, V_{i-w:i+w})
\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> is the window size.</p>
<p>Advantages:</p>
<ul class="simple">
<li><p>Linear complexity with sequence length</p></li>
<li><p>Useful for tasks requiring local context</p></li>
</ul>
<p>Drawbacks:</p>
<ul class="simple">
<li><p>Limited in capturing long-range dependencies</p></li>
</ul>
</section>
<section id="sparse-attention">
<h3><span class="section-number">9.4.5. </span>Sparse Attention<a class="headerlink" href="#sparse-attention" title="Link to this heading">#</a></h3>
<p>Sparse Attention introduces patterns of sparsity in the attention matrix.</p>
<p>Key idea:</p>
<ul class="simple">
<li><p>Predefined or learned patterns determine which tokens can attend to which other tokens</p></li>
</ul>
<p>Advantages:</p>
<ul class="simple">
<li><p>Can capture both local and global context</p></li>
<li><p>Reduces computational complexity</p></li>
</ul>
<p>Examples:</p>
<ul class="simple">
<li><p>Longformer: combines sliding window attention with global attention</p></li>
<li><p>Big Bird: uses random, window, and global attention patterns</p></li>
</ul>
</section>
</section>
<section id="activation">
<h2><span class="section-number">9.5. </span>Activation<a class="headerlink" href="#activation" title="Link to this heading">#</a></h2>
<p>Certainly! I’ll expand on each of these activation functions and their formulas, providing more context and explanations.</p>
<ol class="arabic simple">
<li><p>FFN (Feed-Forward Network) Block:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
FFN(x)=f(xW_1+b_1)W_2+b_2
\]</div>
<p>The FFN block is a crucial component in many transformer-based architectures. It typically consists of two linear transformations with a non-linear activation function in between.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the input vector</p></li>
<li><p><span class="math notranslate nohighlight">\(W_1\)</span> and <span class="math notranslate nohighlight">\(W_2\)</span> are weight matrices</p></li>
<li><p><span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(b_2\)</span> are bias vectors</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is an activation function (often ReLU or GELU)</p></li>
</ul>
<p>The FFN block helps in capturing complex patterns and increasing the model’s capacity. The first transformation (<span class="math notranslate nohighlight">\(xW_1+b_1\)</span>) usually projects the input to a higher dimensional space (often 4 times the input dimension), and the second transformation projects it back to the original dimension.</p>
<ol class="arabic simple" start="2">
<li><p>GeLU (Gaussian Error Linear Unit):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\operatorname{GeLU}(x) \approx 0.5x\left(1+\tanh \left(\sqrt{\frac{2}{\pi}}\left(x+0.044715x^3\right)\right)\right)
\]</div>
<p>GeLU is a smooth approximation of the ReLU function that incorporates properties of the Gaussian cumulative distribution function.</p>
<ul class="simple">
<li><p>It’s differentiable everywhere, unlike ReLU</p></li>
<li><p>For positive inputs, it behaves similarly to ReLU</p></li>
<li><p>For negative inputs, it has a small, smooth curve instead of being zero</p></li>
</ul>
<p>The formula provided is an approximation of the true GeLU function, which is computationally efficient while maintaining the key properties of GeLU. It’s used in models like BERT and GPT-3, often outperforming ReLU in deep networks.</p>
<ol class="arabic simple" start="3">
<li><p>Swish:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\operatorname{Swish}_\beta(x)=x \cdot \sigma(\beta x)
\]</div>
<p>Swish is a self-gated activation function introduced by Google Brain.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function: <span class="math notranslate nohighlight">\(\sigma(x) = \frac{1}{1+e^{-x}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is a trainable parameter or can be set to 1</p></li>
</ul>
<p>Properties of Swish:</p>
<ul class="simple">
<li><p>Smooth and non-monotonic</p></li>
<li><p>Unbounded above and bounded below</p></li>
<li><p>Approaches linear function for large positive inputs</p></li>
<li><p>Can outperform ReLU in very deep networks</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>GLU (Gated Linear Unit) in FFN:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; GLU(x)=\sigma(xW+b) \otimes xV \\
&amp; FFN_{GLU}=(f(xW_1) \otimes xV)W_2
\end{aligned}
\end{split}\]</div>
<p>GLU introduces a gating mechanism to the FFN block.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function</p></li>
<li><p><span class="math notranslate nohighlight">\(\otimes\)</span> represents element-wise multiplication</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is an activation function (often GeLU)</p></li>
<li><p><span class="math notranslate nohighlight">\(W\)</span>, <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(W_1\)</span>, and <span class="math notranslate nohighlight">\(W_2\)</span> are weight matrices</p></li>
</ul>
<p>The gating mechanism allows the network to control information flow, potentially capturing more complex dependencies. The sigmoid function acts as a gate, determining how much of the linear transformation should pass through.</p>
<ol class="arabic simple" start="5">
<li><p>GeGLU (GeLU-based Gated Linear Unit):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
GeGLU(x)=GeLU(xW) \otimes xV
\]</div>
<p>GeGLU combines the GeLU activation with the gating mechanism of GLU.</p>
<ul class="simple">
<li><p>GeLU is applied to one branch (<span class="math notranslate nohighlight">\(xW\)</span>)</p></li>
<li><p>The other branch (<span class="math notranslate nohighlight">\(xV\)</span>) remains linear</p></li>
<li><p>Element-wise multiplication combines the two branches</p></li>
</ul>
<p>This formulation can provide the benefits of both GeLU activation and gated mechanisms, potentially leading to improved performance in some tasks.</p>
<ol class="arabic simple" start="6">
<li><p>SwiGLU (Swish-based Gated Linear Unit):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
SwiGLU=\text{Swish}_\beta(xW) \otimes xV
\]</div>
<p>SwiGLU replaces the GeLU function in GeGLU with the Swish activation.</p>
<ul class="simple">
<li><p>Swish is applied to one branch (<span class="math notranslate nohighlight">\(xW\)</span>)</p></li>
<li><p>The other branch (<span class="math notranslate nohighlight">\(xV\)</span>) remains linear</p></li>
<li><p>Element-wise multiplication combines the two branches</p></li>
</ul>
<p>The use of Swish can potentially provide different dynamics compared to GeLU, and the trainable parameter <span class="math notranslate nohighlight">\(\beta\)</span> in Swish adds an extra degree of flexibility to the model.</p>
<p>These variations on activation functions and gating mechanisms represent ongoing research in improving the performance and capabilities of neural networks, especially in the context of large language models. Each has its own strengths and may be more suitable for different types of tasks or model architectures.</p>
<p>Examples in LLM:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>LLM</p></th>
<th class="head text-center"><p>Activation Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>GPT3</p></td>
<td class="text-center"><p>GeLU</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>LLaMA</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>LLaMA2</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>baichuan</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>ChatGLM- <br> 6B</p></td>
<td class="text-center"><p>GeLU</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>ChatGLM2- <br> 6B</p></td>
<td class="text-center"><p>SwiGLU</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Bloom</p></td>
<td class="text-center"><p>GeLU</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Falcon</p></td>
<td class="text-center"><p>GeLU</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="tokenziation-vocabulary-and-weight-tying">
<h2><span class="section-number">9.6. </span>Tokenziation, vocabulary, and weight tying<a class="headerlink" href="#tokenziation-vocabulary-and-weight-tying" title="Link to this heading">#</a></h2>
<section id="bpe-tokenization">
<h3><span class="section-number">9.6.1. </span>BPE Tokenization<a class="headerlink" href="#bpe-tokenization" title="Link to this heading">#</a></h3>
<p>Byte Pair Encoding (BPE) is a commonly used subword tokenization algorithm in NLP. It starts with individual characters and iteratively merges the most frequent pairs to create new subword units, repeating this process N times to build the final subword vocabulary. The following is a summary of the algorithm.</p>
<div class="proof algorithm admonition" id="BPE-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (BPE)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Word list <span class="math notranslate nohighlight">\(W\)</span>, Number of desired merges <span class="math notranslate nohighlight">\(N\)</span></p>
<p><strong>Output</strong> Subword vocabulary <span class="math notranslate nohighlight">\(V = \emptyset\)</span></p>
<ol class="arabic simple">
<li><p>Represent each word as a sequence of characters</p></li>
<li><p>Initialize the subword vocabulary as a set of single characters</p></li>
<li><p>For i in 1 to N:
3.1 Calculate the frequency of each consecutive character pair
3.2 Find the character pair <span class="math notranslate nohighlight">\((x, y)\)</span> with the highest frequency
3.3 Merge the character pair <span class="math notranslate nohighlight">\(c = (x, y)\)</span>, update the subword vocabulary <span class="math notranslate nohighlight">\(V = V \cup c\)</span>.</p></li>
<li><p>Return the subword vocabulary <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
</ol>
</section>
</div><div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 9.1 </span></p>
<section class="example-content" id="proof-content">
<p>GPT-2’s vocabulary size is 50,257, corresponding to 256 basic byte tokens, a special end-of-text token, and 50,000 tokens obtained through merging process.</p>
</section>
</div></section>
<section id="from-bpe-to-bbpe">
<h3><span class="section-number">9.6.2. </span>From BPE to BBPE<a class="headerlink" href="#from-bpe-to-bbpe" title="Link to this heading">#</a></h3>
<p>BPE (Byte Pair Encoding) and BBPE (Byte-level BPE) are both subword tokenization following the same idea of merging algorithm <a class="reference internal" href="#BPE-algorithm">Algorithm 9.1</a> but operating on different granularities.</p>
<p>In short, BPE Works on character or unicode level whereas BBPE works on byte level of UTF-8 representation. Their comparison is summarized as the following.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<span style="background-color: #e4ac94"><strong>BPE</strong></span></div>
<p class="sd-card-text">In BPE, the generation of subwords is more consistent with linguistic rules (e.g., utilizing word roots). The subword choices often better align with common vocabulary.</p>
<p class="sd-card-text">However, it often requires different treatments for different languages (like English vs Chinese) and it cannot effectively represent emojis and unseen special tokens.</p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<span style="background-color: #b4c9da"><strong>BBPE</strong></span></div>
<p class="sd-card-text">BBPE has following advantages:</p>
<ul class="simple">
<li><p class="sd-card-text">It can process all character sets (including Unicode characters), making it suitable for multilingual scenarios.</p></li>
<li><p class="sd-card-text">It provides good support for unconventional symbols and emojis.</p></li>
</ul>
<p class="sd-card-text">However, as BBPE is working on smaller granularity level than characters, it might result in larger vocabulary size (i.e., larger embedding layers) and unnatural subword units.</p>
</div>
</div>
</div>
</div>
</div>
<p>Currently, many mainstream large language models (such as the GPT series, Mistral<a class="footnote-reference brackets" href="#footnote1" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, etc.) primarily use BBPE instead of BPE. The reasons for the widespread adoption of this method include:</p>
<ul class="simple">
<li><p>Ability to process multilingual text: Large models typically need to handle vast amounts of text in different languages. BBPE operates at the byte level, allowing it to process all Unicode characters, performing particularly well for languages with complex character sets (such as Chinese, Korean, Arabic).</p></li>
<li><p>Unified tokenization: The BBPE method does not rely on language-specific character structures. Therefore, it can be uniformly applied to multilingual tasks without adding extra complexity, simplifying the tokenization process in both pre-training and downstream tasks.</p></li>
<li><p>Compatibility with emojis and special characters: Modern large language models need to process large amounts of internet data, which contains many emojis, special characters, and non-standard symbols. BBPE can better support these types of symbols.</p></li>
</ul>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 9.1 </span> (What does Byte-level mean?)</p>
<section class="remark-content" id="proof-content">
<p>“Byte-level” in the context of BBPE means that the algorithm operates on individual bytes of data rather than on characters or higher-level text units. Note that characters are typically encoded using schemes like UTF-8, where a single character might be represented by one or more bytes. In other words, BBPE treats the input as a sequence of raw bytes, without interpreting them as characters or considering character boundaries.</p>
<p>Below is more context about UTF-8 encoding.</p>
<ol class="arabic simple">
<li><p>ASCII encoding:</p>
<ul class="simple">
<li><p>In the original ASCII encoding, each character is represented by a single byte (8 bits).</p></li>
<li><p>This allows for 256 different characters (2^8 = 256).</p></li>
<li><p>ASCII mainly covers English letters, numbers, and some basic symbols.</p></li>
</ul>
</li>
<li><p>Unicode and UTF-8:</p>
<ul class="simple">
<li><p>Unicode was developed to represent characters from all writing systems in the world.</p></li>
<li><p>UTF-8 is a variable-width encoding scheme for Unicode.</p></li>
<li><p>In UTF-8, characters can be encoded using 1 to 4 bytes:</p>
<ul>
<li><p>ASCII characters still use 1 byte</p></li>
<li><p>Many other characters use 2 or 3 bytes</p></li>
<li><p>Some very rare characters use 4 bytes</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Examples:</p>
<ul class="simple">
<li><p>The letter ‘A’ (U+0041 in Unicode) is represented as a single byte: 01000001</p></li>
<li><p>The Euro symbol ‘€’ (U+20AC) is represented by three bytes: 11100010 10000010 10101100</p></li>
<li><p>The emoji ‘😊’ (U+1F60A) is represented by four bytes: 11110000 10011111 10011000 10001010</p></li>
</ul>
</li>
</ol>
<p>This multi-byte representation for single characters is why text processing algorithms that work at the character level can be more complex than those that work at the byte level, especially when dealing with multilingual text.</p>
</section>
</div></section>
</section>
<section id="parameter-composition-in-transformer-models">
<h2><span class="section-number">9.7. </span>Parameter composition in Transformer models<a class="headerlink" href="#parameter-composition-in-transformer-models" title="Link to this heading">#</a></h2>
<section id="input-layer">
<h3><span class="section-number">9.7.1. </span>Input layer<a class="headerlink" href="#input-layer" title="Link to this heading">#</a></h3>
<p>Word embedding: <span class="math notranslate nohighlight">\(n_{vocab} \times d_{model}\)</span>
Position embedding: <span class="math notranslate nohighlight">\(n_{max\_len} \times d_{model}\)</span></p>
</section>
<section id="attention-layer">
<h3><span class="section-number">9.7.2. </span>Attention layer<a class="headerlink" href="#attention-layer" title="Link to this heading">#</a></h3>
<p>In general <span class="math notranslate nohighlight">\(n_{head} \times d_{head} = d_{model}\)</span>
QKV transformation matrix for <span class="math notranslate nohighlight">\(n_{head}\)</span>: <span class="math notranslate nohighlight">\(3 \times n_{head} \times d_{model} \times d_{head}\)</span>
There is a transformation matrix takes the multi-head attention output <span class="math notranslate nohighlight">\(n_{head} \times d_{head}\)</span> as the input and outputs a <span class="math notranslate nohighlight">\(d_{model}\)</span> feature vector. This transformation matrix has weight parameters <span class="math notranslate nohighlight">\(n_{head} \times d_{head} \times d_{model}\)</span>
In total, we have <span class="math notranslate nohighlight">\(4n_{head}d_{head}d_{model} = 4d_{model}^2\)</span>.</p>
</section>
<section id="feed-forward-layer">
<h3><span class="section-number">9.7.3. </span>Feed-forward layer<a class="headerlink" href="#feed-forward-layer" title="Link to this heading">#</a></h3>
<p>The feed-forward network after the attention layer is a two-layer, with two weight matrices of the sizes <span class="math notranslate nohighlight">\(d_{model} \times d_{ff}\)</span> and <span class="math notranslate nohighlight">\(d_{ff} \times d_{model}\)</span> and two bias vectors of the sizes <span class="math notranslate nohighlight">\(d_{model}\)</span> and <span class="math notranslate nohighlight">\(d_{ff}\)</span>.
In general <span class="math notranslate nohighlight">\(d_{ff} = 4d_{model}\)</span>, so the total number of parameters are <span class="math notranslate nohighlight">\(8d_{model}^2 + 5d_{model}\)</span>.</p>
</section>
<section id="output-layer">
<h3><span class="section-number">9.7.4. </span>Output layer<a class="headerlink" href="#output-layer" title="Link to this heading">#</a></h3>
<p>The weight-matrix in the output Softmax layer is often tied to the embedding layer.</p>
</section>
<section id="total-weight">
<h3><span class="section-number">9.7.5. </span>Total weight<a class="headerlink" href="#total-weight" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[n_{vocab} \times d_{model} + n_{max\_len} \times d_{model} + n_{layer}(4d_{model}^2 + 8d_{model}^2 + 5d_{model})\]</div>
<p><span class="math notranslate nohighlight">\(n_{max\_len} = 2048\)</span> in GPT-3</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1522">
<caption><span class="caption-number">Table 9.1 </span><span class="caption-text">Parameters in a Transformer</span><a class="headerlink" href="#id1522" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Module</p></th>
<th class="head text-left"><p>Computation</p></th>
<th class="head text-left"><p>Parameter Name</p></th>
<th class="head text-left"><p>Shape</p></th>
<th class="head text-left"><p>Parameter Number</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Attention</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\({Q} / {K} / {V}\)</span></p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, {d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(3 d^2+3 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Output 映射</p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, {d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(d^2+d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>layernorm</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>FFN</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(f_1\)</span></p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}, 4 {~d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 d^2+d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(f_2\)</span></p></td>
<td class="text-left"><p>weight / bias</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([4 {~d}, {~d}] /[4 {~d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 d^2+4 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Layernorm</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{d}] /[{d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Embedding</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{V}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Total</strong></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(V d+L\left(12 d^2+13 d\right)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>The total number of parameters scales linearly with number of layers <span class="math notranslate nohighlight">\(L\)</span> and quadratically with model hidden dimensionality <span class="math notranslate nohighlight">\(d\)</span>.</p>
<div class="proof example admonition" id="example-3">
<p class="admonition-title"><span class="caption-number">Example 9.2 </span></p>
<section class="example-content" id="proof-content">
<p>Take the following GPT-3 13B and 175B as an example, 175B model has approximate 2.4 times of <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(d_{model}\)</span>. Extrapolating from 13B model, we estimate the 175B model to have model parameters of <span class="math notranslate nohighlight">\(13\times 2.4^3 = 179B\)</span>, which is very close.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(n_{\text{params}}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(n_{\text{layers}}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d_{\text{model}}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(n_{\text{heads}}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d_{\text{head}}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GPT-3 13B</p></td>
<td><p>13.0B</p></td>
<td><p>40</p></td>
<td><p>5140</p></td>
<td><p>40</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 175B or “GPT-3”</p></td>
<td><p>175.0B</p></td>
<td><p>96</p></td>
<td><p>12288</p></td>
<td><p>96</p></td>
<td><p>128</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</div></section>
</section>
<section id="dense-architecture-examples">
<h2><span class="section-number">9.8. </span>Dense Architecture Examples<a class="headerlink" href="#dense-architecture-examples" title="Link to this heading">#</a></h2>
</section>
<section id="summary">
<h2><span class="section-number">9.9. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table" id="id1523">
<caption><span class="caption-number">Table 9.2 </span><span class="caption-text">Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and MCL denotes the maximum context length during training.</span><a class="headerlink" href="#id1523" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model</p></th>
<th class="head text-center"><p>Size</p></th>
<th class="head text-center"><p>Normalization</p></th>
<th class="head text-center"><p>PE</p></th>
<th class="head text-center"><p>Activation</p></th>
<th class="head text-center"><p>Bias</p></th>
<th class="head text-center"><p>#L</p></th>
<th class="head text-center"><p>#H</p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(d_{\text {model }}\)</span></p></th>
<th class="head text-center"><p>MCL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>GPT3 [55]</p></td>
<td class="text-center"><p>175B</p></td>
<td class="text-center"><p>Pre LayerNorm</p></td>
<td class="text-center"><p>Learned</p></td>
<td class="text-center"><p>GeLU</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\checkmark\)</span></p></td>
<td class="text-center"><p>96</p></td>
<td class="text-center"><p>96</p></td>
<td class="text-center"><p>12288</p></td>
<td class="text-center"><p>2048</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Llama</p></td>
<td class="text-center"><p>207B</p></td>
<td class="text-center"><p>Pre RMSNorm</p></td>
<td class="text-center"><p>Learned</p></td>
<td class="text-center"><p>SwiGLU</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\checkmark\)</span></p></td>
<td class="text-center"><p>64</p></td>
<td class="text-center"><p>128</p></td>
<td class="text-center"><p>16384</p></td>
<td class="text-center"><p>1024</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Qwen 2 <span id="id9">[<a class="reference internal" href="#id1515" title="An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. 2024. URL: https://arxiv.org/abs/2407.10671, arXiv:2407.10671.">YYH+24</a>]</span></p></td>
<td class="text-center"><p>72B</p></td>
<td class="text-center"><p>Pre RMSNorm</p></td>
<td class="text-center"><p>RoPe</p></td>
<td class="text-center"><p>SwiGLU</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\checkmark\)</span></p></td>
<td class="text-center"><p>80</p></td>
<td class="text-center"><p>64</p></td>
<td class="text-center"><p>8192</p></td>
<td class="text-center"><p>…</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="llama-architectures">
<h2><span class="section-number">9.10. </span>LLama architectures<a class="headerlink" href="#llama-architectures" title="Link to this heading">#</a></h2>
</section>
<section id="computation-breadown-during-forward-pass">
<h2><span class="section-number">9.11. </span>Computation breadown during forward pass<a class="headerlink" href="#computation-breadown-during-forward-pass" title="Link to this heading">#</a></h2>
<p>b: batch_size
s: seq_len
d: d_model
V: vocabulary_size
L: n_layers</p>
<p>以矩阵乘为例, 输入 <span class="math notranslate nohighlight">\([M, K] \times[K, N]=[M, N]\)</span>, 计算时间复杂度为 <span class="math notranslate nohighlight">\(2 M N K\)</span> 。也就是说输出矩阵 <span class="math notranslate nohighlight">\(M N\)</span> 个元素, 每个元素经过一次乘法和一次加法运算。</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1524">
<caption><span class="caption-number">Table 9.3 </span><span class="caption-text">Computation breakdown</span><a class="headerlink" href="#id1524" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Module</p></th>
<th class="head text-left"><p>Computation</p></th>
<th class="head text-left"><p>Matrix Shape Changes</p></th>
<th class="head text-left"><p>FLOPs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Attention</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\({Q} / {K} / {V}\)</span> Projection</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, {d}] \times [{~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(3\times 2 b s d^2\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(Q K^T\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~d}] \times [{~b}, {~d}, {~s}]\to[{b}, {s}, {s}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s^2 d\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>score <span class="math notranslate nohighlight">\( \times V\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~s}] \times [{~b}, {~s}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s^2 d\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Output</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, {d}] \times[{~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s d^2\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>FFN</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(f_1\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, {~d}] \times[{~d}, 4 {~d}] \to [{b}, {s}, 4 {~d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(8 b s d^2\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(f_2\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{~b}, {~s}, 4 {~d}] \times[4 {~d}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(8 b s d^2\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Embedding</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\([{b}, {s}, 1] \times[{~V}, {~d}]\to[{b}, {s}, {d}]\)</span></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 b s d V\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>In total</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\left(24 b s d^2+4 b d s^2\right) \times L+2 b s d V\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>Some examples and trends</p>
<p>Qwen2 0.5B</p>
<p>Llama 7B</p>
<p>Llama 405B</p>
</section>
<section id="bibliography">
<h2><span class="section-number">9.12. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<p>Good reviews <span id="id10">[<a class="reference internal" href="#id1514" title="Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. 2024. URL: https://arxiv.org/abs/2303.18223, arXiv:2303.18223.">ZZL+24</a>]</span></p>
<div class="docutils container" id="id11">
<div role="list" class="citation-list">
<div class="citation" id="id1520" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">ALTdJ+23</a><span class="fn-bracket">]</span></span>
<p>Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: training generalized multi-query transformer models from multi-head checkpoints. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a>, <a class="reference external" href="https://arxiv.org/abs/2305.13245">arXiv:2305.13245</a>.</p>
</div>
<div class="citation" id="id1510" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">IS15</a><span class="fn-bracket">]</span></span>
<p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. 2015. URL: <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>, <a class="reference external" href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>.</p>
</div>
<div class="citation" id="id1521" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Sha19</a><span class="fn-bracket">]</span></span>
<p>Noam Shazeer. Fast transformer decoding: one write-head is all you need. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a>, <a class="reference external" href="https://arxiv.org/abs/1911.02150">arXiv:1911.02150</a>.</p>
</div>
<div class="citation" id="id392" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">XYH+20</a><span class="fn-bracket">]</span></span>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In <em>International Conference on Machine Learning</em>, 10524–10533. PMLR, 2020.</p>
</div>
<div class="citation" id="id1515" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">YYH+24</a><span class="fn-bracket">]</span></span>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.10671">https://arxiv.org/abs/2407.10671</a>, <a class="reference external" href="https://arxiv.org/abs/2407.10671">arXiv:2407.10671</a>.</p>
</div>
<div class="citation" id="id1509" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZS19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Biao Zhang and Rico Sennrich. Root mean square layer normalization. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1910.07467">https://arxiv.org/abs/1910.07467</a>, <a class="reference external" href="https://arxiv.org/abs/1910.07467">arXiv:1910.07467</a>.</p>
</div>
<div class="citation" id="id1500" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">ZZL+23</a><span class="fn-bracket">]</span></span>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. <em>arXiv preprint arXiv:2303.18223</em>, 2023.</p>
</div>
<div class="citation" id="id1514" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">ZZL+24</a><span class="fn-bracket">]</span></span>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a>, <a class="reference external" href="https://arxiv.org/abs/2303.18223">arXiv:2303.18223</a>.</p>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnote1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">1</a><span class="fn-bracket">]</span></span>
<p>See <a class="reference external" href="https://docs.mistral.ai/guides/tokenization/">https://docs.mistral.ai/guides/tokenization/</a> for details for tokenizer construction and usage in Mistral LLMs.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_LLM_arch"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_foundation/GPT_series.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>GPT Series</p>
      </div>
    </a>
    <a class="right-next"
       href="LLM_moe_sparse_architectures.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>MOE sparse models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">9.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-embeddings">9.2. Position Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-position">9.2.1. Absolute Position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotary-postion-embedding">9.2.2. Rotary Postion Embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mechanism">9.2.2.1. The mechanism</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practival-implementations">9.2.2.2. Practival Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">9.3. Layer normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-basics">9.3.1. Layer normalization basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-norm-root-mean-square-norm">9.3.2. RMS Norm (Root Mean Square Norm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-position">9.3.3. Layer normalization position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-example-choices">9.3.4. Layer normalization example choices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-and-variants">9.4. Self-attention and Variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-mha">9.4.1. Multi-Head Attention (MHA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-query-attention-mqa">9.4.2. Multi Query Attention (MQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention-gqa">9.4.3. Grouped Query Attention (GQA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sliding-window-attention">9.4.4. Sliding Window Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-attention">9.4.5. Sparse Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">9.5. Activation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenziation-vocabulary-and-weight-tying">9.6. Tokenziation, vocabulary, and weight tying</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpe-tokenization">9.6.1. BPE Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-bpe-to-bbpe">9.6.2. From BPE to BBPE</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-composition-in-transformer-models">9.7. Parameter composition in Transformer models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">9.7.1. Input layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-layer">9.7.2. Attention layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-layer">9.7.3. Feed-forward layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">9.7.4. Output layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#total-weight">9.7.5. Total weight</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-architecture-examples">9.8. Dense Architecture Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">9.9. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-architectures">9.10. LLama architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-breadown-during-forward-pass">9.11. Computation breadown during forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">9.12. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>