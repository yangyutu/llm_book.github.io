
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>20. Text Embedding Fundamentals &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_text_embedding/text_embedding_fundamentals';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="21. LLM Text Embedding" href="text_embedding_LLM.html" />
    <link rel="prev" title="19. Advanced Prompting Techniques" href="../chapter_prompt/advanced_prompt.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">14. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">20. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_embedding_LLM.html">21. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">22. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">23. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">24. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">25. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">26. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/conversational_IR.html">27. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">28. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">29. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Text Embedding Fundamentals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">20.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextual-text-embedding-fundamentals">20.2. Contextual Text Embedding Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anisotropy">20.2.1. Anisotropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-specificity">20.2.2. Context-specificity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-vs-contextual-word-embeddings">20.2.3. Static vs Contextual Word embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-learning-fundamentals">20.3. Contrastive Learning Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosentloss">20.3.1. coSENTLoss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-quality-intrinsic-measurement">20.3.2. Embedding quality intrinsic measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anisotropy-problem-in-embeddings">20.3.3. Anisotropy problem in embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-based-embeddings">20.4. Transformer based embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-bert">20.4.1. Sentence-BERT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simcse">20.4.2. SimCSE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cert">20.4.2.1. CERT</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-t5">20.4.3. Sentence T5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-dataset">20.4.4. Benchmark dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multinli-data">20.4.4.1. MultiNLI data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-semantic-textual-similarity-tasks">20.4.4.2. Standard semantic textual similarity tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-transfer-and-distillation">20.5. Knowledge Transfer and Distillation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-mono-language-to-multi-lingual">20.5.1. From mono language to multi lingual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matryoshka-representation-learning">20.6. Matryoshka Representation Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">20.6.1. Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-embedding-model">20.7. LLM Embedding Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nv-embed">20.7.1. NV-Embed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-embedding-distillation">20.8. LLM Embedding Distillation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#software">20.9. Software</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.10. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="text-embedding-fundamentals">
<span id="chapter-text-embedding-sec-text-embedding-fundamentals"></span><h1><span class="section-number">20. </span>Text Embedding Fundamentals<a class="headerlink" href="#text-embedding-fundamentals" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">20.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>There are many NLP tasks involves determining the relationship of two sentences, including semantic similarity, semantic relation reasoning, questioning answering etc. For example, Quora needs to determine if a question asked by a user has a semantically similar duplicate. The GLUE benchmark as an example [\autoref{ch:neural-network-and-deep-learning:ApplicationNLP:sec:BERTDownstreamTasks}], 6 of them are tasks that require learning sentences Inter-relationship. Specifically,</p>
<p><strong>MRPC</strong>: The Microsoft Research Paraphrase Corpus <span id="id1">[<a class="reference internal" href="#id1342" title="William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). 2005.">DB05</a>]</span> is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.\</p>
<p><strong>QQP</strong>: The <a class="reference external" href="https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs">Quora Question Pairs</a> dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent. As in MRPC, the class distribution in <span class="math notranslate nohighlight">\(\mathrm{QQP}\)</span> is unbalanced <span class="math notranslate nohighlight">\((63 \%\)</span> negative), so we report both accuracy and F1 score. We use the standard test set, for which we obtained private labels from the authors. We observe that the test set has a different label distribution than the training set. \</p>
<p><strong>STS-B</strong>: The Semantic Textual Similarity Benchmark <span id="id2">[<a class="reference internal" href="#id1343" title="Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.">CDA+17</a>]</span> is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data.</p>
<p><strong>Natural language inference</strong>: <span id="id3">[<a class="reference internal" href="#id1355" title="Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.">BAPM15</a>]</span> Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning,
from the lexicon to the content of entire texts.
Natural language inference is the task of determining whether a <em>hypothesis</em> is true (entailment), false (contradiction), or undetermined (neutral) given a <em>premise</em>.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1608">
<caption><span class="caption-number">Table 20.1 </span><span class="caption-text"><em>Entailment</em>, <em>contradiction</em>, and <em>neural</em> examples in a natural language inference task.</span><a class="headerlink" href="#id1608" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p>Premise</p></th>
<th class="head text-center"><p>Label</p></th>
<th class="head text-center"><p>Hypothesis</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>A man inspects the uniform of a figure in some East Asian country.</p></td>
<td class="text-center"><p>Contraction</p></td>
<td class="text-center"><p>The man is sleeping.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>An older and younger man smiling.</p></td>
<td class="text-center"><p>Neutral</p></td>
<td class="text-center"><p>Two men are smiling and laughing at the cats playing on the floor.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>A soccer game with multiple males playing.</p></td>
<td class="text-center"><p>Entailment</p></td>
<td class="text-center"><p>Some men are playing a sport.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Although BERT model and its variant have achieved new state-of-the-art among many sentence-pair classification and regression tasks. It has many practical challenges in tasks like large-scale semantic similarity comparison, clustering, and information retrieval via semantic search, etc. These tasks require that both sentences are fed into the network, which causes a massive computational overhead for large BERT model. Considering the task of finding the most similar pair among <span class="math notranslate nohighlight">\(N\)</span> sentences, then it requires <span class="math notranslate nohighlight">\(N^2/2\)</span> forward pass computation of BERT.</p>
<p>An alternative approach is to derive a semantically meaningful sentence embeddings for each sentence. A sentence embedding is a dense vector representation of a sentence. Sentences with similar semantic meanings are close and sentences with different meanings are apart. With sentence embeddings, similarity search can be realized simply via a distance or similarity metrics, such as cosine similarity. Besides performing similarity comparison between two sentences, the sentence embedding can also be used as generic sentence feature for different NLP tasks.</p>
<p>Early on, there have been efforts directed to derive sentence embedding by aggregating word embeddings. For example, one can average the BERT token embedding output as the sentence embedding. Another common practice is to use the output of the first token (the [CLS] token) as the sentence, which is found to be worse than averaging GloVe embeddings\cite{reimers2019sentence}. Recently, contrastive learning and Siamese type of learning have been successfully applied in deriving sentence embeddings, which will be the focus of our following sections.</p>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 20.1 </span> (Why we need sentence embedding fine tuning)</p>
<section class="remark-content" id="proof-content">
<p>Pre-trained BERT models do not produce efficient and independent sentence embeddings as they always need to be fine-tuned in an end-to-end supervised setting. This is because we can think of a pre-trained BERT model as an indivisible whole and semantics is spread across all layers, not just the final layer. Without fine-tuning, it may be ineffective to use its internal representations independently. It is also hard to handle unsupervised tasks such
as clustering, topic modeling, information retrieval, or semantic search. Because we have to evaluate many sentence pairs during clustering tasks, for instance, this causes massive computational overhead.</p>
</section>
</div></section>
<section id="contextual-text-embedding-fundamentals">
<h2><span class="section-number">20.2. </span>Contextual Text Embedding Fundamentals<a class="headerlink" href="#contextual-text-embedding-fundamentals" title="Link to this heading">#</a></h2>
<section id="anisotropy">
<h3><span class="section-number">20.2.1. </span>Anisotropy<a class="headerlink" href="#anisotropy" title="Link to this heading">#</a></h3>
<p><span id="id4">[<a class="reference internal" href="#id1594" title="Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.">Eth19</a>]</span></p>
<p>Contextualized representations are <strong>anisotropic</strong> in all non-input layers. If word representations from a particular layer were isotropic (i.e., directionally uniform), then the average cosine similarity between uniformly randomly sampled words would be 0. The closer this average is to 1, the more anisotropic the representations. The geometric interpretation of anisotropy is that the word representations all occupy a narrow cone in the vector space rather than being uniform in all directions; the greater the anisotropy, the narrower this cone.</p>
<p>As seen in <a class="reference internal" href="#chapter-text-embedding-fig-text-embedding-anisotropy"><span class="std std-numref">Fig. 20.1</span></a>, this implies that in almost all layers of BERT and GPT-2, the representations of all words occupy a narrow cone in the vector space.</p>
<p>Contextual word embeddings are generally more anisotropic in higher layers.</p>
<p>anisotropy is also viewed as representation degeneration problem <span id="id5">[<a class="reference internal" href="#id1596" title="Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009, 2019.">GHT+19</a>]</span>, where the expressiveness of the embedding model is been compromised.</p>
<figure class="align-default" id="chapter-text-embedding-fig-text-embedding-anisotropy">
<a class="reference internal image-reference" href="../../_images/self_similarity_across_layers.png"><img alt="../../_images/self_similarity_across_layers.png" src="../../_images/self_similarity_across_layers.png" style="width: 730.4000000000001px; height: 289.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.1 </span><span class="caption-text">In almost all layers of BERT and GPT-2, the word representations are anisotropic (i.e., not directionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.</span><a class="headerlink" href="#chapter-text-embedding-fig-text-embedding-anisotropy" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="context-specificity">
<h3><span class="section-number">20.2.2. </span>Context-specificity<a class="headerlink" href="#context-specificity" title="Link to this heading">#</a></h3>
<p>For example, the word <em>dog</em> appears in <em>A panda dog is running on the road</em> and <em>A dog is trying to get bacon off his back</em>.
If a model generated the same representation for <em>dog</em> in both these sentences, we could infer that there was no contextualization; conversely, if the two representations were different, we could infer that they were contextualized to some extent.</p>
<p>It is found that:</p>
<ul class="simple">
<li><p>Contextualized word embeddings are more context-specific in higher layers. Particularly, representations in GPT-2 are the most context-specific, with those in GPT-2’s last layer being almost maximally context-specific.</p></li>
</ul>
<p>In image classification models, lower layers recognize more generic features such as edges while upper layers recognize more class-specific features.</p>
<p>Therefore, it follows that upper layers of neural language models learn more context-specific representations, so
as to predict the next word for a given context
more accurately.</p>
<ul class="simple">
<li><p>Stopwords (e.g., <em>the</em>, <em>of</em>, <em>to</em>) have among the most context-specific representations, meaning that their representation are highly dependent on its context.</p></li>
</ul>
</section>
<section id="static-vs-contextual-word-embeddings">
<h3><span class="section-number">20.2.3. </span>Static vs Contextual Word embeddings<a class="headerlink" href="#static-vs-contextual-word-embeddings" title="Link to this heading">#</a></h3>
<p>A word’s embedding changes when the word resides in different contexts. It is found that, on average, less than 5% of such variance can be explained by the first principal component, which can be viewed as a static embedding. This also indicates that contextually word embedding cannot be easily replaced by a static embedding.</p>
<p>However, we can still use principal components of contextualized representations in <strong>lower layers</strong> as proxy static word embeddings, which can be used in low-resource scenarios. It is found <span id="id6">[<a class="reference internal" href="#id1594" title="Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.">Eth19</a>]</span> that such proxy static embedding can still outperform GloVe and FastText on many benchmarks.</p>
<p>We can create static embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer.</p>
<p>Given that upper layers are much more contextspecific than lower layers, and given that GPT2’s representations are more context-specific than ELMo and BERT’s (see Figure 2), this suggests that the PCs of highly context-specific representations are less effective on traditional benchmarks.</p>
<p><span id="id7">[<a class="reference internal" href="#id1595" title="Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. Improving neural language generation with spectrum control. In International Conference on Learning Representations. 2019.">WHH+19</a>]</span></p>
</section>
</section>
<section id="contrastive-learning-fundamentals">
<h2><span class="section-number">20.3. </span>Contrastive Learning Fundamentals<a class="headerlink" href="#contrastive-learning-fundamentals" title="Link to this heading">#</a></h2>
<section id="cosentloss">
<span id="chapter-text-embedding-sec-text-embedding-contrastive-learning-cosentloss"></span><h3><span class="section-number">20.3.1. </span>coSENTLoss<a class="headerlink" href="#cosentloss" title="Link to this heading">#</a></h3>
<p>Following the prior study (Su, 2022), we employ the cosine objective function for end-to-end optimization of cosine similarity between representations, as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text {cos }}=\log \left[1+\sum_{s\left(\mathbf{X}_i, \mathbf{X}_j\right)&gt;s\left(\mathbf{X}_m, \mathbf{X}_n\right)} \exp{\left(\cos \left(\mathbf{X}_m, \mathbf{X}_n\right)-\cos \left(\mathbf{X}_i, \mathbf{x}_j\right)\right)/\tau}\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> is a temperature hyperparameter, <span class="math notranslate nohighlight">\(\cos (\cdot)\)</span> is the cosine similarity function, and <span class="math notranslate nohighlight">\(s(u, v)\)</span> is the similarity between <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span>. By optimizing the <span class="math notranslate nohighlight">\(\mathcal{L}_{\text {cos }}\)</span>, we expect the cosine similarity of the high similarity pair to be greater than that of the low similarity pair.</p>
</section>
<section id="embedding-quality-intrinsic-measurement">
<h3><span class="section-number">20.3.2. </span>Embedding quality intrinsic measurement<a class="headerlink" href="#embedding-quality-intrinsic-measurement" title="Link to this heading">#</a></h3>
<p>Embedding quality, often as a result of contrastive learning, can be intrinsically measured using two properties: <strong>Alignment</strong> and <strong>uniformity</strong> <span id="id8">[<a class="reference internal" href="#id1438" title="Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, 9929–9939. PMLR, 2020.">WI20</a>]</span>. Given a distribution of positive pairs <span class="math notranslate nohighlight">\(p_{\text {pos }}\)</span>, alignment calculates expected distance between embeddings of the paired instances (assuming representations are already normalized):</p>
<div class="math notranslate nohighlight">
\[
\ell_{\mathrm{align}} \triangleq \underset{\left(x, x^{+}\right) \sim p_{\mathrm{pos}}}{\mathbb{E}}\left\|f(x)-f\left(x^{+}\right)\right\|^2
\]</div>
<p>On the other hand, uniformity measures how well the embeddings are uniformly distributed:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\ell_{\text {uniform }} \triangleq \log \underset{\substack{\text { i.i.d. } \\ x, y \sim p_{\text {data }}}}{\mathbb{E}} \quad e^{-2\|f(x)-f(y)\|^2}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{\text {data }}\)</span> denotes the data distribution. These two metrics are well aligned with the objective of contrastive learning: positive instances should stay close and embeddings for random instances should scatter on the hypersphere. In the following sections, we will also use the two metrics to justify the inner workings of our approaches.</p>
</section>
<section id="anisotropy-problem-in-embeddings">
<h3><span class="section-number">20.3.3. </span>Anisotropy problem in embeddings<a class="headerlink" href="#anisotropy-problem-in-embeddings" title="Link to this heading">#</a></h3>
<p><span id="id9">[<a class="reference internal" href="#id1594" title="Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.">Eth19</a>]</span></p>
<p>Recent work identifies an anisotropy problem in
language representations (Ethayarajh, 2019; Li
et al., 2020), i.e., the learned embeddings occupy a
narrow cone in the vector space, which severely
limits their expressiveness.</p>
<p>Wang et al. (2020) show that singular values
of the word embedding matrix in a language model
decay drastically: except for a few dominating singular values, all others are close to zero.</p>
<p>The anisotropy problem is naturally connected to
uniformity (Wang and Isola, 2020), both highlighting that embeddings should be evenly distributed
in the space. Intuitively, optimizing the contrastive
learning objective can improve uniformity (or ease
the anisotropy problem), as the objective pushes
negative instances apart.</p>
</section>
</section>
<section id="transformer-based-embeddings">
<h2><span class="section-number">20.4. </span>Transformer based embeddings<a class="headerlink" href="#transformer-based-embeddings" title="Link to this heading">#</a></h2>
<section id="sentence-bert">
<h3><span class="section-number">20.4.1. </span>Sentence-BERT<a class="headerlink" href="#sentence-bert" title="Link to this heading">#</a></h3>
<p>While a pretrained BERT has provided contextulized embeddings for each token, it is found that high-quality, semantically meaning sentence embeddings can not be directly derived from token embedding.</p>
<p>Sentence-BERT <span id="id10">[<a class="reference internal" href="#id1346" title="Nils Reimers and Iryna Gurevych. Sentence-bert: sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.">RG19</a>]</span> enables sentence embedding extraction from the BERT model after fine-tuning the BERT model via Siamese type of learning<a class="reference internal" href="#chapter-text-embedding-fig-bert-sentencebert"><span class="std std-numref">Fig. 20.2</span></a>. Specifically, three strategies can be used to derived sentence embedding</p>
<ul class="simple">
<li><p>the output of the CLS-token</p></li>
<li><p>the mean of all output vectors (<strong>default</strong>)</p></li>
<li><p>max-over-time of the output vectors</p></li>
</ul>
<figure class="align-default" id="chapter-text-embedding-fig-bert-sentencebert">
<a class="reference internal image-reference" href="../../_images/sentenceBERT.png"><img alt="../../_images/sentenceBERT.png" src="../../_images/sentenceBERT.png" style="width: 621.9px; height: 374.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.2 </span><span class="caption-text">(<strong>left</strong>) Sentence-BERT architecture with classification objective function for training. The two BERT networks are pre-trained have tied weights (Siamese network structure). (<strong>right</strong>) Sentence-BERT architecture at inference. The similarity between two input sentences can be computed as the similarity score between two sentence embeddings.</span><a class="headerlink" href="#chapter-text-embedding-fig-bert-sentencebert" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>Spearman</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Avg. GloVe embeddings</p></td>
<td class="text-center"><p>58.02</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Avg. BERT embeddings</p></td>
<td class="text-center"><p>46.35</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>SBERT-NLI-base</p></td>
<td class="text-center"><p>77.03</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SBERT-NLI-base + STSb FT</p></td>
<td class="text-center"><p>85.35</p></td>
</tr>
</tbody>
</table>
</div>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 20.2 </span> (Training time and inference time inconsistency)</p>
<section class="remark-content" id="proof-content">
<p>In the inference time, the pooled embeddings from the last layer are directly used in cosine similarity scoring. In the training time, the pooled embeddings plus its difference vector are passed through an MLP network first and then make prediction. The pooled embeddings are not optimized directly towards cosine similarity scoring. This issue is discussed in coSENTLoss</p>
</section>
</div><!-- ### Universal Sentence Encoder

\cite{cer2018universal}

### Contrastive learning

#### Contrastive learning via data augmentation

\cite{shen2020simple}

```{figure} images/../deepLearning/ApplicationsNLP/sentence_representation/data_augmented_contrastive_learning/data_cut_off_demo
:name: fig:datacutoffdemo
Figure 1: Schematic illustration of the proposed cutoff augmentation strategies, including token cutoff, feature
		cutoff and span cutoff, respectively. Blue area indicates that the corresponding elements within the sentence’s
		input embedding matrix are removed and converted to 0. Notably, this is distinct from Dropout, which randomly
		transforms elements to 0 (without considering any underlying structure of the matrix).
``` -->
</section>
<section id="simcse">
<h3><span class="section-number">20.4.2. </span>SimCSE<a class="headerlink" href="#simcse" title="Link to this heading">#</a></h3>
<p>SimCSE <span id="id11">[<a class="reference internal" href="#id1434" title="Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.">GYC21</a>]</span> introduces a simple Dropout strategy to identify positive examples and conduct unsupervised learning.
As shown in <a class="reference internal" href="#chapter-text-embedding-fig-bert-sim-sce-demo"><span class="std std-numref">Fig. 20.3</span></a>,</p>
<ul class="simple">
<li><p>Positive example pairs can be constructed by passing one sentence passing through the encoder network with different dropout masking.</p></li>
<li><p>Negative example pairs are coming from in-batch negatives.</p></li>
</ul>
<figure class="align-default" id="chapter-text-embedding-fig-bert-sim-sce-demo">
<a class="reference internal image-reference" href="../../_images/sim_sce_demo.png"><img alt="../../_images/sim_sce_demo.png" src="../../_images/sim_sce_demo.png" style="width: 741.4000000000001px; height: 243.10000000000002px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.3 </span><span class="caption-text">(Left) Unsupervised SimCSE predicts the input sentence itself from in-batch negatives, with different hidden dropout masks applied. (Right) Supervised SimCSE leverages the NLI datasets and takes the entailment (premisehypothesis) pairs as positives, and contradiction pairs as well as other in-batch instances as negatives.</span><a class="headerlink" href="#chapter-text-embedding-fig-bert-sim-sce-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors (Hadsell et al., 2006). It assumes a set of paired examples <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left(x_{i}, x_{i}^{+}\right)\right\}_{i=1}^{m}\)</span>, where <span class="math notranslate nohighlight">\(x_{i}\)</span> and <span class="math notranslate nohighlight">\(x_{i}^{+}\)</span>are semantically related.  with in-batch negatives</p>
<p>let <span class="math notranslate nohighlight">\(\mathbf{h}_{i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}_{i}^{+}\)</span>denote the representations of <span class="math notranslate nohighlight">\(x_{i}\)</span> and <span class="math notranslate nohighlight">\(x_{i}^{+}\)</span>, the training objective for <span class="math notranslate nohighlight">\(\left(x_{i}, x_{i}^{+}\right)\)</span>with a mini-batch of <span class="math notranslate nohighlight">\(N\)</span> pairs is:</p>
<div class="math notranslate nohighlight">
\[
\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> is a temperature hyperparameter and <span class="math notranslate nohighlight">\(\operatorname{sim}\left(\mathbf{h}_{1}, \mathbf{h}_{2}\right)\)</span> is the cosine similarity <span class="math notranslate nohighlight">\(\frac{\mathbf{h}_{1}^{\top} \mathbf{h}_{2}}{\left\|\mathbf{h}_{1}\right\| \cdot\left\|\mathbf{h}_{2}\right\|}.\)</span></p>
<p><strong>supervised contrastive learning with hard negatives</strong></p>
<p>Finally, we further take the advantage of the NLI datasets by using its contradiction pairs as hard negatives. In NLI datasets, given one premise, annotators are required to manually write one sentence that is absolutely true (entailment), one that might be true (neutral), and one that is definitely false (contradiction). Therefore, for each premise and its entailment hypothesis, there is an accompanying contradiction hypothesis.</p>
<p>Formally, we extend <span class="math notranslate nohighlight">\(\left(x_{i}, x_{i}^{+}\right)\)</span>to <span class="math notranslate nohighlight">\(\left(x_{i}, x_{i}^{+}, x_{i}^{-}\right)\)</span>, where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the premise, <span class="math notranslate nohighlight">\(x_{i}^{+}\)</span>and <span class="math notranslate nohighlight">\(x_{i}^{-}\)</span>are entailment and contradiction hypotheses. The training objec-
tive <span class="math notranslate nohighlight">\(\ell_{i}\)</span> is then defined by <span class="math notranslate nohighlight">\(N\)</span> is mini-batch size</p>
<div class="math notranslate nohighlight">
\[-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{-}\right) / \tau}\right)}\]</div>
<div class="pst-scrollable-table-container"><table class="table" id="id1609">
<caption><span class="caption-number">Table 20.2 </span><span class="caption-text">Effects of different dropout probabilities <span class="math notranslate nohighlight">\(p\)</span> (from 0.0 to 0.5) on the STS-B development set (Spearman’s correlation). Fixed 0.1: default 0.1 dropout rate but apply the same dropout mask on both <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_i^{+}\)</span>.</span><a class="headerlink" href="#id1609" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p><span class="math notranslate nohighlight">\(p\)</span></p></th>
<th class="head text-center"><p>0.0</p></th>
<th class="head text-center"><p>0.01</p></th>
<th class="head text-center"><p>0.05</p></th>
<th class="head text-center"><p>0.1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>STS-B</p></td>
<td class="text-center"><p>71.1</p></td>
<td class="text-center"><p>72.6</p></td>
<td class="text-center"><p>81.1</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 2 . 5}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td class="text-center"><p>0.15</p></td>
<td class="text-center"><p>0.2</p></td>
<td class="text-center"><p>0.5</p></td>
<td class="text-center"><p>Fixed 0.1</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>STS-B</p></td>
<td class="text-center"><p>81.4</p></td>
<td class="text-center"><p>80.5</p></td>
<td class="text-center"><p>71.0</p></td>
<td class="text-center"><p>43.6</p></td>
</tr>
</tbody>
</table>
</div>
<section id="cert">
<h4><span class="section-number">20.4.2.1. </span>CERT<a class="headerlink" href="#cert" title="Link to this heading">#</a></h4>
<p>\cite{fang2020cert}</p>
<p>CERT: Contrastive self-supervised Encoder Representations
from Transformers, which pretrains language representation models using contrastive selfsupervised learning at the sentence level. CERT creates augmentations of original sentences
using back-translation. Then it finetunes a pretrained language encoder (e.g., BERT) by
predicting whether two augmented sentences originate from the same sentence.</p>
<p>CERT takes a pretrained language representation model (e.g., BERT) and finetunes it using
contrastive self-supervised learning on the input data of the target task.</p>
<figure class="align-default" id="fig-certworkflow">
<img alt="docs/chapter_text_embedding/deepLearning/ApplicationsNLP/sentence_representation/CERT/CERT_workflow" src="docs/chapter_text_embedding/deepLearning/ApplicationsNLP/sentence_representation/CERT/CERT_workflow" />
<figcaption>
<p><span class="caption-number">Fig. 20.4 </span><span class="caption-text">The workflow of CERT. Given the large-scale input texts (without labels) from
source tasks, a BERT model is first pretrained on these texts. Then we continue
to train this pretrained BERT model using CSSL on the input texts (without
labels) from the target task. We refer to this model as pretrained CERT model.
Then we finetune the CERT model using the input texts and their associated
labels in the target task and get the final model that performs the target task.</span><a class="headerlink" href="#fig-certworkflow" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="sentence-t5">
<h3><span class="section-number">20.4.3. </span>Sentence T5<a class="headerlink" href="#sentence-t5" title="Link to this heading">#</a></h3>
<p><span id="id12">[<a class="reference internal" href="#id1604" title="Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.">NAC+21</a>]</span></p>
</section>
<section id="benchmark-dataset">
<h3><span class="section-number">20.4.4. </span>Benchmark dataset<a class="headerlink" href="#benchmark-dataset" title="Link to this heading">#</a></h3>
<section id="multinli-data">
<h4><span class="section-number">20.4.4.1. </span>MultiNLI data<a class="headerlink" href="#multinli-data" title="Link to this heading">#</a></h4>
<p><span id="id13">[<a class="reference internal" href="#id1361" title="Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.">WNB17</a>]</span></p>
<p>The Multi-Genre Natural Language Inference (MultiNLI) corpus <sup>[^3]</sup> is a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. It has over 433,000 examples and is one of the largest datasets available for natural language inference (a.k.a recognizing textual entailment). The dataset is also designed so that existing machine learning models trained on the Stanford NLI corpus can also be evaluated using MultiNLI.</p>
</section>
<section id="standard-semantic-textual-similarity-tasks">
<h4><span class="section-number">20.4.4.2. </span>Standard semantic textual similarity tasks<a class="headerlink" href="#standard-semantic-textual-similarity-tasks" title="Link to this heading">#</a></h4>
<p><span id="id14">[<a class="reference internal" href="#id1409" title="Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, and others. Semeval-2015 task 2: semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), 252–263. 2015.">ABC+15</a>, <a class="reference internal" href="#id1410" title="Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: multilingual semantic textual similarity. In SemEval&#64; COLING, 81–91. 2014.">ABC+14</a>, <a class="reference internal" href="#id1411" title="Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. Semeval-2016 task 1: semantic textual similarity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (Association for Computational Linguistics), 2016.">ABC+16</a>, <a class="reference internal" href="#id1412" title="Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: a pilot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), 385–393. 2012.">ACDGA12</a>, <a class="reference internal" href="#id1413" title="Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared task: semantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity, 32–43. 2013.">ACD+13</a>, <a class="reference internal" href="#id1343" title="Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.">CDA+17</a>]</span></p>
</section>
</section>
</section>
<section id="knowledge-transfer-and-distillation">
<h2><span class="section-number">20.5. </span>Knowledge Transfer and Distillation<a class="headerlink" href="#knowledge-transfer-and-distillation" title="Link to this heading">#</a></h2>
<section id="from-mono-language-to-multi-lingual">
<h3><span class="section-number">20.5.1. </span>From mono language to multi lingual<a class="headerlink" href="#from-mono-language-to-multi-lingual" title="Link to this heading">#</a></h3>
<p><span id="id15">[<a class="reference internal" href="#id1599" title="Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. arXiv preprint arXiv:2004.09813, 2020.">RG20</a>]</span></p>
</section>
</section>
<section id="matryoshka-representation-learning">
<h2><span class="section-number">20.6. </span>Matryoshka Representation Learning<a class="headerlink" href="#matryoshka-representation-learning" title="Link to this heading">#</a></h2>
<p>new state-of-the-art (text) embedding models started producing embeddings with increasingly higher output dimensions, i.e., every input text is represented using more values. Although this improves performance, it comes at the cost of efficiency of downstream tasks such as search or classification.</p>
<p><span id="id16">[<a class="reference internal" href="#id1593" title="Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, and others. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233–30249, 2022.">KBR+22</a>]</span></p>
<p>These Matryoshka embedding models are trained such that these small truncated embeddings would still be useful. In short, Matryoshka embedding models can produce useful embeddings of various dimensions.</p>
<p>Matryoshka embedding models aim to store more important information in earlier dimensions, and less important information in later dimensions. This characteristic of Matryoshka embedding models allows us to truncate the original (large) embedding produced by the model, while still retaining enough of the information to perform well on downstream tasks.</p>
<p>For Matryoshka Embedding models, the training aims to optimize the quality of your embeddings at various different dimensionalities. For example, output dimensionalities are 768, 512, 256, 128, and 64. The loss values for each dimensionality are added together, resulting in a final loss value. The optimizer will then try and adjust the model weights to lower this loss value.</p>
<p>In practice, this incentivizes the model to frontload the most important information at the start of an embedding, such that it will be retained if the embedding is truncated.</p>
<section id="loss-function">
<h3><span class="section-number">20.6.1. </span>Loss Function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h3>
<p>For MRL, we choose <span class="math notranslate nohighlight">\(\mathcal{M}=\{8,16, \ldots, 1024,2048\}\)</span> as the nesting dimensions.
Suppose we are given a labelled dataset <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left(x_1, y_1\right), \ldots,\left(x_N, y_N\right)\right\}\)</span> where <span class="math notranslate nohighlight">\(x_i \in \mathcal{X}\)</span> is an input point and <span class="math notranslate nohighlight">\(y_i \in[L]\)</span> is the label of <span class="math notranslate nohighlight">\(x_i\)</span> for all <span class="math notranslate nohighlight">\(i \in[N]\)</span>. MRL optimizes the multi-class classification loss for each of the nested dimension <span class="math notranslate nohighlight">\(m \in \mathcal{M}\)</span> using standard empirical risk minimization using a separate linear classifier, parameterized by <span class="math notranslate nohighlight">\(\mathbf{W}^{(m)} \in \mathbb{R}^{L \times m}\)</span>. All the losses are aggregated after scaling with their relative importance <span class="math notranslate nohighlight">\(\left(c_m \geq 0\right)_{m \in \mathcal{M}}\)</span> respectively. That is, we solve</p>
<div class="math notranslate nohighlight">
\[
\min_{\left\{\mathbf{W}^{(m)}\right\}_{m \in \mathcal{M}}} \frac{1}{N} \sum_{i \in[N]} \sum_{m \in \mathcal{M}} c_m \cdot \mathcal{L}\left(\mathbf{W}^{(m)} \cdot h_i^{1: m} ; y_i\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}: \mathbb{R}^L \times[L] \rightarrow \mathbb{R}_{+}\)</span>is the multi-class softmax cross-entropy loss function. This is a standard optimization problem that can be solved using sub-gradient descent methods. We set all the importance scales, <span class="math notranslate nohighlight">\(c_m=1\)</span> for all <span class="math notranslate nohighlight">\(m \in \mathcal{M}\)</span>; see Section 5 for ablations. Lastly, despite only optimizing for <span class="math notranslate nohighlight">\(O(\log (d))\)</span> nested dimensions, MRL results in accurate representations, that interpolate, for dimensions that fall between the chosen granularity of the representations (Section 4.2).</p>
</section>
</section>
<section id="llm-embedding-model">
<h2><span class="section-number">20.7. </span>LLM Embedding Model<a class="headerlink" href="#llm-embedding-model" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/2401.00368">Improving text embeddings with large language models</a>
<a class="reference external" href="https://arxiv.org/pdf/2405.17428">NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.</a></p>
<section id="nv-embed">
<h3><span class="section-number">20.7.1. </span>NV-Embed<a class="headerlink" href="#nv-embed" title="Link to this heading">#</a></h3>
<p>NV-Embed from Nvidia <span id="id17">[<a class="reference internal" href="#id1585" title="Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024.">LRX+24</a>]</span> proposed several improvement techniques on LLM-based embedding model, which include:</p>
<ul class="simple">
<li><p>Model architecture improvement, which introduces a <strong>latent attention layer</strong> to obtain better pooled embeddings.</p></li>
<li><p>Traing process improvement, which introduces a <strong>two-stage contrastive instruction-tuning method</strong>.</p></li>
</ul>
<p>There are two popular methods to obtain the embedding for a sequence of tokens:</p>
<ul class="simple">
<li><p>Mean pooling of the all hidden vectors of the last layer, which is commonly used in bidirectional embedding models.</p></li>
<li><p>Last <EOS> token embedding, which is more popular for decoder-only LLM based embedding models.</p></li>
</ul>
<p>However, both methods have certain limitations. Mean pooling simply takes the average of token embeddings and may dilute the important information from key phrases, meanwhile the semantics of the last <EOS> token embedding may be dominated by last few tokens.</p>
<p>The latent attention layer aims to improve the <strong>mean pooling method</strong>. Denote the last layer hidden from decoder as the query <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{l \times d}\)</span>, where <span class="math notranslate nohighlight">\(l\)</span> is the length of sequence, and <span class="math notranslate nohighlight">\(d\)</span> is the hidden dimension. They are sent to attend the latent array <span class="math notranslate nohighlight">\(K=V \in \mathbb{R}^{r \times d}\)</span>, which are <strong>trainable matrices</strong>, used to obtain better representation, where <span class="math notranslate nohighlight">\(r\)</span> is the number of latents in the dictionary. The output of this cross-attention is denoted by <span class="math notranslate nohighlight">\(O \in \mathbb{R}^{l \times d}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
O=\operatorname{softmax}\left(Q K^T\right) V.
\]</div>
<p>Intuitively, each token’s represention in <span class="math notranslate nohighlight">\(O\)</span> (which is a <span class="math notranslate nohighlight">\(d\)</span> vector) is a linear combination of the <span class="math notranslate nohighlight">\(r\)</span> row vectors in <span class="math notranslate nohighlight">\(V\)</span>(or <span class="math notranslate nohighlight">\(K\)</span>).</p>
<p>This has the spirit of <strong>sparse dictionary learning</strong><span id="id18">[<a class="reference internal" href="#id1105" title="Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning, 689–696. ACM, 2009.">MBPS09</a>]</span>, which aims to learn a <strong>sparse set of atom vectors</strong>, such that each representation can be transformed to a linear combination of atom vectors.</p>
<p>An additional 2-layer MLP was added to further transfrom the <span class="math notranslate nohighlight">\(O\)</span> vectors.  Finally, a mean pooling after MLP layers to obtain the embedding of whole sequences.</p>
<p>In the paper, authors used latent attention layer with <span class="math notranslate nohighlight">\(r\)</span> of 512 and the number of heads as 8 for multi-head attention.</p>
<figure class="align-default" id="chapter-application-ir-llm-fig-embedding-nv-embedding-latent-attention-layer">
<a class="reference internal image-reference" href="../../_images/latent_attention_layer.png"><img alt="../../_images/latent_attention_layer.png" src="../../_images/latent_attention_layer.png" style="width: 644.0px; height: 252.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.5 </span><span class="caption-text">The illustration of proposed architecture design comprising of decoder-only LLM followed
by latent attention layer. Latent attention layer functions as a form of cross-attention where the decoder-only LLM output serves as queries (Q) and trainable latent array passes through the keyvalue inputs, followed by MLP. Image from <span id="id19">[<a class="reference internal" href="#id1585" title="Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024.">LRX+24</a>]</span>.</span><a class="headerlink" href="#chapter-application-ir-llm-fig-embedding-nv-embedding-latent-attention-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The two-stage instruction tuning method include</p>
<ul class="simple">
<li><p>First-stage contrastive training with instructions on a variety of retrieval datasets, utilizing in-batch negatives and curated hard-negative examples.</p></li>
<li><p>Second stage contrastive instruction-tuning on a combination of retrieval and non-retrieval datasets (e.g., classification ) without applying the trick of in-batch negatives.</p></li>
</ul>
<p>The design rationale behind the two-stage finetunings are:</p>
<ul class="simple">
<li><p>It is found that retrieval task presents greater difficulty compared to the non-retrieval tasks there is one stage training fully dedicated to the retrieval task.</p></li>
<li><p>In second stage, as the retrieval and non-retrieval tasks are blended, it is necessary to remove in-batch negatives trick. Since the negative may come from the the class and are not true negatives.</p></li>
</ul>
</section>
</section>
<section id="llm-embedding-distillation">
<h2><span class="section-number">20.8. </span>LLM Embedding Distillation<a class="headerlink" href="#llm-embedding-distillation" title="Link to this heading">#</a></h2>
<p><span id="id20">[<a class="reference internal" href="#id1601" title="Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, and others. Gecko: versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024.">LDR+24</a>]</span></p>
</section>
<section id="software">
<h2><span class="section-number">20.9. </span>Software<a class="headerlink" href="#software" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/deepset-ai/haystack">haystack</a></p>
</section>
<section id="bibliography">
<h2><span class="section-number">20.10. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id21">
<div role="list" class="citation-list">
<div class="citation" id="id1409" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">ABC+15</a><span class="fn-bracket">]</span></span>
<p>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, and others. Semeval-2015 task 2: semantic textual similarity, english, spanish and pilot on interpretability. In <em>Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)</em>, 252–263. 2015.</p>
</div>
<div class="citation" id="id1410" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">ABC+14</a><span class="fn-bracket">]</span></span>
<p>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: multilingual semantic textual similarity. In <em>SemEval&#64; COLING</em>, 81–91. 2014.</p>
</div>
<div class="citation" id="id1411" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">ABC+16</a><span class="fn-bracket">]</span></span>
<p>Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. Semeval-2016 task 1: semantic textual similarity, monolingual and cross-lingual evaluation. In <em>SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511.</em> ACL (Association for Computational Linguistics), 2016.</p>
</div>
<div class="citation" id="id1412" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">ACDGA12</a><span class="fn-bracket">]</span></span>
<p>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: a pilot on semantic textual similarity. In <em>* SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)</em>, 385–393. 2012.</p>
</div>
<div class="citation" id="id1413" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">ACD+13</a><span class="fn-bracket">]</span></span>
<p>Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared task: semantic textual similarity. In <em>Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity</em>, 32–43. 2013.</p>
</div>
<div class="citation" id="id1355" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">BAPM15</a><span class="fn-bracket">]</span></span>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. <em>arXiv preprint arXiv:1508.05326</em>, 2015.</p>
</div>
<div class="citation" id="id1343" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CDA+17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: semantic textual similarity-multilingual and cross-lingual focused evaluation. <em>arXiv preprint arXiv:1708.00055</em>, 2017.</p>
</div>
<div class="citation" id="id1342" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">DB05</a><span class="fn-bracket">]</span></span>
<p>William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In <em>Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</em>. 2005.</p>
</div>
<div class="citation" id="id1594" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Eth19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id6">2</a>,<a role="doc-backlink" href="#id9">3</a>)</span>
<p>Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. <em>arXiv preprint arXiv:1909.00512</em>, 2019.</p>
</div>
<div class="citation" id="id1596" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">GHT+19</a><span class="fn-bracket">]</span></span>
<p>Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration problem in training natural language generation models. <em>arXiv preprint arXiv:1907.12009</em>, 2019.</p>
</div>
<div class="citation" id="id1434" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">GYC21</a><span class="fn-bracket">]</span></span>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: simple contrastive learning of sentence embeddings. <em>arXiv preprint arXiv:2104.08821</em>, 2021.</p>
</div>
<div class="citation" id="id1593" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">KBR+22</a><span class="fn-bracket">]</span></span>
<p>Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, and others. Matryoshka representation learning. <em>Advances in Neural Information Processing Systems</em>, 35:30233–30249, 2022.</p>
</div>
<div class="citation" id="id1585" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LRX+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id17">1</a>,<a role="doc-backlink" href="#id19">2</a>)</span>
<p>Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: improved techniques for training llms as generalist embedding models. <em>arXiv preprint arXiv:2405.17428</em>, 2024.</p>
</div>
<div class="citation" id="id1601" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">LDR+24</a><span class="fn-bracket">]</span></span>
<p>Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, and others. Gecko: versatile text embeddings distilled from large language models. <em>arXiv preprint arXiv:2403.20327</em>, 2024.</p>
</div>
<div class="citation" id="id1105" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">MBPS09</a><span class="fn-bracket">]</span></span>
<p>Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In <em>Proceedings of the 26th annual international conference on machine learning</em>, 689–696. ACM, 2009.</p>
</div>
<div class="citation" id="id1604" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">NAC+21</a><span class="fn-bracket">]</span></span>
<p>Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: scalable sentence encoders from pre-trained text-to-text models. <em>arXiv preprint arXiv:2108.08877</em>, 2021.</p>
</div>
<div class="citation" id="id1346" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">RG19</a><span class="fn-bracket">]</span></span>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: sentence embeddings using siamese bert-networks. <em>arXiv preprint arXiv:1908.10084</em>, 2019.</p>
</div>
<div class="citation" id="id1599" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">RG20</a><span class="fn-bracket">]</span></span>
<p>Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. <em>arXiv preprint arXiv:2004.09813</em>, 2020.</p>
</div>
<div class="citation" id="id1595" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">WHH+19</a><span class="fn-bracket">]</span></span>
<p>Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. Improving neural language generation with spectrum control. In <em>International Conference on Learning Representations</em>. 2019.</p>
</div>
<div class="citation" id="id1438" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">WI20</a><span class="fn-bracket">]</span></span>
<p>Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In <em>International Conference on Machine Learning</em>, 9929–9939. PMLR, 2020.</p>
</div>
<div class="citation" id="id1361" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">WNB17</a><span class="fn-bracket">]</span></span>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. <em>arXiv preprint arXiv:1704.05426</em>, 2017.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_text_embedding"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_prompt/advanced_prompt.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Advanced Prompting Techniques</p>
      </div>
    </a>
    <a class="right-next"
       href="text_embedding_LLM.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>LLM Text Embedding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">20.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextual-text-embedding-fundamentals">20.2. Contextual Text Embedding Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anisotropy">20.2.1. Anisotropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-specificity">20.2.2. Context-specificity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-vs-contextual-word-embeddings">20.2.3. Static vs Contextual Word embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-learning-fundamentals">20.3. Contrastive Learning Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosentloss">20.3.1. coSENTLoss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-quality-intrinsic-measurement">20.3.2. Embedding quality intrinsic measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anisotropy-problem-in-embeddings">20.3.3. Anisotropy problem in embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-based-embeddings">20.4. Transformer based embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-bert">20.4.1. Sentence-BERT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simcse">20.4.2. SimCSE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cert">20.4.2.1. CERT</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-t5">20.4.3. Sentence T5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-dataset">20.4.4. Benchmark dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multinli-data">20.4.4.1. MultiNLI data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-semantic-textual-similarity-tasks">20.4.4.2. Standard semantic textual similarity tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-transfer-and-distillation">20.5. Knowledge Transfer and Distillation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-mono-language-to-multi-lingual">20.5.1. From mono language to multi lingual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matryoshka-representation-learning">20.6. Matryoshka Representation Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">20.6.1. Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-embedding-model">20.7. LLM Embedding Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nv-embed">20.7.1. NV-Embed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-embedding-distillation">20.8. LLM Embedding Distillation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#software">20.9. Software</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.10. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>