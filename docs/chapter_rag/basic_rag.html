
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>20. RAG &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_rag/basic_rag';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="21. Information Retrieval and Text Ranking" href="../chapter_application_IR/information_retrieval_fundamentals.html" />
    <link rel="prev" title="19. Advanced Prompting Techniques" href="../chapter_prompt/advanced_prompt.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">14. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">20. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">21. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">22. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>RAG</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">20.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-frameworks">20.2. RAG Frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-rag">20.2.1. Basic RAG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimizations">20.2.2. RAG Optimizations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-challenges-in-practice">20.2.3. RAG Challenges in Practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-documents">20.3. RAG Optimization: Documents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-data-sources">20.3.1. Indexing Data Sources</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-source-augmentation">20.3.2. Data Source Augmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-splitting-and-granularity">20.3.3. Document Splitting and Granularity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-query-understanding-and-rewriting">20.4. RAG Optimization: Query Understanding and Rewriting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">20.4.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-vocalbulary-mismatch">20.4.2. Approach to Vocalbulary Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-complex-multi-concept-queries">20.4.3. Approach to Complex Multi-Concept Queries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-multi-turn-conversations">20.4.4. Approach to Multi-Turn Conversations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-retriever-and-reranker">20.5. RAG Optimization: Retriever and ReRanker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-model-enhancement">20.5.1. Retrieval Model Enhancement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-result-quality-control">20.5.2. Retrieval Result Quality Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-llm-understanding-generation">20.6. RAG Optimization: LLM Understanding &amp; Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-objective">20.6.1. Motivation and Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-finetuning">20.6.2. Model Finetuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-rag">20.7. Advanced RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-rag">20.7.1. Agentic RAG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag">20.7.2. GraphRAG</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-evaluation">20.8. RAG Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-rag-discussion">20.9. Further RAG Discussion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-vs-prompting-and-fine-tuning">20.9.1. RAG vs Prompting and Fine Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-vs-long-context-llm">20.9.2. RAG vs Long Context LLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.10. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="rag">
<h1><span class="section-number">20. </span>RAG<a class="headerlink" href="#rag" title="Link to this heading">#</a></h1>
<section id="motivation">
<h2><span class="section-number">20.1. </span>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>LLMs have revolutionized natural language processing, but they still face several significant challenges, particularly in <strong>knowledge intensive tasks</strong>:</p>
<ul class="simple">
<li><p><strong>Hallucination</strong>: LLMs can generate plausible-sounding but factually incorrect information when they are prompted with rare or ambiguous queries, e.g., what is kuula? (a fishing god).</p></li>
<li><p><strong>Outdated knowledge</strong>: The knowledge of LLMs is limited to their pre-training data, which can quickly become obsolete.</p></li>
<li><p><strong>Untraceable reasoning</strong>: The decision-making process of LLMs is often unclear, making it difficult to verify or understand their outputs.</p></li>
<li><p><strong>Expensive cost to inject knowledge</strong>: Although one can inject domain knowledge or updated knowledge via continuous pretraining or finetining, the cost of data collections and training is very high.</p></li>
</ul>
<p>To address these challenges, researchers and developers have been exploring promising solutions. One such solution is to integrate of LLMs’ inherent knowledge with external knowledge bases during model generation process. This approach is known as <strong>Retrieval-Augmented Generation (RAG)</strong> [<a class="reference internal" href="#chapter-rag-fig-rag-demo"><span class="std std-numref">Fig. 20.1</span></a>].</p>
<figure class="align-default" id="chapter-rag-fig-rag-demo">
<a class="reference internal image-reference" href="../../_images/RAG_demo.png"><img alt="../../_images/RAG_demo.png" src="../../_images/RAG_demo.png" style="width: 712.1999999999999px; height: 414.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.1 </span><span class="caption-text">Illustration of RAG process applied to question answering. It mainly consists of basic steps. 1) Offline Indexing. Documents are split into chunks,
encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top <span class="math notranslate nohighlight">\(k\)</span> relevant chunks as context or knowledge supplement. 3)
Generation. The original question and the retrieved context are fed into LLM to generate the final answer. Image from <span id="id1">[]</span>.</span><a class="headerlink" href="#chapter-rag-fig-rag-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Compared to LLM’s responses that are relied on its own internal knowledge, RAG exhibits the following advantages:</p>
<p><strong>Improved Accuracy and Reliability</strong>: By supplementing the LLM’s knowledge with current, factual information from external sources, RAG can significantly reduce hallucinations and increase the accuracy of generated content.</p>
<p><strong>Superior Performance on Knowledge-Intensive Tasks</strong>: RAG excels in tasks that require specific, detailed information, such as question-answering, fact-checking, and research assistance.</p>
<p><strong>Continuous Knowledge Updates</strong>: Unlike traditional LLMs, which require retraining to incorporate new information, RAG systems can be updated by simply modifying the external knowledge base. This allows for more frequent and efficient knowledge updates.</p>
<p><strong>Domain-Specific Information Integration</strong>: RAG enables the integration of specialized knowledge from particular fields or industries, making it possible to create more focused and accurate outputs for specific domains.</p>
</section>
<section id="rag-frameworks">
<h2><span class="section-number">20.2. </span>RAG Frameworks<a class="headerlink" href="#rag-frameworks" title="Link to this heading">#</a></h2>
<section id="basic-rag">
<h3><span class="section-number">20.2.1. </span>Basic RAG<a class="headerlink" href="#basic-rag" title="Link to this heading">#</a></h3>
<p>RAG is a technique that combines the powerful language understanding generation capabilities of LLMs with the information retrieval ability of a retrieval system/search engine. This hybrid approach aims to leverage the strengths of both systems to produce more accurate, up-to-date, and verifiable outputs.</p>
<p>The <strong>RAG</strong> framework is built upon three fundamental components [<a class="reference internal" href="#chapter-rag-fig-rag-framework-demo"><span class="std std-numref">Fig. 20.2</span></a>]:</p>
<ul class="simple">
<li><p><strong>Indexing building</strong>, which transforms raw documents into a format that enables efficient retrieval and knowledge integration. For example, one can split a document into multiple chunks, and encode each of them into dense embedding vectors (for dense retrieval) or inverted index (for sparse retrieval). Indexing building is usually done offline.</p></li>
<li><p><strong>Retrieval</strong>, which is the process of extracting relevant paragraph/chunks from the index in response to a query. This step involves online query understanding and processing - transform the query into embedding vector (for dense retrieval) or terms (for sparse retrieval) and retrieving documents using query vectors or terms.</p></li>
<li><p><strong>Generation</strong>, which involves using the retrieved information along with the language model’s inherent knowledge to produce a response. This step leverages the power of large language models to understand context, integrate the retrieved information, and generate coherent and relevant text.</p></li>
</ul>
<figure class="align-default" id="chapter-rag-fig-rag-framework-demo">
<a class="reference internal image-reference" href="../../_images/Basic_RAG_framework.png"><img alt="../../_images/Basic_RAG_framework.png" src="../../_images/Basic_RAG_framework.png" style="width: 388.5px; height: 514.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.2 </span><span class="caption-text">Illustration of a basic RAG framework.</span><a class="headerlink" href="#chapter-rag-fig-rag-framework-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 20.1 </span> (A minimal RAG example)</p>
<section class="example-content" id="proof-content">
<p>The Vanilla RAG (Retrieval-Augmented Generation) operates in a simplified manner as follows:</p>
<ul class="simple">
<li><p>The text is divided into chunks.</p></li>
<li><p>These chunks are then encoded into vectors using a Transformer encoder model, and all these vectors are stored in a vector database.</p></li>
<li><p>Finally, a Language Model (LLM) prompt is created, and the model answers user queries based on the context retrieved from the top-k most relevant results found through vector indexing in the vector database.</p></li>
</ul>
<p>During interaction, the same encoder model is used to vectorize user queries. Vector indexing is then performed to identify the top-k most relevant results from the vector database. These indexed text chunks are retrieved and provided as context to the LLM prompt for generating responses to user queries.</p>
<p><em>Example LLM prompt</em>:</p>
<p>Give the answer to the user query delimited by triple backticks <code class="docutils literal notranslate"><span class="pre">{query}</span></code> using the information given in context delimited by triple backticks <code class="docutils literal notranslate"><span class="pre">{context}</span></code>. If there is no relevant information in the provided context, try to answer yourself, but tell user that you did not have any relevant context to base your answer on. Be concise and output the answer of size less than 80 tokens.</p>
</section>
</div></section>
<section id="rag-optimizations">
<h3><span class="section-number">20.2.2. </span>RAG Optimizations<a class="headerlink" href="#rag-optimizations" title="Link to this heading">#</a></h3>
<p>Various optimizations can be applied to the basic RAG framework [<a class="reference internal" href="#chapter-rag-fig-rag-framework-demo"><span class="std std-numref">Fig. 20.2</span></a>] to improve the quality and reliability of the system’s outputs. These optimizations address common challenges in RAG systems, including query-document mismatch, retrieval accuracy, and output reliability.</p>
<p>As shown in <a class="reference internal" href="#chapter-rag-fig-rag-framework-optimization-demo"><span class="std std-numref">Fig. 20.3</span></a>, we can divide optimizations into the following categories:</p>
<p><strong>Document Understanding &amp; Augmentation</strong>:Instead of performing mechanism chunking to split the document, we can apply language understanding models to split documents into semnatically coherent units. Besides, we can enriches documents with additional context, metadata (e.g., stamptime), and alternative representations (queries related to document) before they enter the indexing phase. This enrichment makes documents more discoverable and helps maintain their semantic context even when split into chunks for processing.</p>
<p><strong>Query Understanding and Rewriting</strong>: This enhancement addresses one of the fundamental challenges in information retrieval: the vocabulary mismatch between query language and document language. The system can employ LLM to analyze and reformulate user queries, making them more effective for retrieval. It can help complex, multi-concept queries by decomposing the original query into multiple manageable subqueries.</p>
<p><strong>Hybrid retrieval</strong>: Instead of using only sparse retrieval or dense retrieval, one can combine them together to form a hybrid retrieval system. For example, encoder models used in dense retriever can produce additional features for rankers in the sparse retriever side.</p>
<p><strong>Re-ranking</strong>: After the initial retrieval, the Re-ranking step can significantly improves the quality and precision of retrieved content before it reaches the LLM. Documents are re-ranked using a much powerful model based on their contextual relevance to the user query, not just the vector semantic similarity. In addition, the re-ranking process often penalize similiar documents to promote a diverse set of relevant documents are sent to LLM.</p>
<p><strong>LLM Understanding &amp; Generation</strong>：Even with high-quality inputs to LLM, LLM can still produce poor results. This can be alleviated by improving model size, pretraining data quality and distribution, and fine-tuning strategies.</p>
<p><strong>Output verification</strong>: As an additional quality control, after the LLM’s output, we can add additional step to validates the LLM’s output against the retrieved and re-ranked sources. This final check ensures accuracy and consistency.</p>
<figure class="align-default" id="chapter-rag-fig-rag-framework-optimization-demo">
<a class="reference internal image-reference" href="../../_images/Basic_RAG_framework_optimization.png"><img alt="../../_images/Basic_RAG_framework_optimization.png" src="../../_images/Basic_RAG_framework_optimization.png" style="width: 388.5px; height: 493.49999999999994px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.3 </span><span class="caption-text">Optimization of the basic RAG framework in different components.</span><a class="headerlink" href="#chapter-rag-fig-rag-framework-optimization-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="rag-challenges-in-practice">
<h3><span class="section-number">20.2.3. </span>RAG Challenges in Practice<a class="headerlink" href="#rag-challenges-in-practice" title="Link to this heading">#</a></h3>
<p>The following key factors are essential to a successful application of RAG in real-world. These four keys are sequentially dependent and any issues on one-of-them will cause response of poor quality.</p>
<figure class="align-default" id="chapter-rag-fig-rag-framework-key-success-factors">
<a class="reference internal image-reference" href="../../_images/RAG_key_success_factors.png"><img alt="../../_images/RAG_key_success_factors.png" src="../../_images/RAG_key_success_factors.png" style="width: 767.55px; height: 74.55px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.4 </span><span class="caption-text">Key factors underlying a successful RAG product.</span><a class="headerlink" href="#chapter-rag-fig-rag-framework-key-success-factors" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Following table summarize practical challenges and possible causes when applying RAG into actual product.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Issue Type</p></th>
<th class="head"><p>Document Understanding</p></th>
<th class="head"><p>Query Understanding &amp; Ranking Service</p></th>
<th class="head"><p>LLM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Hallucination</p></td>
<td><p>Chunking, truncation, text extraction errors, incomplete</p></td>
<td><p></p></td>
<td><p>Model generate hallucination</p></td>
</tr>
<tr class="row-odd"><td><p>Refusal to Answer</p></td>
<td><p></p></td>
<td><p>Search results irrelevant &amp; incomplete</p></td>
<td><p>Model not understanding content</p></td>
</tr>
<tr class="row-even"><td><p>Incomplete Response</p></td>
<td><p>Incomplete chunking</p></td>
<td><p>Search results irrelevant &amp; incomplete</p></td>
<td><p>Model summary incomplete</p></td>
</tr>
<tr class="row-odd"><td><p>Slow Response Time</p></td>
<td><p></p></td>
<td><p>Search too slow</p></td>
<td><p>Large model parameters</p></td>
</tr>
</tbody>
</table>
</div>
<!-- ## RAG paradigam overview


% https://arxiv.org/pdf/2312.10997
 -->
</section>
</section>
<section id="rag-optimization-documents">
<h2><span class="section-number">20.3. </span>RAG Optimization: Documents<a class="headerlink" href="#rag-optimization-documents" title="Link to this heading">#</a></h2>
<section id="indexing-data-sources">
<h3><span class="section-number">20.3.1. </span>Indexing Data Sources<a class="headerlink" href="#indexing-data-sources" title="Link to this heading">#</a></h3>
<p>In the indexing stage, there are different data sources, and each has its benefits and challenges.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1534">
<caption><span class="caption-number">Table 20.1 </span><span class="caption-text">Retrieval Data Sources</span><a class="headerlink" href="#id1534" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Examples</p></th>
<th class="head text-left"><p>Benefits</p></th>
<th class="head text-left"><p>Challenges</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Unstructured Data</p></td>
<td class="text-left"><p>Text, Web pages, Wikipedia, domain specific corpus</p></td>
<td class="text-left"><p>Large availability</p></td>
<td class="text-left"><p>Need quality control (e.g., remove bias, noisy content) during indexing time; Difficult to parse</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Semi-structured Data</p></td>
<td class="text-left"><p>PDFs, structured markdowns, or Data that contains a combination of text, table, image information.</p></td>
<td class="text-left"><p>Cleaner content than unstructured text and web pages</p></td>
<td class="text-left"><p>Challenges are chunking while preserving table completeness. Converting table to text requires additional tools.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Structured Data</p></td>
<td class="text-left"><p>Knowledge base, knowledge graph.</p></td>
<td class="text-left"><p>Organized, clean information</p></td>
<td class="text-left"><p>High cost to maintain up-to-date information; Need tools to generate KG search queries; requires additional effort to build, validate, and maintain structured databases.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Performing search in a structured knowledge base usually involving additional preprocessing steps than performing search in unstructred/semi-structred data source
<span id="id2">[]</span>. For example, to search entity in a knowledge base, we need to first extract entities from the query [<code class="xref std std-numref docutils literal notranslate"><span class="pre">chapter_rag_fig_rag_knowledge_base_demo</span></code>].</p>
<figure class="align-default" id="chapter-rag-fig-rag-knowledge-base-demo">
<a class="reference internal image-reference" href="../../_images/RAG_knowledge_base.png"><img alt="../../_images/RAG_knowledge_base.png" src="../../_images/RAG_knowledge_base.png" style="width: 561.0px; height: 285.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.5 </span><span class="caption-text">Comparison between retrieval results from document corpus and knowledge bases. Retrieval from knowledge
base (based on key phrases extracted by LLM) could avoid concept/entity missing issue. Image from <span id="id3">[]</span>.</span><a class="headerlink" href="#chapter-rag-fig-rag-knowledge-base-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="data-source-augmentation">
<h3><span class="section-number">20.3.2. </span>Data Source Augmentation<a class="headerlink" href="#data-source-augmentation" title="Link to this heading">#</a></h3>
<p>For unstructured data sources, it can improve document understanding and feature derivation by augmenting the data source with additional information.</p>
<p>For example, chunks can be enriched with metadata information such as page number, file name, author,category timestamp. Timestamp can be used to improve time-aware retrieval model, ensuring the fresh knowledge are ranked higher and avoiding outdated information.</p>
<p>Augmented data can also be artificially constructed. For example, adding summaries of paragraph, as well as introducing queries can be answered by the paragraphs (known as doc2query <span id="id4">[<a class="reference internal" href="#id1402" title="Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.">NYLC19</a>]</span>).</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../../_images/doc_2_query.png"><img alt="../../_images/doc_2_query.png" src="../../_images/doc_2_query.png" style="width: 508.2px; height: 374.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.6 </span><span class="caption-text">Use doc2query to enhance retrieval performance. Image from <span id="id5">[<a class="reference internal" href="#id1402" title="Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.">NYLC19</a>]</span>.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="document-splitting-and-granularity">
<h3><span class="section-number">20.3.3. </span>Document Splitting and Granularity<a class="headerlink" href="#document-splitting-and-granularity" title="Link to this heading">#</a></h3>
<p>During document indexing stage, we need to split documents into different chunks. Such retrieval granularity ranges from fine to coarse, including Phrase, Sentence, Paragrpahs.</p>
<p>Coarse-grained retrieval units fundamentally improve the <strong>recall at the cost of precision</strong>; that is, it can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks. Intutively, encoding a large chunk of text into a single vector will have information loss.</p>
<p>On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee content completeness and semantic integrity (i.e., not enough context).</p>
<p>From a high level, an ideal splitting should consider the following factors:</p>
<ul class="simple">
<li><p><strong>Semantic coherence</strong>: Chunks should maintain semantic coherence - split boundaries should respect natural semantic units and closely related information should stay in the same chunk.</p></li>
<li><p><strong>Size consistency</strong>: Chunks should be sized appropriately for the embedding model.</p></li>
<li><p><strong>Information density</strong>: Each chunk should contain sufficient information to be independently meaningful.</p></li>
</ul>
<p>There are different approaches to splitting:</p>
<ul class="simple">
<li><p><strong>Mechanical splitting</strong> based on a fixed window size. This has the lowest processing cost, but there is no guaratee on maintaining semantic completeness for each chunk - it can create arbitrary breakpoints in the middle of sentences. One mitigation is to use overlapping sliding windows during splitting.</p></li>
<li><p><strong>Structure-aware splitting</strong> by leveraging document structures (headers, sections). This also has low processing cost and it respect the hierachical organization of the docuemnt. However, the chunk size can vary a lot as different documents can organize differently. Also, this method can only apply to relatively formal text data with such structural annotations.</p></li>
<li><p><strong>Semantic-based splitting.</strong> This method invovles using language understanding model to predict the semantic relationship between sentences and paragraphs. For example, we can use BERT to predict if two sentences are sementically close via the next sentence prediction task.  Sentences and paragraphs that are closely related to each other will be grouped into the same chunk. This method is much costly compared to previously two approaches, but it preserves topic coherence within the chunk.</p></li>
</ul>
</section>
</section>
<section id="rag-optimization-query-understanding-and-rewriting">
<h2><span class="section-number">20.4. </span>RAG Optimization: Query Understanding and Rewriting<a class="headerlink" href="#rag-optimization-query-understanding-and-rewriting" title="Link to this heading">#</a></h2>
<section id="id7">
<h3><span class="section-number">20.4.1. </span>Motivation<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Query understanding and rewriting is a crucial component in optimizing RAG  systems. The effectiveness of RAG heavily depends on the quality of the retrieval step, which in turn relies on how well the system understands and processes user queries. Raw user queries that ** don’t directly match the way information is indexed offline** can lead to poor retrieval results.</p>
<p>Specifically, Key challenges that necessitate query rewriting include:</p>
<ul class="simple">
<li><p><strong>Vocabulary mismatch</strong> between user queries and stored documents, particularly for <strong>domain-specific queries</strong> containing terminology and jargon</p></li>
<li><p><strong>Implicit context</strong> that needs to be made explicit</p></li>
<li><p><strong>Complex queries</strong> that combine multiple concepts</p></li>
<li><p><strong>Contextual understanding</strong> from multi-turn conversations</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table" id="id1535">
<caption><span class="caption-number">Table 20.2 </span><span class="caption-text">Retrieval Data Sources</span><a class="headerlink" href="#id1535" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Query Type</p></th>
<th class="head text-left"><p>Example</p></th>
<th class="head text-left"><p>Rewrite Angle/Result</p></th>
<th class="head text-left"><p>Explanation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Vocabulary mismatch</p></td>
<td class="text-left"><p>Why is my car not starting</p></td>
<td class="text-left"><p>Car ignition failure diagnosis</p></td>
<td class="text-left"><p>Document side is likely to contain words like ignition.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Vocabulary mismatch</p></td>
<td class="text-left"><p>Can my boss fire me for being sick</p></td>
<td class="text-left"><p>Employment termination regulations regarding medical leave</p></td>
<td class="text-left"><p>Document side is likely to contain termination, medical leave.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Implicit context</p></td>
<td class="text-left"><p>Current state tax rates</p></td>
<td class="text-left"><p>State tax rate for CA residents in 2024</p></td>
<td class="text-left"><p>Incoporate time and location information into the query to improve precision.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Complex queries</p></td>
<td class="text-left"><p>Can I take ibuprofen with my blood pressure medication while pregnant?</p></td>
<td class="text-left"><p>Decompose to three different subqueries on ibuprofen, blood pressure medication, and pregancy.</p></td>
<td class="text-left"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Multi-turn conversational query</p></td>
<td class="text-left"><p>Turn 1: “Tell me about Tesla Model 3”; Turn 2: “What about its safety features?”</p></td>
<td class="text-left"><p>Safety features of Tesla Model 3</p></td>
<td class="text-left"><p>Incoporate previous context into the rewritten query</p></td>
</tr>
</tbody>
</table>
</div>
<!-- 

Motivation, why we need query understanding and rewrite


Understand what scenario we need to rewrite
* Highly specialized domains like law, medical
* Complex, multi-concept queries: convert to multiple subquestions, convert to multiple subqueries
* Multi-turn conversions: rewrite query based on historical conversation and context

How to rewrite?
* Use classical model to perform synoym expansion, acroynyn expansion, etc. 
* Use generative model to perform query write
* Use RAG + LLM to perform query understanding and rewrite
 -->
<!-- 

Highly Specialized Domains

In domains like law and medicine, effective query rewriting is essential because:
- Technical terminology may have multiple variations or synonyms
- Professional jargon needs to be mapped to standardized terms
- Concepts may be expressed differently in formal vs. informal language
- Domain-specific acronyms need expansion

````{prf:example}
- Original query: "What's the standard treatment for BP?"
- Rewritten: "What is the standard treatment for high blood pressure OR hypertension"
```` -->
</section>
<section id="approach-to-vocalbulary-mismatch">
<h3><span class="section-number">20.4.2. </span>Approach to Vocalbulary Mismatch<a class="headerlink" href="#approach-to-vocalbulary-mismatch" title="Link to this heading">#</a></h3>
<p>Vocalbulary mismatch often occur in querstion-answering in high-specialized domains, like law, science, engineering, etc.
The reason is that concepts are often expressed differently in document language (more formal) vs. query language (less formal). Query understanding and rewriting is essential to bridge such gap by rewriting user’s query concept to professional jargon.</p>
<p>Traditional techniques focus on lexical and syntactic rule-based transformations, including</p>
<ul class="simple">
<li><p>Spelling correction and normalization</p></li>
<li><p>Stop word removal and stemming</p></li>
<li><p>Query expansion using WordNet or domain-specific dictionary or co-occurrence statistics</p></li>
<li><p>Acronym restoring using predefined mappings</p></li>
</ul>
<p>Tradictional approaches are usually computationally efficient, and it is predictable and interpretable. However, it often requires significant manual effort to craft rules and update dictionary.</p>
<p>With the advancement of LLM, LLM can be used for more sophisticated query rewriting, as shown in the following example.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 20.2 </span> (LLM prompt to query rewrite)</p>
<section class="example-content" id="proof-content">
<p><strong>Prompt:</strong>
Given a query <em>What’s the state tax rate?</em> from a user located at <em>San Jose, US</em> on <em>June, 2024</em>.
Rewrite this query to help retrieve comprehensive results from search engine like Google.</p>
</section>
</div><p>This approach leverages the excellent language understanding and generation of LLM, and it can capture implicit context and semantic variations without explicit rules. However, it also has the following drawbacks.</p>
<ul class="simple">
<li><p>High computational cost. LLM inference is much costly than traditional technique. One can reduce the cost by using effectively distilled SLM and by using the triggering model to decide when to invoke LLM.</p></li>
<li><p>Lack of knowledge for queries involving rare entity names. This is an inherent drawback of LLM, which can be addressed by using a query-rewrite oriented RAG system.</p></li>
</ul>
</section>
<section id="approach-to-complex-multi-concept-queries">
<h3><span class="section-number">20.4.3. </span>Approach to Complex Multi-Concept Queries<a class="headerlink" href="#approach-to-complex-multi-concept-queries" title="Link to this heading">#</a></h3>
<p>Many user queries combine multiple concepts or requirements that need to be decomposed for effective retrieval. The query decomposition involves the following steps:</p>
<ul class="simple">
<li><p>Breaking down complex queries into simpler, atomic questions</p></li>
<li><p>Creating multiple focused, logically-related subqueries</p></li>
</ul>
<div class="proof example admonition" id="example-2">
<p class="admonition-title"><span class="caption-number">Example 20.3 </span></p>
<section class="example-content" id="proof-content">
<p>The original query <em>Can I take ibuprofen with my blood pressure medication while pregnant?</em>” can be decomposed into:</p>
<ol class="arabic simple">
<li><p>Is ibuprofen safe during pregnancy?</p></li>
<li><p>What are the interactions between ibuprofen and blood pressure medication”</p></li>
</ol>
</section>
</div><p>LLM is best probably the best tool to handle complex queries. To guide LLM to produce useful and desired outcome, we can apply various prompting technique, like few shot prompting and CoT [<a class="reference internal" href="../chapter_prompt/basic_prompt.html#chapter-prompt-sec-cot-prompting"><span class="std std-ref">Chain-of-Thought (CoT) Prompting</span></a>]. For queries involving complex reaonsing, we can use <a class="reference internal" href="../chapter_prompt/advanced_prompt.html#chapter-prompt-sec-step-back-prompting"><span class="std std-ref">Step Back Prompting</span></a>.</p>
</section>
<section id="approach-to-multi-turn-conversations">
<h3><span class="section-number">20.4.4. </span>Approach to Multi-Turn Conversations<a class="headerlink" href="#approach-to-multi-turn-conversations" title="Link to this heading">#</a></h3>
<p>In conversational QA scenario (e.g., a chatbot), understanding user’s query requires the understanding of previous conversational turns. In complex scenarioes, it needs the properly handling of</p>
<ul class="simple">
<li><p>Resolving pronouns and references (user might use this, that, he, she to refer to entities in previous turns)</p></li>
<li><p>Potential conflicting aspect (user might need agree and then disagree as the conversation evolves)</p></li>
<li><p>Topic switches (user might switch to another topic during one conversation session)</p></li>
</ul>
<p>Given the complexity of multi-turn conversations, using LLM can offer a clean solution rather than using multiple specialized NLP modules/models.</p>
</section>
</section>
<section id="rag-optimization-retriever-and-reranker">
<h2><span class="section-number">20.5. </span>RAG Optimization: Retriever and ReRanker<a class="headerlink" href="#rag-optimization-retriever-and-reranker" title="Link to this heading">#</a></h2>
<section id="retrieval-model-enhancement">
<h3><span class="section-number">20.5.1. </span>Retrieval Model Enhancement<a class="headerlink" href="#retrieval-model-enhancement" title="Link to this heading">#</a></h3>
<p>In the basic RAG system, only a dense retrieval model based on vector embedding is used. In general, dense embeddings has good performance on recall as fundamentally it is an inexact, semantically based approach. On the other hand, sparse retrieval (e.g., inverted index plus BM25), which relies on exact tem matching, has good performance on precision, particularly for queries with low level details (e.g., specific number, year, name) and rare entities.</p>
<p>Combining both sparse and dense approaches can capture different relevance features and can benefit from each other by leveraging complementary relevance information. For instance, both retrieval approaches can be used to generate initial recalled results and send to the next stage for re-ranking.</p>
<p>There are other ways deep model can be used to enhance sparse model. For example,</p>
<ul class="simple">
<li><p><strong>Term weight prediction</strong>: Deep model can be used to predict contextualized term weights in a query, which can help sparse retrieval model to pay more attention to important terms give the query as a context. (See <a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html#ch-neural-network-and-deep-learning-applicationnlp-irsearch-contextualized-term-importance"><span class="std std-ref">Contextualized Term Importance</span></a>)</p></li>
<li><p><strong>Enhance sparse retrieval ranker</strong>: Sparse retrieval in practice often use much more sophositcated rankers than BM25. These ranker can benefit from query-document semantic similarity feature besides query document exact matching features.</p></li>
</ul>
<p>We are usually face the training data scarcity issue when we adapt a generic retrieval models to a highly specialized domain (e.g., medical, law, scientific). <span id="id8">[]</span> introduces a LLM-based approach, PROMPTAGATOR,  to enhance task-specific retrievers.</p>
<p>As shown in the <a class="reference internal" href="#chapter-rag-fig-promptagator-demo"><span class="std std-numref">Fig. 20.7</span></a>, PROMPTAGATOR consists of three components:</p>
<ul class="simple">
<li><p>Prompt-based query generation, a task-specific prompt will be combined with a large language model to produce queries for all documents.</p></li>
<li><p>Consistency filtering, which cleans the generated data based on round-trip consistency - query should be answered by the passage from which the query was generated.</p></li>
<li><p>Retriever training, in which a retriever will be trained using the filtered synthetic data.</p></li>
</ul>
<figure class="align-default" id="chapter-rag-fig-promptagator-demo">
<a class="reference internal image-reference" href="../../_images/promptagator_training.png"><img alt="../../_images/promptagator_training.png" src="../../_images/promptagator_training.png" style="width: 765.8px; height: 225.39999999999998px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.7 </span><span class="caption-text">Illustration of PROMPTAGATOR, which generates synthetic data using LLM. Synthetic data, after consistency filtering, is used to train a retriever in labeled data scarcity domain. Image from <span id="id9">[]</span>.</span><a class="headerlink" href="#chapter-rag-fig-promptagator-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="retrieval-result-quality-control">
<h3><span class="section-number">20.5.2. </span>Retrieval Result Quality Control<a class="headerlink" href="#retrieval-result-quality-control" title="Link to this heading">#</a></h3>
<p>For RAG, maintaining high-quality retrieved data is crucial for generating accurate and coherent responses. Low quality content like redundancy and irrelvant will mislead LLM. In the following, we discuss several different approaches for quality control.</p>
<p><strong>Reranking</strong>: Reranking is a commonly used, and effective quality control measure to improve the precision of retrieved results. Reranking can employ both rule-based methods (utilizing metrics like Diversity, Relevance, and MRR) and model-based approaches (e.g., BERT cross-encoder). The outcome of reranking is to select the most relevant and useful paragraphs for the LLM to consume.</p>
<p><strong>Context Compression</strong>: A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .</p>
<p><strong>LLM-based quality evaluator</strong>: We can also prompt the LLM to evaluate the retrieved content before generating the final answer. This allows the LLM to filter out documents with poor relevance.</p>
</section>
</section>
<section id="rag-optimization-llm-understanding-generation">
<h2><span class="section-number">20.6. </span>RAG Optimization: LLM Understanding &amp; Generation<a class="headerlink" href="#rag-optimization-llm-understanding-generation" title="Link to this heading">#</a></h2>
<section id="motivation-and-objective">
<h3><span class="section-number">20.6.1. </span>Motivation and Objective<a class="headerlink" href="#motivation-and-objective" title="Link to this heading">#</a></h3>
<p>Recent developments in RAG systems have shown that while base LLMs are powerful, they often struggle with specific challenges in retrieval-augmented tasks. These challenges include hallucination, poor grounding in retrieved contexts, and inconsistent handling of edge cases. Traditional RAG systems rely heavily on prompt engineering and architectural improvements, but these approaches may not fully address the fundamental need for LLMs to better understand and utilize retrieved information. Furthermore, base LLMs often lack robust mechanisms for identifying and rejecting responses when context is insufficient or when faced with harmful queries.</p>
<p>The primary objective is to develop a specialized finetuning methodology that enhances an LLM’s ability to:</p>
<ol class="arabic simple">
<li><p>Generate faithful and reliable responses that are strictly grounded in the provided context</p></li>
<li><p>Recognize and explicitly reject queries when context is insufficient</p></li>
<li><p>Identify and appropriately handle harmful or inappropriate requests</p></li>
<li><p>Maintain consistent response quality across diverse query types</p></li>
<li><p>Produce helpful responses while adhering to safety guidelines</p></li>
</ol>
</section>
<section id="model-finetuning">
<h3><span class="section-number">20.6.2. </span>Model Finetuning<a class="headerlink" href="#model-finetuning" title="Link to this heading">#</a></h3>
<p>Our approach follows a systematic pipeline consisting of five key stages:</p>
<p>Data Sources
The foundation begins with two primary data sources:</p>
<ul class="simple">
<li><p>Public datasets that cover diverse domains and query types</p></li>
<li><p>Synthetic question-context pairs generated by combining queries with relevant search results, ensuring comprehensive coverage of both standard and edge cases</p></li>
</ul>
<p>Data Structure
The training data is carefully structured to include:</p>
<ul class="simple">
<li><p>Rejection samples where the LLM should learn to identify and reject inappropriate queries</p></li>
<li><p>Domain-diverse examples to ensure broad applicability</p></li>
<li><p>Multi-turn dialogue scenarios to handle complex information needs</p></li>
</ul>
<p>Instruction Structure
The instruction design incorporates:</p>
<ul class="simple">
<li><p>Explicit hallucination control mechanisms to ensure responses remain grounded in provided context</p></li>
<li><p>Source attribution patterns to maintain traceability</p></li>
<li><p>Rich text generation guidelines for comprehensive yet concise responses</p></li>
<li><p>Custom style definitions to maintain consistent output format</p></li>
</ul>
<p>Sample Selection
The training process emphasizes:</p>
<ul class="simple">
<li><p>Balanced mixing of positive and negative examples</p></li>
<li><p>Model evaluation criteria focused on faithfulness and safety</p></li>
<li><p>Rule-based filtering to ensure quality and relevance</p></li>
</ul>
<p>Model Training
The final stage employs:</p>
<ul class="simple">
<li><p>SFT (Supervised Fine-Tuning) to teach basic response generation and context utilization</p></li>
<li><p>DPO (Direct Preference Optimization) to refine model behavior based on human preferences</p></li>
</ul>
<p>This methodology ensures that the resulting model not only generates high-quality responses but also knows when and how to reject queries appropriately. The iterative nature of the training process, combined with careful sample selection and evaluation, produces an LLM that is both more reliable and more responsible in its response generation.</p>
<!-- ### Self-Aware LLM
However, we find that the retrieved knowledge does not always help and even has a
negative impact on original responses occasionally.

To better make use of both internal knowledge and external world knowledge,
we investigate eliciting the model’s ability to
recognize what they know and do not know
(which is also called “self-knowledge”) and
propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method
which can let LLMs refer to the questions
they have previously encountered and adaptively call for external resources when dealing with new questions.

investigate eliciting the selfknowledge of LLMs and propose a simple yet effective Self-Knowledge guided Retrieval augmentation (SKR) method to flexibly call the retriever
for making better use of both internal and external
knowledge


The proposed direct prompting and in-context
learning methods can elicit self-knowledge of
LLMs to some extent. However, they have several limitations. First, both methods require designing prompts and calling the LLMs for each new
question, which makes it impractical. Second, incontext learning could also be unstable due to contextual bias and sensitivity

| Template |
| :--- |
| Do you need additional information to answer this question? |
| Would you like any extra prompts to help you? |
| Would you like any additional clues? |
| Can you answer this question based on what you  know? |
| Can you solve this question now? |


```{figure} ../img/chapter_rag/data_source/self_knowledge_LLM_demo.png
---
scale: 60%
name: chapter_rag_fig_rag_demo
---
omparison between two responses given by InstructGPT. The retrieved passages are relevant but not particularly helpful for solving the question, which influences the model’s judgment and leads to incorrect answers. Image from {cite:p}`wang2023self`.
``` -->
</section>
</section>
<section id="advanced-rag">
<h2><span class="section-number">20.7. </span>Advanced RAG<a class="headerlink" href="#advanced-rag" title="Link to this heading">#</a></h2>
<section id="agentic-rag">
<h3><span class="section-number">20.7.1. </span>Agentic RAG<a class="headerlink" href="#agentic-rag" title="Link to this heading">#</a></h3>
<p>Based on the image, I’ll explain the concept of Agentic RAG (Retrieval-Augmented Generation):</p>
<p>Agentic RAG represents an evolution of traditional RAG systems by incorporating multiple specialized agents that work together to produce more accurate and reliable responses. The diagram shows multiple agents then work collaboratively within a structured framework:</p>
<ul class="simple">
<li><p>The Planning Agent determines the strategy for answering the question</p></li>
<li><p>The Synthesis Agent processes and combines information</p></li>
<li><p>These agents interact with the search service to gather necessary information</p></li>
</ul>
<figure class="align-default" id="chapter-rag-fig-rag-agent-rag">
<a class="reference internal image-reference" href="../../_images/agentic_rag_introduction.png"><img alt="../../_images/agentic_rag_introduction.png" src="../../_images/agentic_rag_introduction.png" style="width: 567.9px; height: 187.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.8 </span><span class="caption-text">Illustration of agentic RAG, in which multiple agents work collaboratively to provide grounding.</span><a class="headerlink" href="#chapter-rag-fig-rag-agent-rag" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>What makes this “agentic” is the way it delegates different aspects of the task to specialized agents that each handle specific parts of the process - planning, searching, and synthesizing. This division of labor allows for more sophisticated reasoning and better accuracy compared to simple RAG systems that just retrieve and generate without this structured agency approach.</p>
<p>As shown in <a class="reference internal" href="#chapter-rag-fig-rag-agent-rag-example"><span class="std std-numref">Fig. 20.9</span></a>, the key innovation here appears to be the coordination between these different agents, allowing them to work together to break down complex queries into manageable steps and cross-validate information before producing a final response. This cooperative approach helps reduce errors and provides more reliable answers by combining multiple perspectives and verification steps.</p>
<figure class="align-default" id="chapter-rag-fig-rag-agent-rag-example">
<a class="reference internal image-reference" href="../../_images/agentic_rag_examples.png"><img alt="../../_images/agentic_rag_examples.png" src="../../_images/agentic_rag_examples.png" style="width: 502.2px; height: 392.85px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.9 </span><span class="caption-text">Agentic RAG example in breaking down a complex query into smaller manageable steps.</span><a class="headerlink" href="#chapter-rag-fig-rag-agent-rag-example" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>While agentic RAG can improve the success on complex and reasoning-demanding queries, but it often comes at the cost of increased latency, higher resource usage, and potential reliability issues.</p>
<ul class="simple">
<li><p><strong>High latency</strong>: reasoning steps are lengthy, multiple calls to LLM are required; diffcult to manage and control the response time.</p></li>
<li><p><strong>High cost</strong>: Multiple reasoning and search steps result in high GPU serving costs</p></li>
<li><p><strong>Error prone reasoning</strong>: As there are multiple reasoning steps and each steps can cause error, the final reasoning outcome is error-prone. Reasoning steps might mistakenly reject correct answers.</p></li>
</ul>
</section>
<section id="graphrag">
<h3><span class="section-number">20.7.2. </span>GraphRAG<a class="headerlink" href="#graphrag" title="Link to this heading">#</a></h3>
<p>Baseline RAG was created to help solve this problem, but we observe situations where baseline RAG performs very poorly. For example:</p>
<ul class="simple">
<li><p>Baseline RAG struggles to connect the dots. This happens when answering a question requires traversing disparate pieces of information through their shared attributes in order to provide new synthesized insights.</p></li>
<li><p>Baseline RAG performs poorly when being asked to holistically understand summarized semantic concepts over large data collections or even singular large documents.</p></li>
</ul>
<p>To tackle these challenges for baseline RAG, <strong>GraphRAG</strong> [<span id="id10">[]</span>] was proposed by Microsoft to use knowledge graphs to aid question-and-answer when reasoning about complex information.</p>
<p>The GraphRAG process involves the following steps:</p>
<ul class="simple">
<li><p>Extracting a knowledge graph out of raw text, usually using a LLM,</p></li>
<li><p>Building a community hierarchy for nodes in the knowledge graph,</p></li>
<li><p>Generating summaries or other meta-data for these communities,</p></li>
<li><p>Leveraging these structures when perform RAG-based tasks.</p></li>
</ul>
<p>Compared with Baseline RAG, which finds the top-k semantically related document chunks to use as context for synthesizing the answer, GraphRAG uses subGraphs related to entities in the task or question as context. More specifically, GraphRAG will have the following online querying process:</p>
<ul class="simple">
<li><p>Search related <strong>entities</strong> of the quesion/task (the search could be keyword extraction based or embedding based)</p></li>
<li><p>Get subGraph of those entities (<span class="math notranslate nohighlight">\(k\)</span>-depth) from the <strong>knowledge graph</strong></p></li>
<li><p>Build context based on the subGraph</p></li>
</ul>
</section>
</section>
<section id="rag-evaluation">
<h2><span class="section-number">20.8. </span>RAG Evaluation<a class="headerlink" href="#rag-evaluation" title="Link to this heading">#</a></h2>
<p>Evaluation and benchmarking are crucial steps for RAG development. You cannot improve something you cannot measure it.</p>
<p>RAG application evaluation consists of two facets:
<strong>Retrieval Evaluation</strong>, which evaluates if the retrieved sources are relevant to the query.
<strong>Response Evaluation</strong>, which evaluates if the final LLM response:</p>
<ul class="simple">
<li><p>Be consistent consistent retrieved context,</p></li>
<li><p>Addresses the information need of the query</p></li>
<li><p>Follows additional guidelines if any (specified by the user).</p></li>
</ul>
<p>Evaluting the response quality is not a straight forward task and could be subjective. One popular way is to use a powerful LLM (e.g. GPT-4) to decide the response quality from different aspects. For example,
<strong>Correctness</strong>: Whether the generated answer matches that of the reference answer given the query (requires labels).
semantic similarity whether the predicted answer is semantically similar to the reference answer (requires labels).
<strong>Faithfulness</strong>: Evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there’s hallucination).
<strong>Answer Relevance</strong>: Whether the generated answer is relevant to the query.
<strong>Instruction Following</strong>: Whether additional instructions are followed.
<strong>Alignment with Reference Answer</strong>: If there are high quality reference answer, we can use it to compare the alignment of generated response and reference answer.</p>
<p>We can leverage established relevance and ranking metrics to evaluating the <strong>retrieval quality</strong>, with LLM as query-document relevance labeler.</p>
<p>When we adopt RAG is a new domain, we might not have enough test data to evaluate how the system works. We can leverage LLM to generate synthetic (question, answer) pairs.</p>
<p>There are also efforts to automatic the RAG evaluation process, as shown in the following [<a class="reference internal" href="#chapter-rag-fig-rag-rag-checker"><span class="std std-numref">Fig. 20.10</span></a>] from <strong>RAGChecker</strong> [<span id="id11">[]</span>].</p>
<figure class="align-default" id="chapter-rag-fig-rag-rag-checker">
<a class="reference internal image-reference" href="../../_images/rag_checker_demo.png"><img alt="../../_images/rag_checker_demo.png" src="../../_images/rag_checker_demo.png" style="width: 261.3px; height: 234.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.10 </span><span class="caption-text">llustration of the proposed metrics in RAGChecker. Image from <span id="id12">[]</span>.</span><a class="headerlink" href="#chapter-rag-fig-rag-rag-checker" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="further-rag-discussion">
<h2><span class="section-number">20.9. </span>Further RAG Discussion<a class="headerlink" href="#further-rag-discussion" title="Link to this heading">#</a></h2>
<section id="rag-vs-prompting-and-fine-tuning">
<h3><span class="section-number">20.9.1. </span>RAG vs Prompting and Fine Tuning<a class="headerlink" href="#rag-vs-prompting-and-fine-tuning" title="Link to this heading">#</a></h3>
<p>When adapt an generalist LLM to different usage scenarios, there are different approaches, including direct prompting, fine-tuning, and RAG.</p>
<p>Each method has distinct characteristics as illustrated in <a class="reference internal" href="#chapter-rag-fig-rag-vs-prompt-ft"><span class="std std-numref">Fig. 20.11</span></a>. We used a quadrant chart to illustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can leverage external knowledge via information retrieval, making it excellet for knowledge intentsive tasks. In contrast, FT is suitable for customizing models to specific structures, styles, or formats.</p>
<p>Prompt Engineering requires low modifications to the model. It is suitable for relative simple tasks without intensive external knowledge. RAG is particularly suitable dealing with dynamic knowledge tasks. For these tasks, indexing stage in RAG often has auto-refresh ability, enabling RAG to provide realtime knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with low latency scenarios, RAG has to spend extra time (~200-500ms) to perform retrieval. On the other hand, FT is more static, requiring retraining for knowledge updates and instruction following. It often demands significant computational resources for dataset preparation and training.</p>
<figure class="align-default" id="chapter-rag-fig-rag-vs-prompt-ft">
<a class="reference internal image-reference" href="../../_images/RAG_vs_FT_vs_prompt.png"><img alt="../../_images/RAG_vs_FT_vs_prompt.png" src="../../_images/RAG_vs_FT_vs_prompt.png" style="width: 730.4000000000001px; height: 434.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.11 </span><span class="caption-text">RAG compared with other model optimization methods in the aspects of <strong>External Knowledge Required</strong> and <strong>Model Adaption Required</strong>.  Image from <span id="id13">[]</span>.</span><a class="headerlink" href="#chapter-rag-fig-rag-vs-prompt-ft" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In the following, we also summarize different factors to consider when choosing among prompting, fine-tuning, and RAG.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1536">
<caption><span class="caption-number">Table 20.3 </span><span class="caption-text">Different factors to consider when choosing among prompting, fine-tuning, and RAG.</span><a class="headerlink" href="#id1536" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Factors</p></th>
<th class="head text-left"><p>Prompting</p></th>
<th class="head text-left"><p>Fine-tuning</p></th>
<th class="head text-left"><p>RAG</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Dynamic/up-to-date knowledge</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Reduce hullucilation</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Model behavior customizatio</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
<td class="text-left"><p>❌</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Training cost</p></td>
<td class="text-left"><p>✅</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Explanability</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>General ability</p></td>
<td class="text-left"><p>✅</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Low latency</p></td>
<td class="text-left"><p>✅</p></td>
<td class="text-left"><p>❌</p></td>
<td class="text-left"><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="rag-vs-long-context-llm">
<h3><span class="section-number">20.9.2. </span>RAG vs Long Context LLM<a class="headerlink" href="#rag-vs-long-context-llm" title="Link to this heading">#</a></h3>
<p>Long context LLM is one active research area, as enabling long context can open opportunties to handle more complex tasks.
For example, Gemini 1.5 (<a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</a>) has a context window of 1 million tokens.  This capability makes it possible for long-document question answering, in which user can now incorporate the entire document directly into the prompt.</p>
<p><strong>Do we still need information retrieval component when it comes to long context LLM?</strong> Can we just put a large document as knowledge base in the prompt?
In fact, information retrieval plays an irreplaceable role. Providing LLMs with a large amount of context has the following drawbacks:</p>
<ul class="simple">
<li><p>Inference speed is signficicantly reduced due to long sequences. In comparison,</p></li>
<li><p>The long context usually contain many irrelevant and noisy parts, which will impact the model performance.</p></li>
<li><p>The process is not tracable or explanable.</p></li>
</ul>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">20.10. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<p>Important resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://acl2023-retrieval-lm.github.io/">Retrieval-based Language Models and Applications ACL2023 Tutorial</a></p></li>
</ul>
<div class="docutils container" id="id14">
<div role="list" class="citation-list">
<div class="citation" id="id1402" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NYLC19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. <em>arXiv preprint arXiv:1904.08375</em>, 2019.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_rag"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_prompt/advanced_prompt.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Advanced Prompting Techniques</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_application_IR/information_retrieval_fundamentals.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Information Retrieval and Text Ranking</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">20.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-frameworks">20.2. RAG Frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-rag">20.2.1. Basic RAG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimizations">20.2.2. RAG Optimizations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-challenges-in-practice">20.2.3. RAG Challenges in Practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-documents">20.3. RAG Optimization: Documents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-data-sources">20.3.1. Indexing Data Sources</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-source-augmentation">20.3.2. Data Source Augmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-splitting-and-granularity">20.3.3. Document Splitting and Granularity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-query-understanding-and-rewriting">20.4. RAG Optimization: Query Understanding and Rewriting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">20.4.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-vocalbulary-mismatch">20.4.2. Approach to Vocalbulary Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-complex-multi-concept-queries">20.4.3. Approach to Complex Multi-Concept Queries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-to-multi-turn-conversations">20.4.4. Approach to Multi-Turn Conversations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-retriever-and-reranker">20.5. RAG Optimization: Retriever and ReRanker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-model-enhancement">20.5.1. Retrieval Model Enhancement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-result-quality-control">20.5.2. Retrieval Result Quality Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-optimization-llm-understanding-generation">20.6. RAG Optimization: LLM Understanding &amp; Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-objective">20.6.1. Motivation and Objective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-finetuning">20.6.2. Model Finetuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-rag">20.7. Advanced RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-rag">20.7.1. Agentic RAG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graphrag">20.7.2. GraphRAG</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-evaluation">20.8. RAG Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-rag-discussion">20.9. Further RAG Discussion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-vs-prompting-and-fine-tuning">20.9.1. RAG vs Prompting and Fine Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-vs-long-context-llm">20.9.2. RAG vs Long Context LLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.10. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>