
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Language Models &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_foundation/language_models';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Early Neural Language Models" href="neural_language_models.html" />
    <link rel="prev" title="Introduction: Unveiling the Power of Language Models in the Age of Artificial Intelligence" href="../index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural_language_models.html">Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="word_embeddings.html">Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers_and_bert.html">Transformers and BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">LLM Dense Architectures Fundamentals</a></li>

<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">MOE sparse models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">LLM training fundamentals</a></li>

<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">LLM finetuning</a></li>


<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">LLM alignement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">LLM Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">Inference acceleration: Quantization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">Basic prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">Advanced prompt techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Retrieval-Augmented Generation (RAG)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">Basic RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">Advanced rag techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">Vision transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">Information Retrieval Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">Application of LLM in IR</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/chapter_foundation/language_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Language Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-language-models">Statistical language models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-language-model"><span class="math notranslate nohighlight">\(n\)</span>-gram language model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameter-estimation">Model parameter estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#star-deriving-the-mle"><span class="math notranslate nohighlight">\(\star\)</span> Deriving the MLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choices-of-n-and-bias-variance-trade-off">Choices of <span class="math notranslate nohighlight">\(n\)</span> and bias-variance trade-off</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-vocabulary-oov-words-and-rare-words">Out of vocabulary (OOV) words and rare words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-and-discounting-techniques">Smoothing and discounting techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-alpha-smoothing-and-discounting">Add <span class="math notranslate nohighlight">\(\alpha\)</span> smoothing and discounting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#katz-s-back-off">Katz’s back-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">Evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-perplexity">More on perplexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">Datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-language-models">Neural language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="language-models">
<h1>Language Models<a class="headerlink" href="#language-models" title="Link to this heading">#</a></h1>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Natural languages emerge from formal or casual communications between human beings and only have a limited set of formal rules to follow. Linguists have been directing decades’ efforts to modeling languages via grammars, rules, and structure of natural language. In NLP, language modeling \cite{goldberg2017neural} tasks involve the the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words forming a sentence that make sense [<a class="reference internal" href="#chapter-foundation-fig-languagemodelingtaskdemo"><span class="std std-numref">Fig. 1</span></a>].</p>
<figure class="align-default" id="chapter-foundation-fig-languagemodelingtaskdemo">
<a class="reference internal image-reference" href="../../_images/languageModelingTask_demo.png"><img alt="../../_images/languageModelingTask_demo.png" src="../../_images/languageModelingTask_demo.png" style="width: 627.0px; height: 135.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Illustrating basic language modeling tasks: assigning probability to a sentence (left); and predict the next word based on preceding context (right).</span><a class="headerlink" href="#chapter-foundation-fig-languagemodelingtaskdemo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Language modeling is an indispensable components in real-world applications. A language model can be used directly to generate new sequences of text that appear to have come from the corpus. For example, an AI-writer can generate an article with given key words. Moreover, language models are an integral part of many text-related applications, where they ensure output word sequences in these tasks to have a sensible meaning or to appear fluent like genuine human languages. These tasks include</p>
<ul class="simple">
<li><p>Optical character Recognition, a task converting images to sentences.</p></li>
<li><p>Machine translation, where one sentence from one language is translated to a sentence in another language.</p></li>
<li><p>Image captioning, where a image is used as the input and the output is a descriptive sentence.</p></li>
<li><p>Text summarization, where a large paragraph or a full article is converted several summarizing sentences.</p></li>
<li><p>Speech recognition, where audio signal is converted to meaningful words, phases, and sentences.</p></li>
</ul>
<p>A closed related language modeling task is predict the next word following a sequence of words.</p>
<p>Predicting the next word in a language system can be much more challenging than predicting the next observation in the many physical system. One perspective is that the evolution of many physical systems can governed by physical laws that are well established; on the other hand, the generation of next word, sentence, and even logically coherent paragraph is closely related to human reasoning and intelligence, which remain poorly understood. In fact, it is widely believed that being able to write up logical and coherent paragraphs is an indication of human-level intelligence <span id="id1">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1128" title="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.">RWC+19</a>]</span>.</p>
<p>In this section, we first consider a classical <span class="math notranslate nohighlight">\(n\)</span>-gram statistical language approach <span id="id2">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id890" title="Christopher D Manning and Hinrich Schütze. Foundations of statistical natural language processing. MIT press, 1999.">MSchutze99</a>]</span>, where we count the co-occurrence of words and model the language generation using probabilistic models (e.g., Markov chains). <span class="math notranslate nohighlight">\(n\)</span>-gram models focus on predicting next word based on the superficial co-occurrence counts instead of high-level semantic links between the context and the unknown next word,  It further suffers from the curse of dimensionality since sentences are normally long and language vocabulary size is huge (e.g., <span class="math notranslate nohighlight">\(10^6\)</span>). Over the past two decades, the neural network based language models have attracted considerable attention because these models are able to reveal deep connections between words as well as to alleviate the dimensionality challenge. Our focus in this section is on early development of neural language models; recent developments of pre-trained language models (e.g., GPT 1-3 <span id="id3">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id382" title="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.">BMR+20</a>, <a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1127" title="Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.">RNSS18</a>, <a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1128" title="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.">RWC+19</a>]</span>) will be covered in next section.</p>
</section>
<section id="statistical-language-models">
<h2>Statistical language models<a class="headerlink" href="#statistical-language-models" title="Link to this heading">#</a></h2>
<section id="n-gram-language-model">
<h3><span class="math notranslate nohighlight">\(n\)</span>-gram language model<a class="headerlink" href="#n-gram-language-model" title="Link to this heading">#</a></h3>
<p>The essential goal of language model is to estimate the joint probability of word sequence <span class="math notranslate nohighlight">\(w_{1:N} = w_1,...,w_{N}\)</span>. By definition, the unbiased probability estimation is given by</p>
<div class="math notranslate nohighlight">
\[P\left(w_{1:N}\right) = \frac{\operatorname{count}(w_{1:N})}{\operatorname{count}(\text{all word sequence ever exists in the universe})}.\]</div>
<p>Estimating the probability has the following challenge:</p>
<ul class="simple">
<li><p>Given a vocabulary size <span class="math notranslate nohighlight">\(V\)</span>, the number of events <span class="math notranslate nohighlight">\(w_{1:N}\)</span> is exponentially increasing with <span class="math notranslate nohighlight">\(N\)</span>. As <span class="math notranslate nohighlight">\(N\)</span> can take arbitrarily large value, it would not be possible even to store the model.</p></li>
<li><p>Because of the intractably large number of events, getting reliable estimation of the denominator and the nominator requires impractically large data.</p></li>
</ul>
<p>Alternatively, one can achieve the joint probability estimation via a <strong>truncated conditional probability framework</strong>. To begin with, the joint probability the sequence <span class="math notranslate nohighlight">\(w_{1:N}\)</span> has the following decomposition based on the conditional probability <strong>chain rule</strong>:</p>
<div class="math notranslate nohighlight">
\[P(w_{1:N}) = P(w_{N} | w_{1:N-1}) P(w_{N-1} | w_{1:N-2}) \cdots P(w_{3} | w_{1}, w_{2})P(w_{2} | w_{1}) P(w_{1}).\]</div>
<p>The language modeling task involves the estimation <span class="math notranslate nohighlight">\(P\left(w_1,...,w_N\right)\)</span>. The task is equivalent to estimate the conditional probability of predicting <span class="math notranslate nohighlight">\(w_N\)</span> based on <span class="math notranslate nohighlight">\(w_1,...,w_{N-1}\)</span> thanks to</p>
<div class="math notranslate nohighlight">
\[p(w_N|w_1,...,w_{N-1}) = \frac{w_1,...,w_{N}}{w_1,...,w_{N-1}}.\]</div>
<p>In the truncated-<span class="math notranslate nohighlight">\(n\)</span> conditional probability framework, we approximate <span class="math notranslate nohighlight">\(P(w_{1:N})\)</span> via</p>
<div class="math notranslate nohighlight">
\[P(w_{1:N}) \approx P(w_{N} | w_{N-n:N-1}) P(w_{N-1} | w_{N-1-n:N-2}) \cdots P(w_{3} | w_{1}, w_{2})P(w_{2} | w_{1}) P(w_{1}).\]</div>
<p>The truncated-<span class="math notranslate nohighlight">\(n\)</span> conditional probability framework is formally known as <span class="math notranslate nohighlight">\(n\)</span>-gram statistical language model. The <span class="math notranslate nohighlight">\(n\)</span>-gram statistical language model is one fundamental model that estimates probabilities by counting the co-occurrence of <span class="math notranslate nohighlight">\(n\)</span> consecutive words. More precisely, an <span class="math notranslate nohighlight">\(n\)</span>-gram is a sequence consisting of <span class="math notranslate nohighlight">\(n\)</span> words. For example, given a sentence <em>The homework is due tomorrow</em>, it has:</p>
<ul class="simple">
<li><p>2-grams (or bigrams) like <em>the homework, homework is, is due, due tomorrow</em>;</p></li>
<li><p>3-grams (or trigrams) like <em>the homework is, homework is due, is due tomorrow</em>;</p></li>
<li><p>4-grams like <em>the homework is due, homework is due tomorrow</em>.</p></li>
</ul>
<p>Often we pad the sequence in the front by <span class="math notranslate nohighlight">\(n-1\)</span> <SOS> (i.e., <span class="math notranslate nohighlight">\(w_{-1}, w_{-2}, w_{-n + 1} = \)</span> <SOS>) such that we can write</p>
<div class="math notranslate nohighlight">
\[P(w_{1:N}) \approx \prod_{k=1}^{N} P(w_{k} | w_{k-n+1:k-1}).\]</div>
<p>For example, in the bigram language model, we have</p>
<div class="math notranslate nohighlight">
\[
P\left(w_{N} | w_{1}^{N-1}\right) \approx P\left(w_{N} | w_{N-1}\right)
\]</div>
<p>and therefore</p>
<div class="math notranslate nohighlight">
\[
P\left(w_{1}^{N}\right) \approx \prod_{k=1}^{N} P\left(w_{k} | w_{k-1}\right).
\]</div>
</section>
<section id="model-parameter-estimation">
<h3>Model parameter estimation<a class="headerlink" href="#model-parameter-estimation" title="Link to this heading">#</a></h3>
<p>For a <span class="math notranslate nohighlight">\(n\)</span>-gram language model, there are <span class="math notranslate nohighlight">\(|V|^n\)</span> the model parameters to be estimated:</p>
<div class="math notranslate nohighlight">
\[P(w_n|w_1,...,w_{n-1}),\]</div>
<p>Here <span class="math notranslate nohighlight">\(|V|\)</span> is the vocabulary size.</p>
<p>Given a training corpus, the MLE (maximum likelihood estimator) of <span class="math notranslate nohighlight">\(n\)</span>-gram conditional probability used above can be simply obtained by counting. Specifically, for a bigram model, we have</p>
<div class="math notranslate nohighlight">
\[
P\left(w_{n} | w_{n-1}\right)=\frac{\operatorname{count}\left(w_{n-1}, w_{n}\right)}{\sum_{w} \operatorname{count}\left(w_{n-1}, w\right)} = \frac{\operatorname{count}\left(w_{n-1}, w_{n}\right)}{\operatorname{count}\left(w_{n-1}\right)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{count}(x, y)\)</span> is the total count of bigrams <span class="math notranslate nohighlight">\(x,y\)</span> in the corpus and <span class="math notranslate nohighlight">\(\sum_{w} \operatorname{count}\left(w_{n-1}, w\right)\)</span> is the count of all bigrams starting with <span class="math notranslate nohighlight">\(w_{n-1}\)</span>.</p>
<p>For a trigram model, we have</p>
<div class="math notranslate nohighlight">
\[P\left(w_{n} | w_{n-1}, w_{n-2}\right)=\frac{\operatorname{count}\left(w_{n-2}, w_{n-1}, w_{n}\right)}{\sum_{w\in |V|}(\operatorname{count}\left(w_{n-2}, w_{n-1}, w\right))}=\frac{\operatorname{count}\left(w_{n-2}, w_{n-1}, w_{n}\right)}{\operatorname{count}\left(w_{n-2}, w_{n-1}\right)}.\]</div>
<p>For example,</p>
<div class="math notranslate nohighlight">
\[p(\text { is| artificial intelligence })=\frac{\operatorname{count}(\text { artificial intelligence is})}{\operatorname{count}(\text { artificial intelligence })}.\]</div>
<section id="star-deriving-the-mle">
<h4><span class="math notranslate nohighlight">\(\star\)</span> Deriving the MLE<a class="headerlink" href="#star-deriving-the-mle" title="Link to this heading">#</a></h4>
<p>Consider a sequence of random variables <span class="math notranslate nohighlight">\(w_0, w_1, ...,w_N\)</span> representing a sentence composed of a sequence of tokens taken from the vocabulary <span class="math notranslate nohighlight">\(|V|\)</span>. Let <span class="math notranslate nohighlight">\(w_0\)</span> be deterministic and take the value of starting token <SOS>. For a bigram language model, the model is completely defined by the conditional probability matrix <span class="math notranslate nohighlight">\(\theta \in \mathcal{R}^{|V|\times |V|}\)</span>, with <span class="math notranslate nohighlight">\(P(w_n = j|w_{n-1} = i) = \theta_{ij}\)</span>. Here <span class="math notranslate nohighlight">\(i\)</span> is shorthand for the <span class="math notranslate nohighlight">\(i\)</span> token in the vocabulary.</p>
<p>To estimate the model parameter, we look at the log likelihood of a sample sequence <span class="math notranslate nohighlight">\(w_0, w_1,...,w_N\)</span>, which is is given by</p>
<div class="math notranslate nohighlight">
\[L = \sum_{n=1}^N \ln P(w_n|w_{n-1}) = \sum_{i \in |V|}\sum_{j \in |V|} \operatorname{count}(i, j) \ln  P(j|i) = \sum_{i \in |V|}\sum_{j \in |V|} \operatorname{count}(i, j) \ln \theta_{i,j},\]</div>
<p>Here <span class="math notranslate nohighlight">\(\operatorname{count}(i, j)\)</span> is the count of bigram <span class="math notranslate nohighlight">\(i,j\)</span> in the training corpus.</p>
<p>Because <span class="math notranslate nohighlight">\(\theta_{ij}\)</span> is under additional constraints given by</p>
<div class="math notranslate nohighlight">
\[\sum_{j\in |V|}\theta_{ij} = 1, i=1,...,|V|,\]</div>
<p>we can use Lagrange multiplier to maximize the log likelihood under constraints. The Lagrange is given by</p>
<div class="math notranslate nohighlight">
\[L = \sum_{i \in |V|}\sum_{j \in |V|} \operatorname{count}(i, j) \ln \theta_{i,j} - \sum_{i\in |V|}\lambda_i (\sum_{j\in |V|}\theta_{ij} - 1) = 0.\]</div>
<p>Set <span class="math notranslate nohighlight">\(\partial L / \partial \theta_{ij} = 0\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\frac{\operatorname{count}(i, j)}{\theta_{i,j}} = \lambda_i \Rightarrow \theta_{i,j} = \frac{\operatorname{count}(i, j)}{\lambda_i}.\]</div>
<p>One can easily show that $<span class="math notranslate nohighlight">\(\lambda_i = \sum_{j\in |V|} \operatorname{count}(i, j) = \operatorname{count}(i),\)</span>$
therefore,</p>
<div class="math notranslate nohighlight">
\[\theta_{ij} = \frac{\operatorname{count}(i, j)}{\operatorname{count}(i)}.\]</div>
<p>\subsubsection{Special case: unigram language model}</p>
<p>In the unigram language model, we assume
$<span class="math notranslate nohighlight">\(P(w_1, w_2,...,w_N) = \prod_{i=1}^N P(w_i).\)</span>$</p>
<p>We can empirically estimate
$<span class="math notranslate nohighlight">\(P(w_i) = \frac{\operatorname{count}(w_i)}{\sum_{j=1}^{|V|}\operatorname{count}(w_j)} = \frac{\operatorname{count}(w_i)}{N_{|V|}},\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(\operatorname{count}(w_i)\)</span> is the number of <span class="math notranslate nohighlight">\(w_i\)</span> in the corpus, <span class="math notranslate nohighlight">\(|V|\)</span> is the vocabulary size, and <span class="math notranslate nohighlight">\(N_{|V|}\)</span> is the total number of words in the corpus.</p>
<p>A unigram language model does not capture any transitional relationship between words. When we use a unigram language model to generate sentences, the resulted sentences would mostly consist of high-frequent words and thus hardly make sense.</p>
</section>
</section>
<section id="choices-of-n-and-bias-variance-trade-off">
<h3>Choices of <span class="math notranslate nohighlight">\(n\)</span> and bias-variance trade-off<a class="headerlink" href="#choices-of-n-and-bias-variance-trade-off" title="Link to this heading">#</a></h3>
<p>For a <span class="math notranslate nohighlight">\(n\)</span>-gram model, when <span class="math notranslate nohighlight">\(n\)</span> is too small, say a bigram model, the model can capture synactic structure in the language but can hardly capture the sequential, long distance relationship among words. For example, the syntatic structure like the fact that a noun or an adjective comes after \textit{enjoy} and the fact that a verb in its original form typically comes after \textit{to}. For long-range dependency, consider the following sentences:</p>
<ul class="simple">
<li><p>Gorillas always like to groom their friends.</p></li>
<li><p>The computer that’s on the 3rd floor of our office building crashed.</p></li>
</ul>
<p>In each example, the words written in bold depend on each other: the likelihood of their depends on knowing that gorillas is plural, and the likelihood of crashed depends on knowing that the subject is a computer. If the <span class="math notranslate nohighlight">\(n\)</span>-grams are not big enough to capture this context, then the resulting language model would offer probabilities that are too low for these sentences, and too high for sentences that fail basic linguistic tests like number agreement.</p>
<p>Typically, the longer the context we condition on in predicting the next word, the more coherent the sentences. But a language model with a large <span class="math notranslate nohighlight">\(n\)</span> can overfit to small-sized training corpus as it tends to memorize specific long sequence patterns in the training corpus and to give zero probabilities to those unseen. Specifically, a <span class="math notranslate nohighlight">\(n\)</span>-gram model has model parameter scales like <span class="math notranslate nohighlight">\(O(V^n)\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size.</p>
<p>The choice of appropriate <span class="math notranslate nohighlight">\(n\)</span> is related to bias-variance trade-off. A small <span class="math notranslate nohighlight">\(n\)</span>-gram size introduces high bias, and a large <span class="math notranslate nohighlight">\(n\)</span>-gram size introduces high variance. Since human language is full of long-range dependencies, in practice, we tend to keep <span class="math notranslate nohighlight">\(n\)</span> large and at the same time use smoothing tricks, as some sort of bias, to achieve low-variance estimates of the model parameters.</p>
</section>
<section id="out-of-vocabulary-oov-words-and-rare-words">
<h3>Out of vocabulary (OOV) words and rare words<a class="headerlink" href="#out-of-vocabulary-oov-words-and-rare-words" title="Link to this heading">#</a></h3>
<p>Some early speech and language applications might just involve a closed vocabulary, in which the vocabulary is known in advance and the runtime test set will contain words from the vocabulary. Modern natural language application typically involves an open vocabulary in which out of vocabulary words can occur. For those cases, which can simply add a pseudo-word <UNK>, and treat all the OOV words as the <UNK>.</p>
<p>In applications where we don’t have a prior vocabulary in advance and need to create one from corpus, we can limit the size of vocabulary by replacing low-frequency rare words by <UNK> or some other more fine-grained special symbols (i.e., replacing rare organization names by <ORG> and replacing rare people names by <PEOPLE>).</p>
<p>Note that how the vocabulary is constructed and how we map rare words to <UNK> affects how we evaluate the quality of language model using likelihood based metrics (e.g., perplexity). For example, a language model can achieve artificially low perplexity by choosing a small vocabulary and assigning many words to the unknown word.</p>
</section>
</section>
<section id="smoothing-and-discounting-techniques">
<h2>Smoothing and discounting techniques<a class="headerlink" href="#smoothing-and-discounting-techniques" title="Link to this heading">#</a></h2>
<section id="add-alpha-smoothing-and-discounting">
<h3>Add <span class="math notranslate nohighlight">\(\alpha\)</span> smoothing and discounting<a class="headerlink" href="#add-alpha-smoothing-and-discounting" title="Link to this heading">#</a></h3>
<p>One fundamental limit of the baseline counting method is the zero probability assigned to <span class="math notranslate nohighlight">\(n\)</span>-grams that do not appear in the training corpus. This has a significant impact when we use the model to score a sentence. For example,  one zero probability of any unseen bigram <span class="math notranslate nohighlight">\(w_{i-1},w_i\)</span> will cause the probability of the sequence <span class="math notranslate nohighlight">\(w_1,...,w_i,...,w_n\)</span> to be zero.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 1 </span></p>
<section class="example-content" id="proof-content">
<p>Suppose in the training corpus we have only seen following phrases starting with <em>denied</em>: <em>denied the allegations</em>, <em>denied the speculation</em>, <em>denied the rumors</em>, <em>denied the report</em>. Now suppose a sentence in the test set has the
phrase <em>denied the offer</em>. Our model will incorrectly estimate that <span style="color:red"><strong>the probability of the sentence is 0</strong></span>, which is obviously not true.</p>
</section>
</div><p>A simply remedy is to add <span class="math notranslate nohighlight">\(\alpha\)</span> <strong>imaginary</strong> counts to all the n-gram that can be constructed from the vocabulary, on top of the actual counts observed in the corpus.<a class="footnote-reference brackets" href="#myref1" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
<p>Take bigram language model as an example, the resulting estimation is given by</p>
<div class="math notranslate nohighlight">
\[P_{\text{Add-}\alpha}^{*}\left(w_{n} | w_{n-1}\right)=\frac{\operatorname{count}\left(w_{n-1}, w_{n}\right)+\alpha}{\sum_{w_{n}'\in |V|}(\operatorname{count}\left(w_{n-1}, w_{n}'\right)+ \alpha)}=\frac{\operatorname{count}\left(w_{n-1}, w_{n}\right)+\alpha}{\operatorname{count}\left(w_{n-1}\right)+ |V|\alpha}.\]</div>
<p>Here in the numerator, we add <span class="math notranslate nohighlight">\(\alpha\)</span> imaginary counts to the actual counts. In the denomerator, we add <span class="math notranslate nohighlight">\(|V|\alpha\)</span> to ensure that the probabilities are properly normalized, where <span class="math notranslate nohighlight">\(|V|\)</span> is the vocabulary size. Note that it is possible that <span class="math notranslate nohighlight">\(\operatorname{count}(w_{n-1}) = 0\)</span>. We also have <span class="math notranslate nohighlight">\(\operatorname{count}(w_{n-1}, w_n) = 0\)</span> and <span class="math notranslate nohighlight">\(P_{\text{Add-}\alpha}^{*}\left(w_{n} | w_{n-1}\right) = 1/|V|\)</span>.</p>
<p>Similarly, for a trigram language model, we have</p>
<div class="math notranslate nohighlight">
\[P_{\text{Add-}\alpha}^{*}\left(w_{n} | w_{n-1}, w_{n-2}\right)=\frac{\operatorname{count}\left(w_{n-2}, w_{n-1}, w_{n}\right)+\alpha}{\sum_{w_{n}'\in |V|}(\operatorname{count}\left(w_{n-2}, w_{n-1}, w_{n}'\right)+ \alpha)}=\frac{\operatorname{count}\left(w_{n-2}, w_{n-1}, w_{n}\right)+\alpha}{\operatorname{count}\left(w_{n-2}, w_{n-1}\right)+ |V|\alpha}.\]</div>
<p>When <span class="math notranslate nohighlight">\(\operatorname{count}\left(w_{n-2}, w_{n-1}\right) = 0\)</span>, we have <span class="math notranslate nohighlight">\(P_{\text{Add-}\alpha}^{*}\left(w_{n} | w_{n-1}, w_{n-2}\right) = 1/|V|.\)</span></p>
<p>To better understand the bias introduced from adding <span class="math notranslate nohighlight">\(\alpha\)</span> imaginary counts, we can compute the effective count of an event. Let  <span class="math notranslate nohighlight">\(c_i\)</span> is the count of event <span class="math notranslate nohighlight">\(i\)</span>. Let <span class="math notranslate nohighlight">\(M=\sum_{i=1}^E c_i\)</span> be the total counts of all possible events E in the corpus.
The effective count for event <span class="math notranslate nohighlight">\(i\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
c_i^*=\left(c_i+\alpha\right) \frac{M}{M+E \alpha},
\]</div>
<p>Note that the effective counts defined this way ensures that total count <span class="math notranslate nohighlight">\(M\)</span> satisfies <span class="math notranslate nohighlight">\(\sum_{i=1}^E c_i^*=\sum_{i=1}^E c_i=M\)</span> and the probability of the event <span class="math notranslate nohighlight">\(i\)</span> is <span class="math notranslate nohighlight">\(c_i^*/\sum_i c_i^* = c_i/M\)</span>.</p>
<p>In general, if <span class="math notranslate nohighlight">\(c_i &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(c_i* &lt; c_i\)</span>; if <span class="math notranslate nohighlight">\(c_i = 0\)</span>, then <span class="math notranslate nohighlight">\(c_i^* &gt; c_i = 0\)</span>. Therefore, when we are adding imaginary counts to all the events, the effective counts of all observed events are decreased and the effective counts for all unobserved events are increased. From the probability mass perspective, add-<span class="math notranslate nohighlight">\(\alpha\)</span> smoothing is equivalent to remove some probability mass from observed events and re-distribute the collected probability mass to all unobserved events.</p>
<p>We can also use the concept of discounting\footnote{A related definition is absolute discounting, which is given by <span class="math notranslate nohighlight">\(d_i = c_i - c_i^*\)</span>. } to reflect the change between <span class="math notranslate nohighlight">\(c^*_i\)</span> and <span class="math notranslate nohighlight">\(c_i\)</span>, which is given by
The discount for each <span class="math notranslate nohighlight">\(\mathrm{n}\)</span>-gram is then computed as,</p>
<div class="math notranslate nohighlight">
\[
d_i=\frac{c_i^*}{c_i}=\frac{\left(c_i+\alpha\right)}{c_i} \frac{M}{(M+E \alpha)}.
\]</div>
<p>The bias we add to the n-gram model is reflected on the value of <span class="math notranslate nohighlight">\(d_i\)</span>. When <span class="math notranslate nohighlight">\(\alpha = 0, d_i = 1.0\)</span>, there is no bias in the n-gram model estimation. For non-zero <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(d_i &lt; 1\)</span>. The smaller the <span class="math notranslate nohighlight">\(d_i\)</span>, the large the bias we introduce. For example, when the vocabulary size <span class="math notranslate nohighlight">\(|V|\)</span> is large, but the total number of tokens <span class="math notranslate nohighlight">\(M\)</span> is small, <span class="math notranslate nohighlight">\(d_i\)</span> approaches zero and the bias is thus large.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<section class="example-content" id="proof-content">
<p>Consider following 7 events with a total count of 20. The following table demonstrates the effects of adding 1 smoothing.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>event</p></th>
<th class="head"><p>counts</p></th>
<th class="head"><p>new counts</p></th>
<th class="head"><p>effective counts</p></th>
<th class="head"><p>unsmoothed probability</p></th>
<th class="head"><p>smoothed probability</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(c_0\)</span></p></td>
<td><p>8</p></td>
<td><p>9</p></td>
<td><p>6.67</p></td>
<td><p>0.4</p></td>
<td><p>0.33</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(c_1\)</span></p></td>
<td><p>5</p></td>
<td><p>6</p></td>
<td><p>4.44</p></td>
<td><p>0.25</p></td>
<td><p>0.22</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(c_2\)</span></p></td>
<td><p>4</p></td>
<td><p>5</p></td>
<td><p>3.70</p></td>
<td><p>0.2</p></td>
<td><p>0.19</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(c_3\)</span></p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>2.22</p></td>
<td><p>0.1</p></td>
<td><p>0.11</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(c_4\)</span></p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>1.48</p></td>
<td><p>0.05</p></td>
<td><p>0.07</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(c_5\)</span></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0.74</p></td>
<td><p>0</p></td>
<td><p>0.04</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(c_6\)</span></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0.74</p></td>
<td><p>0</p></td>
<td><p>0.04</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</div></section>
<section id="katz-s-back-off">
<h3>Katz’s back-off<a class="headerlink" href="#katz-s-back-off" title="Link to this heading">#</a></h3>
<p>The smoothing method discussed above borrows probability mass from observed <span class="math notranslate nohighlight">\(n\)</span>-grams and redistributes it to all <span class="math notranslate nohighlight">\(n-\)</span>grams, including both observed ones and unobserved ones. The borrowed amount is controlled by the parameter <span class="math notranslate nohighlight">\(\alpha\)</span>. A variation of such re-distribution is to just redistribute the borrowed probablity mass to those unobserved one. A key characteristic of these redistribution methods is that redistribution is carried out <strong>equally</strong>.</p>
<p>Another popular smoothing approach is \textbf{back-off}. The key idea is that the redistribution of the probability mass to an unobserved <span class="math notranslate nohighlight">\(n\)</span>-gram is dependent on the statistics of its constituent <span class="math notranslate nohighlight">\(n-1\)</span> gram. The latest development of <span class="math notranslate nohighlight">\(n\)</span>-gram language modeling is to use modified Kneser Ney smoothing\cite{chen1999empirical}.</p>
<p>In the <span class="math notranslate nohighlight">\(n\)</span>-gram Katz’s backoff model,</p>
<div class="math notranslate nohighlight">
\[\begin{split}P\left(w_i \mid w_{i-n+1:i-1}\right) 
	= \begin{cases}d_{w_{i-n+1:i}} \frac{\operatorname{count}\left(w_{i-n+1: i}\right)}{\operatorname{count}\left(w_{i-n+1:i-1}\right)} &amp; \text { if } \operatorname{count}\left(w_{i-n+1:i}\right) &gt; k \\
		\beta_{w_{i-n+1:i-1}} \lambda\left(w_i \mid w_{i-n+2:i-1}\right) &amp; \text { otherwise }\end{cases}
	\end{split}\]</div>
<p>Here the quantity <span class="math notranslate nohighlight">\(\beta\)</span> is the left-over probability mass for the <span class="math notranslate nohighlight">\((n-1)\)</span>-gram given by:</p>
<div class="math notranslate nohighlight">
\[
\beta_{w_{i-n+1:i-1}}=1- \sum_{\left\{w_i: \operatorname{count}\left(w_{i-n+1:i}\right)&gt;k\right\}}d_{w_{i-n+1:i}} \frac{\operatorname{count}\left(w_{i-n+1:i}\right)}{\operatorname{count}\left(w_{i-n+1:i-1}\right)}
	\]</div>
<p>Then the back-off redistribution weight, <span class="math notranslate nohighlight">\(\lambda\)</span>, is computed as follows:</p>
<div class="math notranslate nohighlight">
\[
\lambda(w_i \mid w_{i-n+2:i-1}=\frac{P\left(w_i \mid w_{i-n+2:i-1}\right)}{\sum_{\left\{w_i: \operatorname{count}\left(w_{i-n+1} \cdots w_i\right) \leq k\right\}} P\left(w_i \mid w_{i-n+2:i-1}\right)}
\]</div>
<p>We can interpret the back-off formula in the following way</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the amount of probability mass that has been borrowed from non-zero events.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> specifies how the borrowed weight should be re-distributed into zero event, and <span class="math notranslate nohighlight">\(\lambda\)</span> is proportional to the <span class="math notranslate nohighlight">\(n-1\)</span>-gram conditional probability</p></li>
</ul>
<p>In the simple case of backing off from bigrams to unigrams, the bigram probabilities are,</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(i \mid j)= \begin{cases}d\frac{\operatorname{count}(i, j)}{\operatorname{count}(j)} &amp; \text { if } \operatorname{count}(i, j)&gt;0 \\ \beta(j) \times \frac{P(i)}{\sum_{i^{\prime}: \operatorname{count}\left(i^{\prime}, j\right)=0} {P}^{\left(i^{\prime}\right)}} &amp; \text { if } \operatorname{count}(i, j)=0\end{cases}.\end{split}\]</div>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 1 </span> (Practical simple back-off)</p>
<section class="remark-content" id="proof-content">
<p>So far we have discussed the back-off idea of redistributing probability mass to unobserved n-grams based on their corresponding lower-order grams. All probabilities will be re-normalized after redistribution. In the practical applications, there are even simpler back-off strategy by satisfying some degree of accuracy. Consider a trigram language model.</p>
<ul class="simple">
<li><p>If we have trigram probability, use it;</p></li>
<li><p>If we don’t have trigram probability, use its bigram probabilities;</p></li>
<li><p>if we don’t even have bigram probabilities, then use unigram probabilities.</p></li>
</ul>
</section>
</div></section>
</section>
<section id="model-evaluation">
<h2>Model evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h2>
<section id="evaluation-metrics">
<h3>Evaluation metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading">#</a></h3>
<p>Given a training corpus, we can build different language models with different <span class="math notranslate nohighlight">\(n\)</span>-gram settings as well as other model smoothing hyperparameter. There model training hyperparameters give different bias and variance trade-off and we need an evaluation method to gauge which hyperparameter is better for intended applications.</p>
<p>One principled way is to evaluate the model predicted likelihood on an unseen test set, which consist of natural language sentences, from the intended applications. with unseen sentences and then compare the probability or its variant computed from different candidate models on the test set. A language model that assigns a higher probability to the test set is considered a better one.
We define \textbf{perplexity} of a language model on a test set as the inverse probability of the test set, normalized by the number of words. For a test set <span class="math notranslate nohighlight">\(W = (w_{1} w_{2} \ldots w_{N})\)</span> (<span class="math notranslate nohighlight">\(N\)</span> can be millions),</p>
<div class="math notranslate nohighlight">
\[\operatorname{perplexity}(W) =\exp\left(-\frac{1}{N}\sum_{i=1}^N \ln P\left(w_{1} w_{2} \ldots w_{N}\right)\right), \]</div>
<p>where <span class="math notranslate nohighlight">\(P\left(w_{1} w_{2} \ldots w_{N}\right)\)</span> can be estimated in an <span class="math notranslate nohighlight">\(n\)</span>-gram model via</p>
<div class="math notranslate nohighlight">
\[
P\left(w_{1:n}\right) \approx \prod_{i=1}^{N} P\left(w_{i} | w_{i-n+1:n-1}\right).
\]</div>
<p>Intuitively, perplexity is roughly the \textbf{inverse probability} of the test set. Therefore, a good larnguage model will assign high probability to the test set and produce a low perplexity value. Since <span class="math notranslate nohighlight">\(\ln P(w_{1:N}) \le 0\)</span>, the perplexity is always greater than or equal to 1.0.  Since the longer the sentence, the more negative <span class="math notranslate nohighlight">\(\ln P\)</span> tends to be, the normalization by sentence length <span class="math notranslate nohighlight">\(N\)</span> reduces such impact.</p>
<div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 2 </span> (Caveats)</p>
<section class="remark-content" id="proof-content">
<p>To compare the performance of two different language models, it is necessary to **keep the vocabulary and the actual word to special token mapping the same. **</p>
<p>Suppose we construct the vocabulary by mapping words whose frequencies are below certain threshold to the <UNK> token. When we use a large threshold, we will get a smaller vocabulary.</p>
<p>In the inference stage, rare words in the test corpus will be mapped to the <UNK> token and we tend to overestimate their transition probabilities, thus causing the final perplexity to be lower.</p>
</section>
</div></section>
<section id="more-on-perplexity">
<h3>More on perplexity<a class="headerlink" href="#more-on-perplexity" title="Link to this heading">#</a></h3>
<p>A state-of-the-art language model typically can achieve perplexity value ranging from 20 to 200 on a general test set. How do we interpret the perplexity value? Perplexity can be thought of as the effective vocabulary size under the model, that is, given the context, the number of possible options for the next word.</p>
<p>Some intuition behind such interpretation of perplexity is as follows. Say we have a vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> whose size is <span class="math notranslate nohighlight">\(V\)</span>, and a trigram model simply predicts a uniform distribution given by</p>
<div class="math notranslate nohighlight">
\[
p(w \mid u, v)=\frac{1}{V}
\]</div>
<p>for all <span class="math notranslate nohighlight">\(u, v, w\)</span>. In this case, it can be shown that the perplexity is equal to <span class="math notranslate nohighlight">\(N\)</span>.</p>
<div class="math notranslate nohighlight">
\[\operatorname{perplexity} = \exp(-\frac{1}{N}\sum_{i=1}^N \ln \frac{1}{V}).\]</div>
<p>If, for example, the perplexity of the model is 120 (even though the vocabulary size is say 10,000), then this is roughly equivalent to having an effective vocabulary size of 120 for the next word given the context.</p>
<div class="proof remark admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 3 </span> (Relationship to Cross Entropy)</p>
<section class="remark-content" id="proof-content">
<p>Given a test set <span class="math notranslate nohighlight">\(W\)</span>, we can also write</p>
<div class="math notranslate nohighlight">
\[\operatorname{perplexity}(W) \approx \exp(CE(P_{true}, P)),\]</div>
<p>where <span class="math notranslate nohighlight">\(CE(P_{true}, P)\)</span> is the cross-entropy of the true distribution <span class="math notranslate nohighlight">\(P_{true}\)</span> and  our estimate <span class="math notranslate nohighlight">\(P\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[CE = -E_{W\sim P_{True}}[\ln P] =\lim_{N\to\infty} -\frac{1}{N}\sum_{w_i\in W} \ln P(w_i|w_{1:i-1}).\]</div>
<p>This means that, training a language model over well-written sentences means we are maximizing the normalized sentence probabilities given by the language model, which is more or less equivalent to minimizing the cross entropy loss of the model predicted distribution and the distribution of well-written sentences.</p>
<p>We know that cross-entorpy loss is at its minimal when the two distributions are exactly matched.</p>
</section>
</div></section>
<section id="benchmarking">
<h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h3>
<section id="datasets">
<h4>Datasets<a class="headerlink" href="#datasets" title="Link to this heading">#</a></h4>
<p>The Penn Tree Bank (PTB) <span id="id5">[<a class="reference internal" href="../chapter_prompt/advanced_prompt.html#id1460" title="Mary Ann Marcinkiewicz. Building a large annotated corpus of english: the penn treebank. Using Large Corpora, 1994.">Mar94</a>]</span> dataset is an early dataset for training and evaluating language model. The Penn Tree Bank dataset is made of articles from the Wall Street Journal, contains around <span class="math notranslate nohighlight">\(929 \mathrm{k}\)</span> training tokens, and has a vocabulary size of <span class="math notranslate nohighlight">\(10 \mathrm{k}\)</span>.</p>
<p>In the Penn Tree Bank dataset, words were lower-cased, numbers were replaced with N, newlines were replaced with <eos>, and all other punctuation was removed. The vocabulary is the most frequent 10k words with the rest of the tokens being replaced by an unk token</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>No</p></th>
<th class="head"><p>Sentence</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>pierre <unk> N years old will join the board as a nonexecutive director nov. N</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>mr. <unk> is chairman of <unk> n.v. the dutch publishing group</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>nonexecutive director of this british industrial conglomerate</p></td>
</tr>
</tbody>
</table>
</div>
<p>While the processed version of the PTB above has been frequently used for language modeling, it has many limitations. The tokens in PTB are all lower case, stripped of any punctuation, and limited to a vocabulary of only <span class="math notranslate nohighlight">\(10 \mathrm{k}\)</span> words. These limitations mean that the PTB is unrealistic for real language use, especially when far larger vocabularies with many rare words are involved.</p>
<p>Given that accurately predicting rare words, such as named entities, is an important task for many applications, the lack of a long tail for the vocabulary is problematic.</p>
<p>The wikitext 2 dataset is derived from Wikipedia articles, contains <span class="math notranslate nohighlight">\(2 \mathrm{M}\)</span> training tokens and has a vocabulary size of <span class="math notranslate nohighlight">\(33 \mathrm{k}\)</span>. These datasets contain non-shuffled documents, therefore requiring models to capture inter-sentences dependencies to perform well.</p>
<p>Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.</p>
<p>In comparison to the Mikolov processed version of the Penn Treebank (PTB), the WikiText datasets are larger. WikiText-2 aims to be of a similar size to the PTB while WikiText-103 contains all articles extracted from Wikipedia. The WikiText datasets also retain numbers (as opposed to replacing them with N), case (as opposed to all text being lowercased), and punctuation (as opposed to stripping them out).</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
PennTree</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Train</p></th>
<th class="head"><p>Valid</p></th>
<th class="head"><p>Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Articles</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>Number of tokens</p></td>
<td><p>887,521</p></td>
<td><p>70,390</p></td>
<td><p>78,669</p></td>
</tr>
<tr class="row-even"><td><p>Vocabulary size</p></td>
<td><p>10,000</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>OOV ratio</p></td>
<td><p>4.8%</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
WikiText-2,</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Train</p></th>
<th class="head"><p>Valid</p></th>
<th class="head"><p>Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Articles</p></td>
<td><p>600</p></td>
<td><p>60</p></td>
<td><p>60</p></td>
</tr>
<tr class="row-odd"><td><p>Number of tokens</p></td>
<td><p>2,088,628</p></td>
<td><p>217,646</p></td>
<td><p>245,569</p></td>
</tr>
<tr class="row-even"><td><p>Vocabulary size</p></td>
<td><p>33,278</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>OOV ratio</p></td>
<td><p>2.6%</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
WikiText-103</label><div class="sd-tab-content docutils">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Train</p></th>
<th class="head"><p>Valid</p></th>
<th class="head"><p>Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Articles</p></td>
<td><p>28,475</p></td>
<td><p>60</p></td>
<td><p>60</p></td>
</tr>
<tr class="row-odd"><td><p>Number of tokens</p></td>
<td><p>103,227,021</p></td>
<td><p>217,646</p></td>
<td><p>245,569</p></td>
</tr>
<tr class="row-even"><td><p>Vocabulary size</p></td>
<td><p>267,735</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>OOV ratio</p></td>
<td><p>0.4%</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="neural-language-models">
<h2>Neural language models<a class="headerlink" href="#neural-language-models" title="Link to this heading">#</a></h2>
</section>
<section id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id859" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BGJM17<span class="fn-bracket">]</span></span>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. <em>Transactions of the Association for Computational Linguistics</em>, 5:135–146, 2017.</p>
</div>
<div class="citation" id="id1496" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BNB21<span class="fn-bracket">]</span></span>
<p>Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2109.12948">https://arxiv.org/abs/2109.12948</a>, <a class="reference external" href="https://arxiv.org/abs/2109.12948">arXiv:2109.12948</a>.</p>
</div>
<div class="citation" id="id385" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">BMR+20</a><span class="fn-bracket">]</span></span>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, 33:1877–1901, 2020.</p>
</div>
<div class="citation" id="id1497" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DLBZ22<span class="fn-bracket">]</span></span>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2208.07339">https://arxiv.org/abs/2208.07339</a>, <a class="reference external" href="https://arxiv.org/abs/2208.07339">arXiv:2208.07339</a>.</p>
</div>
<div class="citation" id="id1255" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JGBM16<span class="fn-bracket">]</span></span>
<p>Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. <em>arXiv preprint arXiv:1607.01759</em>, 2016.</p>
</div>
<div class="citation" id="id893" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">MSchutze99</a><span class="fn-bracket">]</span></span>
<p>Christopher D Manning and Hinrich Schütze. <em>Foundations of statistical natural language processing</em>. MIT press, 1999.</p>
</div>
<div class="citation" id="id1463" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Mar94</a><span class="fn-bracket">]</span></span>
<p>Mary Ann Marcinkiewicz. Building a large annotated corpus of english: the penn treebank. <em>Using Large Corpora</em>, 1994.</p>
</div>
<div class="citation" id="id1089" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MCCD13<span class="fn-bracket">]</span></span>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. <em>arXiv preprint arXiv:1301.3781</em>, 2013.</p>
</div>
<div class="citation" id="id1175" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MKB+10<span class="fn-bracket">]</span></span>
<p>Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent neural network based language model. In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, <em>INTERSPEECH</em>, 1045–1048. ISCA, 2010. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10">http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10</a>.</p>
</div>
<div class="citation" id="id1088" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MSC+13<span class="fn-bracket">]</span></span>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In <em>Advances in neural information processing systems</em>, 3111–3119. 2013.</p>
</div>
<div class="citation" id="id495" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NLZ+23<span class="fn-bracket">]</span></span>
<p>Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, and others. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. <em>arXiv preprint arXiv:2311.16452</em>, 2023.</p>
</div>
<div class="citation" id="id1130" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">RNSS18</a><span class="fn-bracket">]</span></span>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. <em>URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf</em>, 2018.</p>
</div>
<div class="citation" id="id1131" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RWC+19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. <em>OpenAI Blog</em>, 1(8):9, 2019.</p>
</div>
<div class="citation" id="id496" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WWS+22<span class="fn-bracket">]</span></span>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. <em>arXiv preprint arXiv:2203.11171</em>, 2022.</p>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="myref1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">1</a><span class="fn-bracket">]</span></span>
<p>When <span class="math notranslate nohighlight">\(\alpha=1\)</span>, this is called \textbf{Laplace smoothing}; when <span class="math notranslate nohighlight">\(\alpha=0.5\)</span>, this is called Jeffreys-Perks law.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_foundation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction: Unveiling the Power of Language Models in the Age of Artificial Intelligence</p>
      </div>
    </a>
    <a class="right-next"
       href="neural_language_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Early Neural Language Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-language-models">Statistical language models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-gram-language-model"><span class="math notranslate nohighlight">\(n\)</span>-gram language model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameter-estimation">Model parameter estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#star-deriving-the-mle"><span class="math notranslate nohighlight">\(\star\)</span> Deriving the MLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choices-of-n-and-bias-variance-trade-off">Choices of <span class="math notranslate nohighlight">\(n\)</span> and bias-variance trade-off</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#out-of-vocabulary-oov-words-and-rare-words">Out of vocabulary (OOV) words and rare words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-and-discounting-techniques">Smoothing and discounting techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-alpha-smoothing-and-discounting">Add <span class="math notranslate nohighlight">\(\alpha\)</span> smoothing and discounting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#katz-s-back-off">Katz’s back-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">Evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-perplexity">More on perplexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">Datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-language-models">Neural language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>