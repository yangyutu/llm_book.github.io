
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. GPT Series &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_foundation/GPT_series';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="9. LLM Architectures Fundamentals" href="../chapter_LLM_arch/LLM_dense_architectures.html" />
    <link rel="prev" title="7. Seq2Seq: T5 and BART" href="t5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_arch/annotated_llama.html">10. *Annotated LLama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">11. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">12. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">13. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">14. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">15. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">16. LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">17. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">18. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">19. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">20. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">21. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">22. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">23. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>GPT Series</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1">8.1. GPT-1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">8.1.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">8.1.2. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1-fine-tuning">8.1.3. GPT-1 Fine Tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2">8.2. GPT-2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">8.3. GPT-3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">8.3.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-overview">8.3.2. Performance Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#language-modeling">8.3.2.1. Language modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#news-article-generation">8.3.2.2. News article generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-translation">8.3.2.3. Machine Translation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#superglue">8.3.2.4. SuperGLUE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#closed-book-question-answering">8.3.2.5. Closed-book question answering</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-sense-reasoning">8.3.2.6. Common Sense Reasoning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-tasks">8.3.2.7. Arithmetic tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">8.4. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gpt-series">
<h1><span class="section-number">8. </span>GPT Series<a class="headerlink" href="#gpt-series" title="Link to this heading">#</a></h1>
<section id="gpt-1">
<h2><span class="section-number">8.1. </span>GPT-1<a class="headerlink" href="#gpt-1" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3><span class="section-number">8.1.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<p>GPT-1 <span id="id1">[<a class="reference internal" href="transformers.html#id1138" title="Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.">RNSS18</a>]</span> and its successors (GPT-2 and GPT-3) are a series Transformer-based model architectures aim to offer generic task-agnostic model architecture and learning schemes to diverse natural language processing (NLP) tasks, such as textual entailment, question answering, semantic similarity assessment.</p>
<p>At that time, specific NLP tasks requires the collection of a massive amount of task-specific labeled data as well as the design of task-specific architectures that learns best from it. Given the broad range of NLP tasks, this paradigm does not scale well and model learning cannot be shared across different tasks.</p>
<p>One of the major contributions of the GPT-1 study is the introduction of a two-stage paradigm to NLP tasks, including an unsupervised pretraining stage and a supervised fine-tuning stage. They demonstrates that a pre-trained model with a small scale fine-tuning can achieve satisfactory results over a range of diverse tasks, not just for a single task.</p>
<p>GPT-1 model consists of multiple transformer decoder layers [<a class="reference internal" href="transformers.html#content-chapter-foundation-transformers-transformers"><span class="std std-ref">Transformers Anatomy</span></a>]. The pretraining task is auto-regression language modeling, that it, predicting the next word given the preceding word sequence. In the Transformer architecture, the activation in the final transformer block is fed into a Softmax function that produces the word probability distributions over an entire vocabulary of words to predict the next word.</p>
<p>In the stage unsupervised pretraining from unlabeled data, the goal is to learn a universal language representation that can be easily adapted to a wide range of tasks. Following the pretraining, the model can be easily fine-tuned to a downstream task by a relatively small amount of task-specific data to achieve effective transfer. This two-stage scheme had a profound impact on the subsequent deep model learning and drew significant interest to pretrained large language models.</p>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-decoder-architecture">
<a class="reference internal image-reference" href="../../_images/GPT_decoder_arch.png"><img alt="../../_images/GPT_decoder_arch.png" src="../../_images/GPT_decoder_arch.png" style="width: 316.0px; height: 637.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">GPT uses the decoder component in the Transformer for language modeling.</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-decoder-architecture" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="pretraining">
<h3><span class="section-number">8.1.2. </span>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">#</a></h3>
<p>The pretraining task of GPT-1 is auto-regressive language modeling, which predict the next words given preceding word sequence. Given an input sequence <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,...,x_T)\)</span>, auto-regressive language modeling maximizes the log likelihood given by</p>
<div class="math notranslate nohighlight">
\[\sum_{t=1}^{T} \log p\left(x_{t} \mid \mathbf{x}_{t-k-1:t-1},\theta\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(p\left(x_{t} \mid \mathbf{x}_{t-k-1:t-1}\right)\)</span> is the predicted probability distribution for token <span class="math notranslate nohighlight">\(x_t\)</span> given preceding token sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{t-k-1:t-1}\)</span> with a context window size <span class="math notranslate nohighlight">\(k\)</span> (<span class="math notranslate nohighlight">\(k\)</span> can range from hundreds to tens of thousands, depending on the model configuration).</p>
<p>The input tokens are first converted to input embeddings by summing up token embedding and position embedding. The input embedding <span class="math notranslate nohighlight">\(H_0\in \mathbb{R}^{T\times d_{model}}\)</span> is then fed into <span class="math notranslate nohighlight">\(L\)</span> Transformer layers to obtain contextualized embedding representation <span class="math notranslate nohighlight">\(H_L\)</span>. The contextualized embedding is then passed through a linear layer and a Softmax layer to produce an output distribution over target tokens:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
	H_{0} &amp;=W_{e}+W_{p} \\
	H_{l} &amp;=\operatorname{TransformerLayer}\left(H_{l-1}\right) \forall \ell \in[1, L] \\
	P(u) &amp;=\operatorname{Softmax}\left(H_{L} W_{e}^{T}\right).
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(W_{e}\)</span> is the token embedding matrix, and <span class="math notranslate nohighlight">\(W_{p}\)</span> is the position embedding matrix.</p>
<p>GPT-1 uses the BooksCorpus dataset for pretraining. BooksCorpus is a large collection of free novel books (11,038 books), containing around 74M sentences and 1G words in 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.). Pretrained GPT-1 can achieve a very low token level perplexity of 18.4 on this corpus.</p>
</section>
<section id="gpt-1-fine-tuning">
<h3><span class="section-number">8.1.3. </span>GPT-1 Fine Tuning<a class="headerlink" href="#gpt-1-fine-tuning" title="Link to this heading">#</a></h3>
<p>To pretrained GPT model can be adopted for different downstream tasks by modifying the inputs format or adding minimal component accordingly.  a task-specific format and then adding minimal component to process the output to get task-specific predictions. As summarized in <a class="reference internal" href="#chapter-foundation-fig-gpt-gpt-architecture"><span class="std std-numref">Fig. 8.2</span></a>,</p>
<ul class="simple">
<li><p>For <strong>sequence tasks</strong> such as text classification, the input is passed through the network as-is, and the output linear layer takes the last activation to make a classification decision.</p></li>
<li><p>For <strong>sentence-pair tasks</strong> such as textual entailment, the input that is made up of two sequences is marked with a delimiter, which helps the pre-trained model to know which part is premise or hypothesis in the case of textual entailment. Finally, the output linear layer takes the last activation to make a classification decision.</p></li>
<li><p>For <strong>sentence vector similarity tasks</strong>, we use the model to encode the two differently-ordered sentence pairs separately into two sequence representations, which are added element-wise before being fed into the linear output layer.</p></li>
<li><p>For tasks like <strong>Question Answering</strong> and <strong>Commonsense Reasoning</strong>, in which we are given a context document <span class="math notranslate nohighlight">\(z\)</span>, a question <span class="math notranslate nohighlight">\(q\)</span>, and a set of possible answers <span class="math notranslate nohighlight">\(\left\{a_{k}\right\}\)</span>, we can concatenate the document context and question with each possible answer. Each of these sequences are processed independently with our model and then normalized via a Softmax layer to produce an output distribution over possible answers.</p></li>
</ul>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-architecture">
<a class="reference internal image-reference" href="../../_images/GPT_arch.png"><img alt="../../_images/GPT_arch.png" src="../../_images/GPT_arch.png" style="width: 797.6px; height: 338.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">(left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into token sequences to be processed by our pre-trained model, followed by a linear+softmax layer. Image from <span id="id2">[<a class="reference internal" href="transformers.html#id1138" title="Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.">RNSS18</a>]</span>.</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-architecture" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The fine-tuning process involves continuing the model training over a labeled dataset <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. Take text classification task as an example. Suppose that each labeled example consists of a sequence of input tokens, <span class="math notranslate nohighlight">\(x^{1}, \ldots, x^{m}\)</span> along with a label <span class="math notranslate nohighlight">\(y\)</span>. The input sequence is first encoded by the pre-trained model to an embedding vector <span class="math notranslate nohighlight">\(h_{l}^{m}\)</span> at the position of the last input token. <span class="math notranslate nohighlight">\(h_{l}^{m}\)</span> is then fed into an linear layer with Softmax to obtain distribution over class labels. The training loss can simply be the binary cross entropy (BCE) loss. It is also found that including language modeling as an auxiliary task during fine-tuning can improve generalization of the fine-tuned model and speed up convergence.</p>
</section>
</section>
<section id="gpt-2">
<h2><span class="section-number">8.2. </span>GPT-2<a class="headerlink" href="#gpt-2" title="Link to this heading">#</a></h2>
<p>GPT-2 <span id="id3">[<a class="reference internal" href="transformers.html#id1139" title="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.">RWC+19</a>]</span>, a successor to the original GPT-1, is a larger model trained on much more training data, called WebText, than the original one. It achieved state-of-the-art results on seven out of the eight tasks in a zero-shot setting in which there is no fine-tuning applied but had limited success in some tasks.
The key contribution of GPT-2 is not about further refining the two-stage pretraining-fine-tuning paradigm in GPT-1, but about investigating the capability of zero-shot learning with extensively pretrained language model alone. In other words, it aims to answer whether language modeling is a universal task that can help the model to gain universal knowledge that can accomplish other language tasks without subsequent supervised learning.</p>
<p>The intuition is that a model can be very skilled in the sense that it can learn much of the information about a language during the pre-training phase, there will be no need to learn extra information through fine-tuning phase. Take machine translation in the following box as an example, which contains examples of naturally occurring demonstrations of English to French and French to English translation found throughout the WebText training set. By learning to predict future words in the language modeling task, we expect the model to automatically acquire the ability to translate when we can provide the right prompt (e.g., <em>translate from English to French</em>) to the model.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 8.1 </span> (Translation pairs can appear naturally in pretraining text corpus)</p>
<section class="example-content" id="proof-content">
<p><em>I’m not the cleverest man in the world</em>, but like they say in French: <em>Je ne suis pas un imbecile</em> [I’m not a fool].</p>
<p>In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: <em>Mentez mentez, il en restera toujours quelque chose</em>, which translates as, <em>Lie lie and something will always remain</em>.</p>
<p>I hate the word <em>perfume</em>, Burr says. It’s somewhat better in French: <em>parfum.</em></p>
<p>If listened carefully at 29:55, a conversation can be heard between two guys in French: <em>Comment on fait pour aller de l’autre coté? -Quel autre coté?</em>, which means <em>How do you get to the other side? - What side?</em>.</p>
<p>If this sounds like a bit of a stretch, consider this question in French: <em>As-tu aller au cinéma?</em>, or <em>Did you go to the movies?</em>, which literally translates as Have-you to go to movies/theater?</p>
<p><em>Brevet Sans Garantie Du Gouvernement</em>, translated to English: <em>Patented without government warranty</em>.</p>
</section>
</div><p>To probe the learned knowledge in the pretraining stage, we specify the task itself through language. That is, a translation task can be specified via <em>translate to french: English text</em>. If corresponding translated French text is produced, it suggests that the pretrained model has acquired the machine translation ability. Likewise, a reading comprehension task can be specified as <em>answer the question: document</em>.</p>
<p>One critical difference between GPT-2 and traditional NLP models that is the task in GPT-2 is formulated within the input, and the model is expected to understand the nature of downstream tasks and provide answers accordingly, while in traditional NLP models we engineer task-specific special symbols and components such that we convert the model’s output to what we need (e.g., a probability). In GPT-2, learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution <span class="math notranslate nohighlight">\(p\)</span> (output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model <span class="math notranslate nohighlight">\(p\)</span> (output|input, task). Here we can view that input and task specification are provided via language itself rather than by changing part of the model parameters or architectures. This is the key to unlock the task-agnostic language understanding ability for large-scaled pretrained language models.</p>
<p>GPT-2 uses the same Transformer decoder architecture like GPT-1, except that the model size is  has been expanded by more than 10 times from GPT-1. The training corpus used by GPT-1is the BookCorpus dataset, while the training corpus used by GPT-2 is crawled from more than 8 million web pages monolingual data, the amount of data is more than 10 times that of the GPT-1.</p>
</section>
<section id="gpt-3">
<h2><span class="section-number">8.3. </span>GPT-3<a class="headerlink" href="#gpt-3" title="Link to this heading">#</a></h2>
<section id="id4">
<h3><span class="section-number">8.3.1. </span>Introduction<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>GPT-3 model <span id="id5">[<a class="reference internal" href="transformers.html#id393" title="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.">BMR+20</a>]</span> has 175 billion parameters, which is 100 times bigger than GPT-2. The architecture of GPT-2 and GPT-3 is similar, with the main differences usually being in the model size and the dataset quantity/quality. As a comparison, GPT-3 has 96 decoder layers with 96 multi-heads attentions and <span class="math notranslate nohighlight">\(d_model\)</span> of 12,288. In comparison, GPT-1 only has 12 layers, 12 heads, and <span class="math notranslate nohighlight">\(d_model\)</span> given by 768. The training data is further expanded from what is used in GPT-2.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head text-right"><p>Quantity (tokens)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Common Crawl (filtered)</p></td>
<td class="text-right"><p>410 billion</p></td>
</tr>
<tr class="row-odd"><td><p>WebText2</p></td>
<td class="text-right"><p>19 billion</p></td>
</tr>
<tr class="row-even"><td><p>Books1</p></td>
<td class="text-right"><p>12 billion</p></td>
</tr>
<tr class="row-odd"><td><p>Books2</p></td>
<td class="text-right"><p>55 billion</p></td>
</tr>
<tr class="row-even"><td><p>Wikipedia</p></td>
<td class="text-right"><p>3 billion</p></td>
</tr>
</tbody>
</table>
</div>
<p>The major motivation of GPT-3 is to examine the few-shot learning ability for pretrained language model. This is inspired by the fact that humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do.
By training on the massive amount of data using a model with large number of parameters, GPT-3 achieved better results on many downstream tasks in zero-shot, one-shot, and few-shot <span class="math notranslate nohighlight">\((K=32)\)</span> settings without any gradient-based fine-tuning.</p>
<p>What is most special about GPT-3 is the ability to perform in-context <strong>few-shot learning</strong> without any model parameter updates via gradient descent [<a class="reference internal" href="#chapter-foundation-fig-gpt-gpt3-few-shot-learning-demo"><span class="std std-numref">Fig. 8.3</span></a>]. In a typical few-shot learning setting, the model is given a natura language description of the task plus a few demonstration examples (e.g., input and output pairs) of the task at inference time and the model is asked to generate an output given a new input. Here the input is called <strong>context</strong> and the output is called a <strong>completion</strong>. Take English sentence to French translation as an example, <span class="math notranslate nohighlight">\(K\)</span> examples of context and completion are presented with one final example of context, and the model is expected to provide the completion. Typically, <span class="math notranslate nohighlight">\(K\)</span> is in the range of 10 and 100. Clearly, few-shot learning is close to how human intelligence works and bring great metrics in reducing task-specific data.</p>
<p>In the extreme end of few shot learning, one-shot learning is the case in which only one demonstration is presented. Further, zero-shot is the case where no demonstrations are given except for a natural language instruction describing the task. Zero-shot setting offers the ultimate test of the model’s learning capacity, but it can also be unfairly hard due to ambiguity.</p>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt3-few-shot-learning-demo">
<a class="reference internal image-reference" href="../../_images/GPT3_few_shot_learning_demo.png"><img alt="../../_images/GPT3_few_shot_learning_demo.png" src="../../_images/GPT3_few_shot_learning_demo.png" style="width: 793.6px; height: 676.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. Traditional fine-tuning requires gradient computation via backpropgation and update the model weight, whereas zero-, one-,
and few-shot, only require the model to perform forward passes at test time.</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt3-few-shot-learning-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="performance-overview">
<h3><span class="section-number">8.3.2. </span>Performance Overview<a class="headerlink" href="#performance-overview" title="Link to this heading">#</a></h3>
<p>In the following, we summarize the probing results of GPT-3 in a broad range of domains.</p>
<section id="language-modeling">
<h4><span class="section-number">8.3.2.1. </span>Language modeling<a class="headerlink" href="#language-modeling" title="Link to this heading">#</a></h4>
<p>We first look at the  language modeling benchmark on Penn Tree Bank, which is a fundamental measure on the model’s capability on understanding and using natural language. GPT-3 is a clear leader in Language Modelling on  with a perplexity of 20.5.</p>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-language-modeling-benchmark">
<a class="reference internal image-reference" href="../../_images/language_modeling_benchmark.png"><img alt="../../_images/language_modeling_benchmark.png" src="../../_images/language_modeling_benchmark.png" style="width: 651.2px; height: 352.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Language modeling benchmark task on Penn Tree Bank. Image from <a class="reference external" href="https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word">https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word</a>.</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-language-modeling-benchmark" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="news-article-generation">
<h4><span class="section-number">8.3.2.2. </span>News article generation<a class="headerlink" href="#news-article-generation" title="Link to this heading">#</a></h4>
<p>Like language modeling, the most natural ability for GPT3 is to generate smooth and nature texts.</p>
<p>Performance is evaluated by how well humans can detect whether generated text is from a model model generated text. Regarding the test,  25 article titles and subtitles from the website are arbitrarily selected, with a mean length of 215 words. Four language models ranging in size from 125M to 175B (GPT-3) are used to generated completions from these titles and subtitles.</p>
<p>Following shows the performance across different model sizes. As the model size increases, the generated texts are more realistic, natural, and coherent, therefore more difficult for humans to detect.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1526">
<caption><span class="caption-number">Table 8.1 </span><span class="caption-text">Human accuracy in identifying whether short (around 200 word) news articles are model generated.</span><a class="headerlink" href="#id1526" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Mean accuracy</p></th>
<th class="head text-center"><p>95% Confidence Interval (low, hi)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Control (deliberately bad model)</p></td>
<td class="text-center"><p>86%</p></td>
<td class="text-center"><p>83%-90%</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 Small</p></td>
<td class="text-center"><p>76%</p></td>
<td class="text-center"><p>72%-80%</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3 2.7B</p></td>
<td class="text-center"><p>62%</p></td>
<td class="text-center"><p>58%-65%</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 6.7B</p></td>
<td class="text-center"><p>60%</p></td>
<td class="text-center"><p>56%-63%</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3 13B</p></td>
<td class="text-center"><p>55%</p></td>
<td class="text-center"><p>52%-58%</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 175B</p></td>
<td class="text-center"><p>52%</p></td>
<td class="text-center"><p>49%-54%</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="machine-translation">
<h4><span class="section-number">8.3.2.3. </span>Machine Translation<a class="headerlink" href="#machine-translation" title="Link to this heading">#</a></h4>
<p>GPT-3’s training data consists of primarily English (93% by word count), with an additional 7% of text in other languages. We expect GPT-3 can learns from a blend of training data that mixes many languages together in a natural way, therefor enabling GPT-3 to perform machine translation in zero-shot and few-shot settings.</p>
<p>The following shows the performance comparison among  supervised SOTA neural machine translation models, unsupervised multi-lingual pretrained language models, and GPT-3. Supervised models are the clear winners in this domain. However, GPT-3 demonstrates its decent performance when performing translation back to English, probably because GPT-3 is a strong English language model.</p>
<p>The performance GPT-3 also has a noticeable skew depending on language direction. Specifically, GPT-3 significantly outperforms prior unsupervised models when translating into English but under-performs when translating in the other direction.</p>
<p>In general, across all three language models tested, there is a smooth upward trend with model capacity. While in the zero-shot setting, GPT-3 underperforms recent unsupervised model, offering a few demonstrations to GPT-3 can quickly boost the BLEU scores.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1527">
<caption><span class="caption-number">Table 8.2 </span><span class="caption-text">Performance comparison among  supervised SOTA neural machine translation models, unsupervised multi-lingual pretrained language models, and GPT-3.</span><a class="headerlink" href="#id1527" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Setting</p></th>
<th class="head text-center"><p>En → Fr</p></th>
<th class="head text-center"><p>Fr → En</p></th>
<th class="head text-center"><p>En → De</p></th>
<th class="head text-center"><p>De → En</p></th>
<th class="head text-center"><p>En → Ro</p></th>
<th class="head text-center"><p>Ro → En</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SOTA (Supervised)</p></td>
<td class="text-center"><p><strong>45.6</strong></p></td>
<td class="text-center"><p>35.0</p></td>
<td class="text-center"><p><strong>41.2</strong></p></td>
<td class="text-center"><p>40.2</p></td>
<td class="text-center"><p><strong>38.5</strong></p></td>
<td class="text-center"><p><strong>39.9</strong>^e^</p></td>
</tr>
<tr class="row-odd"><td><p>XLM</p></td>
<td class="text-center"><p>33.4</p></td>
<td class="text-center"><p>33.3</p></td>
<td class="text-center"><p>26.4</p></td>
<td class="text-center"><p>34.3</p></td>
<td class="text-center"><p>33.3</p></td>
<td class="text-center"><p>31.8</p></td>
</tr>
<tr class="row-even"><td><p>MASS</p></td>
<td class="text-center"><p>37.5</p></td>
<td class="text-center"><p>34.9</p></td>
<td class="text-center"><p>28.3</p></td>
<td class="text-center"><p>35.2</p></td>
<td class="text-center"><p><u>35.2</u></p></td>
<td class="text-center"><p>33.1</p></td>
</tr>
<tr class="row-odd"><td><p>mBART</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>29.8</p></td>
<td class="text-center"><p>34.0</p></td>
<td class="text-center"><p>35.0</p></td>
<td class="text-center"><p>30.5</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3 Zero-Shot</p></td>
<td class="text-center"><p>25.2</p></td>
<td class="text-center"><p>21.2</p></td>
<td class="text-center"><p>24.6</p></td>
<td class="text-center"><p>27.2</p></td>
<td class="text-center"><p>14.1</p></td>
<td class="text-center"><p>19.9</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 One-Shot</p></td>
<td class="text-center"><p>28.3</p></td>
<td class="text-center"><p>33.7</p></td>
<td class="text-center"><p>26.2</p></td>
<td class="text-center"><p>30.4</p></td>
<td class="text-center"><p>20.6</p></td>
<td class="text-center"><p>38.6</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3 Few-Shot</p></td>
<td class="text-center"><p>32.6</p></td>
<td class="text-center"><p><strong>39.2</strong></p></td>
<td class="text-center"><p>29.7</p></td>
<td class="text-center"><p><strong>40.6</strong></p></td>
<td class="text-center"><p>21.0</p></td>
<td class="text-center"><p>39.5</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="superglue">
<h4><span class="section-number">8.3.2.4. </span>SuperGLUE<a class="headerlink" href="#superglue" title="Link to this heading">#</a></h4>
<p>SuperGLUE is a collection of benchmark tests to probe the natural language understanding capacity of a model.</p>
<p>The key take aways from the following SuperGLUE benchmark are:</p>
<ul class="simple">
<li><p><strong>Scaling</strong>: The performance of GPT-3 improves as the model size increases, for both zero shot and few shots.  At its largest size (175B), few-shot GPT-3 outperforms fine-tuned BERT++ and BERT Large models, approaching but not quite reaching the performance of fine-tuned SOTA (State of the Art) models.</p></li>
<li><p><strong>Few-shot in-context learning</strong>: GPT-3 performs well in few-shot settings (K=32), outperforming one-shot and zero-shot learning for most model sizes. In general, more examples provided, the better the performance, plateauing around 16-32 examples.</p></li>
<li><p><strong>Human benchmark</strong>: While GPT-3’s performance is impressive, it still falls short of human-level performance on SuperGLUE</p></li>
<li><p><strong>Slow improvement at large sizes</strong>: steep improvements in performance with initial increases in model size or number of examples, followed by a more gradual improvement as these numbers get larger.</p></li>
</ul>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-language-modeling-supergluebenchmark">
<a class="reference internal image-reference" href="../../_images/SuperGLUE_benchmark_GPT3.png"><img alt="../../_images/SuperGLUE_benchmark_GPT3.png" src="../../_images/SuperGLUE_benchmark_GPT3.png" style="width: 789.1px; height: 375.05px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">Performance on SuperGLUE increases with model size and number of examples in context.</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-language-modeling-supergluebenchmark" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="closed-book-question-answering">
<h4><span class="section-number">8.3.2.5. </span>Closed-book question answering<a class="headerlink" href="#closed-book-question-answering" title="Link to this heading">#</a></h4>
<p>Closed-book question answering is used to examine GPT-3’s ability to answer questions about broad factual knowledge. Contrast with open-book question answering, in which 1) an information retrieval module is first used to retrieve question-relevant paragraphs and then 2) the model is perform reading comprehension on the retrieved text to extract or produce the answers.</p>
<p>The GPT-3 model was tested on Natural Questions, WebQuestions, and TriviaQA datasets, and the results are the following. The performance of GPT-3 improves as the model size increases, for both zero shot and few shots.  At its largest size (175B), few-shot GPT-3 outperforms the fine-tuned SOTA.</p>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-language-modeling-trialbenchmark">
<a class="reference internal image-reference" href="../../_images/TrivialQA_benchmark_GPT3.png"><img alt="../../_images/TrivialQA_benchmark_GPT3.png" src="../../_images/TrivialQA_benchmark_GPT3.png" style="width: 636.0px; height: 423.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models
continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains
over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-language-modeling-trialbenchmark" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="common-sense-reasoning">
<h4><span class="section-number">8.3.2.6. </span>Common Sense Reasoning<a class="headerlink" href="#common-sense-reasoning" title="Link to this heading">#</a></h4>
<p>GPT-3 was tested on three datasets which attempt to capture physical or scientific reasoning.</p>
<p>As for physical or scientific reasoning, GPT-3 is not outperforming fine-tuned SOTA methods; however, there is a clear trend that more examples in the prompt will help.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Setting</p></th>
<th class="head text-center"><p>PIQA</p></th>
<th class="head text-center"><p>ARC (Easy)</p></th>
<th class="head text-center"><p>ARC (Challenge)</p></th>
<th class="head text-center"><p>OpenBookQA</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Fine-tuned SOTA</p></td>
<td class="text-center"><p>79.4</p></td>
<td class="text-center"><p><strong>92.0</strong></p></td>
<td class="text-center"><p><strong>78.5</strong></p></td>
<td class="text-center"><p><strong>87.2</strong></p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 Zero-Shot</p></td>
<td class="text-center"><p><strong>80.5</strong></p></td>
<td class="text-center"><p>68.8</p></td>
<td class="text-center"><p>51.4</p></td>
<td class="text-center"><p>57.6</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3 One-Shot</p></td>
<td class="text-center"><p><strong>80.5</strong></p></td>
<td class="text-center"><p>71.2</p></td>
<td class="text-center"><p>53.2</p></td>
<td class="text-center"><p>58.8</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3 Few-Shot</p></td>
<td class="text-center"><p><strong>82.8</strong></p></td>
<td class="text-center"><p>70.1</p></td>
<td class="text-center"><p>51.5</p></td>
<td class="text-center"><p>65.4</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="arithmetic-tasks">
<h4><span class="section-number">8.3.2.7. </span>Arithmetic tasks<a class="headerlink" href="#arithmetic-tasks" title="Link to this heading">#</a></h4>
<p>Arithematic tasks fall into ene category of reasoning tasks and they are intrinsically challenging to NLP model up to today. As shown by the following the results, while GPT-3 does not excel at arithmetic tasks, we observe a rapid increase of skill starting from 7B model size. This is indicating that arithmetic reasoning is one emergent ability for LLMs.</p>
<figure class="align-default" id="chapter-foundation-fig-gpt-gpt-language-modeling-arithemetic-task-gpt3">
<a class="reference internal image-reference" href="../../_images/arithemetic_task_GPT3.png"><img alt="../../_images/arithemetic_task_GPT3.png" src="../../_images/arithemetic_task_GPT3.png" style="width: 746.9px; height: 489.29999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a
significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175)</span><a class="headerlink" href="#chapter-foundation-fig-gpt-gpt-language-modeling-arithemetic-task-gpt3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- ### Limitations

First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct
predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although
the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to
lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences
or paragraphs.

GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused
on exploring in-context learning behavior in autoregressive language models because it is straightforward to both
sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional
architectures or other training objectives such as denoising. 

Thus our design decision comes at the cost of potentially worse performance on tasks
which empirically benefit from bidirectionality

This could be a possible explanation for GPT-3’s lagging few-shot performance on a
few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves
comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and
RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning
than GPT-3.


Another limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3
takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more
text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efficiency is
an important direction for future work, and might come from grounding in the physical world to provide additional
information, or from algorithmic improvements.

## Applications

### Machine Translation

Machine translation is typically implemented via encoder-decoder paradigm, in which encoder takes input in the original language and the decoder outputs words in the target language. -->
</section>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">8.4. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id385" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">BMR+20</a><span class="fn-bracket">]</span></span>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, 33:1877–1901, 2020.</p>
</div>
<div class="citation" id="id1130" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RNSS18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>)</span>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. <em>URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf</em>, 2018.</p>
</div>
<div class="citation" id="id1131" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">RWC+19</a><span class="fn-bracket">]</span></span>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. <em>OpenAI Blog</em>, 1(8):9, 2019.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_foundation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="t5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Seq2Seq: T5 and BART</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_LLM_arch/LLM_dense_architectures.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>LLM Architectures Fundamentals</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1">8.1. GPT-1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">8.1.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">8.1.2. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1-fine-tuning">8.1.3. GPT-1 Fine Tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2">8.2. GPT-2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">8.3. GPT-3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">8.3.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-overview">8.3.2. Performance Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#language-modeling">8.3.2.1. Language modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#news-article-generation">8.3.2.2. News article generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-translation">8.3.2.3. Machine Translation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#superglue">8.3.2.4. SuperGLUE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#closed-book-question-answering">8.3.2.5. Closed-book question answering</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-sense-reasoning">8.3.2.6. Common Sense Reasoning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-tasks">8.3.2.7. Arithmetic tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">8.4. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>