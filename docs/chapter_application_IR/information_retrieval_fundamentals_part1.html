
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>23. Information Retrieval and Sparse Retrieval &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_application_IR/information_retrieval_fundamentals_part1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="24. Information Retrieval and Dense Models" href="information_retrieval_fundamentals_part2.html" />
    <link rel="prev" title="22. Advanced Prompting Techniques" href="../chapter_prompt/advanced_prompt.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architecture Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reasoning.html">14. LLM Reasoning (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/llama_series.html">17. Llama Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/deepseek_series.html">18. DeepSeek Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">23. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="information_retrieval_fundamentals_part2.html">24. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="application_LLM_in_IR.html">25. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">26. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">27. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Information Retrieval and Sparse Retrieval</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-information-retrieval">23.1. Overview of Information Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ad-hoc-retrieval">23.1.1. Ad-hoc Retrieval</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-domain-question-answering">23.1.2. Open-domain Question Answering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-ir-systems">23.1.3. Modern IR Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-opportunities-in-ir-systems">23.1.4. Challenges And Opportunities In IR Systems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-understanding-and-rewriting">23.1.4.1. Query Understanding And Rewriting</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-match-and-semantic-match">23.1.4.2. Exact Match And Semantic Match</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#robustness-to-document-variations">23.1.4.3. Robustness To Document Variations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency">23.1.4.4. Computational Efficiency</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-ranking-evaluation-metrics">23.2. Text Ranking Evaluation Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">23.2.1. Precision And Recall</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-discounted-cumulative-gain-ndcg">23.2.2. Normalized Discounted Cumulative Gain (NDCG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-metrics">23.2.3. Online Metrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-sparse-ir-fundamentals">23.3. Traditional Sparse IR Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-match-framework">23.3.1. Exact Match Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-vector-space-model">23.3.2. TF-IDF Vector Space Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25">23.3.3. BM25</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25-efficient-implementation">23.3.4. BM25 Efficient Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25f">23.3.5. BM25F</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#query-and-document-expansion">23.4. Query and Document Expansion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">23.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-expansion-via-query-prediction">23.4.2. Document Expansion via Query Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextualized-term-importance">23.5. Contextualized Term Importance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-aware-term-importance-deep-ct">23.5.1. Context-aware Term Importance: Deep-CT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learnable-context-aware-term-importance-deep-impact">23.5.1.1. Learnable Context-aware Term Importance: Deep-Impact</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tw-bert">23.5.2. TW-BERT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-retrieval-demonstrations">23.6. Sparse Retrieval Demonstrations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25-demonstration">23.6.1. BM25 Demonstration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-on-bibliography-and-software">23.7. Note on Bibliography and Software</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">23.7.1. Bibliography</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">23.7.2. Software</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="information-retrieval-and-sparse-retrieval">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch"></span><h1><span class="section-number">23. </span>Information Retrieval and Sparse Retrieval<a class="headerlink" href="#information-retrieval-and-sparse-retrieval" title="Link to this heading">#</a></h1>
<section id="overview-of-information-retrieval">
<span id="index-0"></span><h2><span class="section-number">23.1. </span>Overview of Information Retrieval<a class="headerlink" href="#overview-of-information-retrieval" title="Link to this heading">#</a></h2>
<section id="ad-hoc-retrieval">
<h3><span class="section-number">23.1.1. </span>Ad-hoc Retrieval<a class="headerlink" href="#ad-hoc-retrieval" title="Link to this heading">#</a></h3>
<p>Ad-hoc search and retrieval is a classic <strong>information retrieval (IR)</strong> task consisting of two steps: first, the user specifies his or her information need through a query; second, the information retrieval system fetches documents from a large corpus that are likely to be relevant to the query. Key elements in an ad-hoc retrieval system include</p>
<ul class="simple">
<li><p><strong>Query</strong>, the textual description of information need.</p></li>
<li><p><strong>Corpus</strong>, a large collection of textual documents to be retrieved.</p></li>
<li><p><strong>Relevance</strong> is about whether a retrieved document can meet the user’s information need.</p></li>
</ul>
<p>There has been a long research and product development history on ad-hoc retrieval. Successful products in ad-hoc retrieval include Google search engine [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemogoogle"><span class="std std-numref">Fig. 23.2</span></a>] and Microsoft Bing [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemobing"><span class="std std-numref">Fig. 23.3</span></a>].<br />
One core component within Ad-hoc retrieval is text ranking. The returned documents from a retrieval system or a search engine are typically in the form of an ordered list of texts. These texts (web pages, academic papers, news, tweets, etc.) are ordered with respect to the relevance to the user’s query, or the user’s information need.</p>
<p>A major characteristic of ad-hoc retrieval is the heterogeneity of the query and the documents [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-querylengthdoclengthmsmarco"><span class="std std-numref">Fig. 23.1</span></a>]. A user’s query often comes with potentially unclear intent and is usually very short, ranging from a few words to a few sentences. On the other hand, documents are typically from a different set of authors with varying writing styles and have longer text length, ranging from multiple sentences to many paragraphs. Such heterogeneity poses significant challenges for vocabulary match and semantic match for ad-hoc retrieval tasks.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-querylengthdoclengthmsmarco">
<a class="reference internal image-reference" href="../../_images/query_length_MS_MARCO.png"><img alt="../../_images/query_length_MS_MARCO.png" src="../../_images/query_length_MS_MARCO.png" style="width: 450.0px; height: 360.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.1 </span><span class="caption-text">Query length and document length distribution in Ad-hoc retrieval example using MS MARCO dataset.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-querylengthdoclengthmsmarco" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There have been decades’ research and engineering efforts on developing ad-hoc retrieval models and system. Traditional IR systems primarily rely on techniques to identify exact term matches between a query and a document and compute final relevance score between various weighting schemes. Such exact matching approach has achieved tremendous success due to scalability and computational efficiency - fetching a handful of relevant document from billions of candidate documents. Unfortunately, exact match often suffers from vocabulary mismatch problem where sentences with similar meaning but in different terms are considered not matched. Recent development of deep neural network approach <span id="id1">[<a class="reference internal" href="#id60" title="Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management, 2333–2338. 2013.">HHG+13</a>]</span>, particularly Transformer based pre-trained large language models, has made great progress in semantic matching, or inexact match, by incorporating recent success in natural language understanding and generation. Recently, combining exact matching with semantic matching is empowering many IR and search products.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemogoogle">
<a class="reference internal image-reference" href="../../_images/ad_hoc_retrieval_demo_google.png"><img alt="../../_images/ad_hoc_retrieval_demo_google.png" src="../../_images/ad_hoc_retrieval_demo_google.png" style="width: 643.8px; height: 472.79999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.2 </span><span class="caption-text">Google search engine.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemogoogle" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemobing">
<a class="reference internal image-reference" href="../../_images/ad_hoc_retrieval_demo_bing.png"><img alt="../../_images/ad_hoc_retrieval_demo_bing.png" src="../../_images/ad_hoc_retrieval_demo_bing.png" style="width: 668.5px; height: 473.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.3 </span><span class="caption-text">Microsoft Bing search engine.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-adhocretrievaldemobing" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="open-domain-question-answering">
<h3><span class="section-number">23.1.2. </span>Open-domain Question Answering<a class="headerlink" href="#open-domain-question-answering" title="Link to this heading">#</a></h3>
<p>Another application closely related IR is <strong>open-domain question answering (OpenQA)</strong>, which has found a widespread adoption in products like search engine, intelligent assistant, and automatic customer service. OpenQA is a task to answer factoid questions that humans might ask, using a large collection of documents (e.g., Wikipedia, Web page, or collected document) as the information source. An  OpenQA example is like</p>
<p><strong>Q:</strong> <em>What is the capital of China?</em></p>
<p><strong>A:</strong> <em>Beijing</em>.</p>
<p>Contrast to Ad-hoc retrieval, instead of simply returning a
list of relevant documents, the goal of OpenQA is to identify (or extract) a span of text that directly answers the user’s question. Specifically, for <em>factoid</em> question answering, the OpenQA system primarily focuses on questions that can be answered with short phrases or named entities such as dates, locations, organizations, etc.</p>
<p>A typical modern OpenQA system adopts a two-stage pipeline <span id="id2">[<a class="reference internal" href="#id1407" title="Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051, 2017.">CFWB17</a>]</span> [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-open-domainqa"><span class="std std-numref">Fig. 23.4</span></a>]: (1) A document <strong>retriever</strong> selects a small set of relevant passages that probably contain the answer from a large-scale collection; (2) A document <strong>reader</strong> extracts the answer from relevant documents returned by the document retriever. Similar to ad-hoc search, relevant documents are required to be not only topically related to but also correctly address the question, which requires more semantics understanding beyond exact term matching features. One widely adopted strategy to improve OpenQA system with large corpus is to use an efficient document (or paragraph) retrieval technique to obtain a few relevant documents, and then use an accurate (yet expensive) reader model to read the retrieved documents and find the answer.</p>
<p>Nowadays many web search engines like Google and Bing have been evolving towards higher intelligence by incorporating OpenQA techniques into their search functionalities.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-open-domainqa">
<a class="reference internal image-reference" href="../../_images/open-domain_QA.png"><img alt="../../_images/open-domain_QA.png" src="../../_images/open-domain_QA.png" style="width: 770.4000000000001px; height: 340.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.4 </span><span class="caption-text">A typical open-domain architecture where a retriever retrieves passages from information source relevant to the question.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-open-domainqa" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Compared with ad-hoc retrieval, OpenQA shows reduced heterogeneity between
the question and the answer passage/sentence yet add challenges of precisely understanding the question and locating passages that might contain answers. On one hand, the question is usually in natural language, which is longer than keyword queries and less ambiguous in intent. On the other hand, the answer passages/sentences are usually much shorter text spans than documents, leading to more concentrated topics/semantics. Retrieval and reader models need to capture the patterns expected in the answer passage/sentence based on the intent of the question, such as the matching of the context words, the existence of the expected answer type, and so on.</p>
</section>
<section id="modern-ir-systems">
<h3><span class="section-number">23.1.3. </span>Modern IR Systems<a class="headerlink" href="#modern-ir-systems" title="Link to this heading">#</a></h3>
<p>A traditional IR system, or concretely a search engine, operates through several key steps.</p>
<p>The first step is <strong>crawling</strong>.  A web search engines discover and collect web pages by crawling from site to site; Another vertical search engines such as e-commerce search
engines collect their product information from product description and other product meta data. The second step is <strong>indexing</strong>, which creates an inverted index that maps key words to document ids. The last step is <strong>searching</strong>. Searching is a process that accepts a text query as input, looks up relevant documents from the inverted index, ranks documents, and returns
a list of results, ranked by their relevance to the query.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-traditionalirengine">
<img alt="../../_images/traditional_IR_engine.png" src="../../_images/traditional_IR_engine.png" />
<figcaption>
<p><span class="caption-number">Fig. 23.5 </span><span class="caption-text">Key steps in a traditional IR system.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-traditionalirengine" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The rapid progress of deep neural network learning <span id="id3">[<a class="reference internal" href="#id944" title="Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Volume 1. MIT press Cambridge, 2016.">GBCB16</a>]</span> and their profound impact on natural language processing has also reshaped IR systems and brought IR into a deep learning age. Deep neural networks (e.g., Transformers <span id="id4">[<a class="reference internal" href="#id1178" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span>) have proved their unparalleled capability in semantic understanding over traditional IR margin yet they suffer from high computational cost and latency. This motivates the development of multi-stage retrieval and ranking IR system [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrieverankingarch"><span class="std std-numref">Fig. 23.6</span></a>] in order to better balance trade-offs between effectiveness (i.e., quality and accuracy of final results) and efficiency (i.e., computational cost and latency).</p>
<p>In this multi-stage pipeline, early stage models consists of simpler but more efficient models to reduce the candidate documents from billions to thousands; later stage models consists of complex models to perform accurate ranking for a handful of documents coming from early stages.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrieverankingarch">
<img alt="../../_images/retrieve_ranking_arch.png" src="../../_images/retrieve_ranking_arch.png" />
<figcaption>
<p><span class="caption-number">Fig. 23.6 </span><span class="caption-text">The multi-stage architecture of modern information retrieval systems.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrieverankingarch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In modern search engine, traditional IR models, which is based on term matching, serve as good candidates for early stage model due to their efficiency. The core idea of the traditional approach is to count repetitions of query terms in the document. Large counts indicates higher relevance. Different transformation and weighting schemes for those counts lead to a variety of possible TF-IDF ranking features.</p>
<p>Later stage models are primarily deep learning model. Deep learning models in IR not only provide powerful representations of textual data that capture word and document semantics, allowing a machine to better under queries and documents, but also open doors to multi-modal (e.g., image, video) and multilingual search, ultimately paving the way for developing intelligent search engines that deliver rich contents to users.</p>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 23.1 </span> (Why we need a semantic understanding model)</p>
<section class="remark-content" id="proof-content">
<p>For web-scale search engines like Google or Bing, typically a very small set of popular pages that can answer a good proportion of queries.<span id="id5">[<a class="reference internal" href="#id1446" title="Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. A dual embedding space model for document ranking. arXiv preprint arXiv:1602.01137, 2016.">MNCC16</a>]</span> The vast majority of queries contain common terms. It is possible to use term matching between key words in URL or title and query terms for text ranking; It is also possible to simply memorize the user clicks between common queries between their ideal URLs. For example, a query <em>CNN</em> is always matched to the CNN landing page. These simple methods clearly do not require a semantic understanding on the query and the document content.</p>
<p>However, for new or tail queries as well as new and tail document, a semantic understanding on queries and documents is crucial. For these cases, there is a lack of click evidence between the queries and the documents, and therefore a model that capture the semantic-level relationship between the query and the document content is necessary for text ranking.</p>
</section>
</div></section>
<section id="challenges-and-opportunities-in-ir-systems">
<h3><span class="section-number">23.1.4. </span>Challenges And Opportunities In IR Systems<a class="headerlink" href="#challenges-and-opportunities-in-ir-systems" title="Link to this heading">#</a></h3>
<section id="query-understanding-and-rewriting">
<h4><span class="section-number">23.1.4.1. </span>Query Understanding And Rewriting<a class="headerlink" href="#query-understanding-and-rewriting" title="Link to this heading">#</a></h4>
<p>A user’s query does not always have crystal clear description on the information need of the user. Rather, it often comes with potentially misspellings and unclear intent,  and is usually very short, ranging from a few words to a few sentences [<code class="xref std std-numref docutils literal notranslate"><span class="pre">ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:tab:example_queries_MSMARCO</span></code>]. There are several challenges to understand the user’s query.</p>
<p>Users might use vastly different query representations even though they have the same search intent. For example, suppose users like to get information about <em>distance between Sun and Earth</em>. Common used queries could be</p>
<ul class="simple">
<li><p><em>how far earth sun</em></p></li>
<li><p><em>distance from sun to earth</em></p></li>
<li><p><em>distance of earth from sun</em></p></li>
<li><p><em>how far earth is from the sun</em></p></li>
</ul>
<p>We can see that some of them are just key words rather than a full sentence and some of them might not have the completely correct grammar.</p>
<p>There are also more challenging scenarios where queries are often poorly worded and far from describing the searcher’s actual information needs. Typically, we employ a query rewriting component to expand the search and increase recall,
i.e., to retrieve a larger set of results, with the hope that relevant results will not be missed. Such query rewriting component has multiple sub-components which are summarized below.</p>
<p><strong>Spell checker</strong>
Spell checking queries is an important and necessary feature of modern search. Spell checking enhance user experience by fixing basic spelling mistakes like <em>itlian restaurat</em> to <em>italian restaurant</em>.</p>
<p><strong>Query expansion</strong>
Query expansion improves search result retrieval by adding or substituting terms to the user’s query. Essentially, these additional terms aim to minimize the mismatch between the searcher’s query and available documents. For example, the query <em>italian restaurant</em>, we can expand <em>restaurant</em> to <em>food</em> or <em>cuisine</em> to search all potential candidates.</p>
<p><strong>Query relaxation</strong>
The reverse of query expansion is query relaxation, which expand the search scope when the user’s query is too restrictive. For example, a search for <em>good Italian restaurant</em> can be relaxed to <em>italian restaurant</em>.</p>
<p><strong>Query intent understanding</strong>
This subcomponent aims to figure out the main intent behind the query, e.g., the query <em>coffee shop</em> most likely has a local intent (an interest in nearby places) and the query <em>earthquake</em> may have a news intent. Later on, this intent will help in selecting and ranking the best documents for the query.</p>
<p>Given a rewritten query, It is also important to correctly weigh specific terms in a query such that we can narrow down the search scope. Consider the query <em>NBA news</em>, a relevant document is expected to be about <em>NBA</em> and <strong>news</strong> but have more focus on <em>NBA</em>. There are traditional rule-based approach to determine the term importance as well as recent data-driven approach that determines the term importance based on sophisticated natural language and context understanding.</p>
<p>To improve relevance ranking, it is often necessary to incorporate additional context information (e.g., time, location, etc.) into the user’s query. For example, when a user types in a query <em>coffee shop</em>, retrieve coffee shops by ascending distance to the user’s location can generally improve relevance ranking. Still, there are challenges on deciding for which type of query we need to incorporate the context information.</p>
<figure class="align-default" id="fig-querywordcloud">
<img alt="../../_images/query_word_cloud.png" src="../../_images/query_word_cloud.png" />
<figcaption>
<p><span class="caption-number">Fig. 23.7 </span><span class="caption-text">Word cloud visualization for common query words using MS MARCO data.</span><a class="headerlink" href="#fig-querywordcloud" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="exact-match-and-semantic-match">
<h4><span class="section-number">23.1.4.2. </span>Exact Match And Semantic Match<a class="headerlink" href="#exact-match-and-semantic-match" title="Link to this heading">#</a></h4>
<p>Traditional IR systems retrieve documents mainly by matching keywords in documents with those in search queries. While in many cases exact term match naturally ensure semantic match, there are cases, exact term matching can be insufficient.</p>
<p>The first reason is due to the polysemy of words. That is, a word can mean different things depending on context. The meaning of <em>book</em> is different in <em>text book</em> and <em>book a hotel room</em>. Short queries particularly suffer from Polysemy because they are often devoid of context.</p>
<p>The second reason is due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries. As a result, such a model would have difficulty in retrieving documents that have none of the query terms but turn out to be relevant.</p>
<p>Modern neural-based IR model enable semantic retrieval by learning latent representations of text from data and enable document retrieval based on semantic similarity.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1646">
<caption><span class="caption-number">Table 23.1 </span><span class="caption-text">Retrieval results based on exact matching methods and semantic matching methods.</span><a class="headerlink" href="#id1646" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Query</strong></p></th>
<th class="head"><p>“Weather Related Fatalities”</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Information Need</strong></p></td>
<td><p>A relevant document will report a type of weather event which has directly caused at least one fatality in some location.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Lexical Document</strong></p></td>
<td><p>“.. Oklahoma and South Carolina each recorded three fatalities. There were two each in Arizona, Kentucky, Missouri, Utah and Virginia. Recording a single lightning death for the year were Washington, D.C.; Kansas, Montana, North Dakota, ..”</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Document</p></td>
<td><p>.. Closed roads and icy highways took their toll as at least one motorist was killed in a 17-vehicle pileup in Idaho, a tour bus crashed on an icy stretch of Sierra Nevada interstate and 100-car string of accidents occurred near Seattle …</p></td>
</tr>
</tbody>
</table>
</div>
<p>An IR system solely rely on semantic retrieval is vulnerable to queries that have rare words. This is because rare words are infrequent or even never appear in the training data and learned representation associated with rare words query might be poor due to the nature of data-driven learning. On the other hand, exact matching approach are robust to rare words and can precisely retrieve documents containing rare terms.</p>
<p>Another drawback of semantic retrieval is high false positives: retrieving documents that are only loosely related to the query.</p>
<p>Nowadays, much efforts have been directed to achieve a strong and intelligent modern IR system by combining exact match and semantic match approaches in different ways. Examples include joint optimization of hybrid exact match and semantic match systems, enhancing exact match via semantic based query and document expansion, etc.</p>
</section>
<section id="robustness-to-document-variations">
<h4><span class="section-number">23.1.4.3. </span>Robustness To Document Variations<a class="headerlink" href="#robustness-to-document-variations" title="Link to this heading">#</a></h4>
<p>In response to users’ queries and questions, IR systems needs to search a large collection of text documents, typically at the billion-level scale, to retrieve relevant ones. These documents are comprised of mostly unstructured natural language text, as compared to structured data like tables or forms.</p>
<p>Documents can vary in length, ranging <strong>from sentences</strong> (e.g., searching for similar questions in a community question answering application like Quora) <strong>to lengthy web pages</strong>. A long document might like a short document, covering a similar scope but with more words, which is also known as the <strong>Verbosity hypothesis</strong>. On the other hand, a long document might consist of a number of unrelated short documents concatenated together, which is known as <strong>Scope hypothesis</strong>. The wide variation of document forms lead to different strategies. For example, following the Verbosity hypothesis a long document is represented by a single feature vector. Following the Scope hypothesis, one can break a long document into several semantically distinctive parts and represent each of them as separate feature vectors. We can consider each part as the unit of retrieval or rank the long document by aggregating evidence across its constituent parts.<br />
For full-text scientific articles, we might choose to only consider article titles and abstracts, and ignoring most of the numerical results and analysis.</p>
<p>There are also challenges on breaking a long document into semantically distinctive parts and encode each part into meaningful representation. Recent neural network methods extract semantic parts by clustering tokens in the hidden space and represent documents by multi-vector representations<span id="id6">[<a class="reference internal" href="#id1398" title="Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019.">HSLW19</a>, <a class="reference internal" href="#id1399" title="Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329–345, 2021.">LETC21</a>, <a class="reference internal" href="#id1400" title="Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. Improving document representations by generating pseudo query embeddings for dense retrieval. arXiv preprint arXiv:2105.03599, 2021.">TSJ+21</a>]</span>.</p>
</section>
<section id="computational-efficiency">
<h4><span class="section-number">23.1.4.4. </span>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading">#</a></h4>
<p>IR product such as search engines often serve a huge pool of user and need to handle tremendous volume of search requests during peak time (e.g., when there is breaking news events). To provide the best user experience, computational efficiency of IR models often directly affect user perceived latency. A long standing challenge is to achieve high accuracy/relevance in fetched documents yet to maintain a low latency. While traditional IR methods based on exact term match has excellent computational efficiency and scalability, it suffers from low accuracy due to the vocabulary and semantic mismatch problems. Recent progress in deep learning and natural language process are highlighted by complex transformer-based model <span id="id7">[<a class="reference internal" href="#id1178" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span> that achieved accuracy gain over traditional IR by a large margin yet experienced high latency issues. There are numerous ongoing studies (e.g., <span id="id8">[<a class="reference internal" href="#id1392" title="Luyu Gao, Zhuyun Dai, and Jamie Callan. Coil: revisit exact lexical match in information retrieval with contextualized inverted list. arXiv preprint arXiv:2104.07186, 2021.">GDC21</a>, <a class="reference internal" href="#id1388" title="Bhaskar Mitra and Nick Craswell. An updated duet model for passage re-ranking. arXiv preprint arXiv:1903.07666, 2019.">MC19</a>, <a class="reference internal" href="#id1383" title="Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations of text for web search. In Proceedings of the 26th International Conference on World Wide Web, 1291–1299. 2017.">MDC17</a>]</span>) aiming to bring the benefits from the two sides via hybrid modeling methodology.</p>
<p>To alleviate the computational bottleneck from deep learning based dense retrieval, state-of-the-art search engines also adopts a multi-stage retrieval pipeline system: an efficient first-stage retriever uses a query to fetch a set of documents from the entire document collection, and subsequently one or more more powerful retriever to refine the results.</p>
</section>
</section>
</section>
<section id="text-ranking-evaluation-metrics">
<h2><span class="section-number">23.2. </span>Text Ranking Evaluation Metrics<a class="headerlink" href="#text-ranking-evaluation-metrics" title="Link to this heading">#</a></h2>
<p>Consider a large corpus containing <span class="math notranslate nohighlight">\(N\)</span> documents <span class="math notranslate nohighlight">\(D=\{d_1,...,d_N\}\)</span>. Given a query <span class="math notranslate nohighlight">\(q\)</span>, suppose the retriever and its subsequent re-ranker (if there is) ultimately produce an ordered list of <span class="math notranslate nohighlight">\(k\)</span> relevant document <span class="math notranslate nohighlight">\(R_q = \{d_{i_1},...,d_{i_k}\}\)</span>, where documents <span class="math notranslate nohighlight">\(d_{i_1},...,d_{i_k} \)</span> are ranked by some relevance measure with respect to the query.</p>
<p>In the following, we discuss several commonly used metrics to evaluate text ranking quality and IR system.</p>
<section id="precision-and-recall">
<h3><span class="section-number">23.2.1. </span>Precision And Recall<a class="headerlink" href="#precision-and-recall" title="Link to this heading">#</a></h3>
<p><strong>Precision</strong> and <strong>recall</strong> are metrics concerns the fraction of relevant documents retrieved for a query <span class="math notranslate nohighlight">\(q\)</span>, but they are not concerned with the ranking order.
Specifically, precision computes the fraction of relevant documents with respect to  the total number of documents in the retrieved set <span class="math notranslate nohighlight">\(R_{q}\)</span>, where <span class="math notranslate nohighlight">\(R_{q}\)</span> have <span class="math notranslate nohighlight">\(K\)</span> documents; Recall computes the fraction of relevant documents with respect to the total number of relevant documents in the corpus <span class="math notranslate nohighlight">\(D\)</span>. More formally, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\operatorname{Precision}_{q}&#64;K &amp;=\frac{\sum_{d \in R_{q}} \operatorname{rel}_{q}(d)}{\left|R_{q}\right|} = \frac{1}{K} \sum_{d \in R_{q}} \operatorname{rel}_{q}(d)\\
	\operatorname{Recall}_{q}&#64;K &amp;=\frac{\sum_{d \in R_{q}} \operatorname{rel}_{q}(d)}{\sum_{d \in D} \operatorname{rel}_{q}(d)}
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{rel}_{q}(d)\)</span> is a binary indicator function indicating if document <span class="math notranslate nohighlight">\(d\)</span> is relevant to <span class="math notranslate nohighlight">\(q\)</span>. Note that the denominator <span class="math notranslate nohighlight">\(\sum_{d \in D} \operatorname{rel}_{q}(d)\)</span> is the total number of relevant documents in <span class="math notranslate nohighlight">\(D\)</span>, which is a constant.</p>
<p>Typically, we only retrieve a fixed number <span class="math notranslate nohighlight">\(K\)</span> of documents, i.e., <span class="math notranslate nohighlight">\(|R_q| = K\)</span>, where <span class="math notranslate nohighlight">\(K\)</span> typically takes 100 to 1000. The precision and recall at a given <span class="math notranslate nohighlight">\(K\)</span> can be computed over the total number of queries <span class="math notranslate nohighlight">\(Q\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\operatorname{Precision}&#64;{K} &amp;=\frac{1}{|Q|}\sum_{q\in Q} \operatorname{Precision}_{q}&#64;K \\
	\operatorname{Recall}&#64;{K} &amp;=\frac{1}{|Q|}\sum_{q\in Q} \operatorname{Recall}_{q}&#64;K
\end{align*}\end{split}\]</div>
<p>Because recall has a fixed denominator, recall will increase as <span class="math notranslate nohighlight">\(K\)</span> increases.</p>
</section>
<section id="normalized-discounted-cumulative-gain-ndcg">
<h3><span class="section-number">23.2.2. </span>Normalized Discounted Cumulative Gain (NDCG)<a class="headerlink" href="#normalized-discounted-cumulative-gain-ndcg" title="Link to this heading">#</a></h3>
<p>When we are evaluating retrieval and ranking results based on their relevance to the query, we normally evaluate the ranking result in the following way</p>
<ul class="simple">
<li><p>The ranking result is good if documents with high relevance appear in the top several positions in search engine result list.</p></li>
<li><p>We expect documents with different degree of relevance should contribute to the final ranking in proportion to their relevance.</p></li>
</ul>
<p><strong>Cumulative Gain (CG)</strong> is the sum of the graded relevance scores of all documents in the search result list. CG only considers the relevance of the documents in the search result list, and does not consider the position of these documents in the result list. Given the ranking position of a result list, CG can be defined as:</p>
<div class="math notranslate nohighlight">
\[CG&#64;k = \sum_{i=1}^k s_i\]</div>
<p>where <span class="math notranslate nohighlight">\(s_i\)</span> is the relevance score, or custom defined gain, of document <span class="math notranslate nohighlight">\(i\)</span> in the result list. The relevance score of a document is typically provided by human annotators.</p>
<p><strong>Discounted cumulative gain (DCG)</strong> is the discounted version of CG. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.</p>
<p>The traditional formula of DCG accumulated at a particular rank position <span class="math notranslate nohighlight">\(k\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
DCG&#64;k=\sum_{i=1}^{k} \frac{s_{i}}{\log_{2}(i+1)}=s_{1}+\sum_{i=2}^{k} \frac{s_{i}}{\log _{2}(i+1)}.
\]</div>
<p>An alternative formulation of <span class="math notranslate nohighlight">\(DCG\)</span> places stronger emphasis on more relevant documents:</p>
<div class="math notranslate nohighlight">
\[
{DCG}&#64;k=\sum_{i=1}^{p} \frac{2^{rel_q(d_i)}-1}{\log _{2}(i+1)}.
\]</div>
<p>The <strong>ideal DCG</strong>, <strong>IDCG</strong>, is computed the same way but by sorting all the candidate  documents in the corpus by their relative relevance so that it produces the max possible DCG&#64;k. The <strong>normalized DCG</strong>, <strong>NDCG</strong>, is then given by,</p>
<div class="math notranslate nohighlight">
\[
NDCG&#64;k=\frac{DCG&#64;k}{IDCG&#64;k}.
\]</div>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 23.1 </span></p>
<section class="example-content" id="proof-content">
<p>Consider 5 candidate documents with respect to a query. Let their ground truth relevance scores be</p>
<div class="math notranslate nohighlight">
\[s_1=10, s_2=0,s_3=0,s_4=1,s_5=5,\]</div>
<p>which corresponds to a perfect rank of $<span class="math notranslate nohighlight">\(s_1, s_5, s_4, s_2, s_3.\)</span><span class="math notranslate nohighlight">\(
Let the predicted scores be \)</span><span class="math notranslate nohighlight">\(y_1=0.05, y_2=1.1, y_3=1, y_4=0.5, y_5=0.0,\)</span><span class="math notranslate nohighlight">\(
which corresponds to rank \)</span><span class="math notranslate nohighlight">\(s_2, s_3, s_4, s_1, s_5.\)</span>$</p>
<p>For <span class="math notranslate nohighlight">\(k=1,2\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[DCG&#64;k = 0, NDCG&#64;k=0.\]</div>
<p>For <span class="math notranslate nohighlight">\(k=3\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[DCG&#64;k = \frac{s_4}{\log_2 (3+1)} = 0.5, IDCG&#64;k = 10.0 + \frac{5.0}{\log_2 3} + \frac{1.0}{\log_2 4} = 13.65, NDCG&#64;k=0.0366.\]</div>
<p>For <span class="math notranslate nohighlight">\(k=4\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[DCG&#64;k = \frac{s_4}{\log_2 4} + \frac{s_1}{\log_2 5} = 4.807, IDCG&#64;k = IDCG&#64;3 + 0.0 = 13.65, NDCG&#64;k=0.352.\]</div>
</section>
</div></section>
<section id="online-metrics">
<h3><span class="section-number">23.2.3. </span>Online Metrics<a class="headerlink" href="#online-metrics" title="Link to this heading">#</a></h3>
<p>When a text ranking model is deployed to serve user’s request, we can also measure the model performance by tracking several online metrics.</p>
<p><strong>Click-through rate and dwell time</strong> When a user types a query and starts a search session, we can measure the success of a search session on user’s reactions. On a per-query level, we can define success via click-through rate.
The Click-through rate (CTR) measures the ratio of clicks to impressions.</p>
<div class="math notranslate nohighlight">
\[\operatorname{CTR} = \frac{\text{Number of clicks}}{\text{Number of impressions}},\]</div>
<p>where an impression means a page displayed on the search result page a search engine result page and a click means that the user clicks the page.</p>
<p>One problem with the click-through rate is we cannot simply treat a click as the success of document retrieval and ranking. For example, a click might be immediately followed by a click back as the user quickly realizes the clicked doc is not what he is looking for. We can alleviate this issue by removing clicks that have a short dwell time.</p>
<p><strong>Time to success</strong>: Click-through rate only considers the search session of a single query. In real application case, a user’s search experience might span multiple query sessions until he finds what he needs. For example, the users initially search <em>action movies</em> and they do not find that the ideal results and refine the initial query to a more specific one: <em>action movies by Jackie Chan</em>. Ideally, we can measure the time spent by the user in identifying the page he wants as a metrics.</p>
</section>
</section>
<section id="traditional-sparse-ir-fundamentals">
<h2><span class="section-number">23.3. </span>Traditional Sparse IR Fundamentals<a class="headerlink" href="#traditional-sparse-ir-fundamentals" title="Link to this heading">#</a></h2>
<section id="exact-match-framework">
<h3><span class="section-number">23.3.1. </span>Exact Match Framework<a class="headerlink" href="#exact-match-framework" title="Link to this heading">#</a></h3>
<p>Most traditional approaches to ad-hoc retrieval simply count repetitions of the query terms in the document text and assign proper weights to matched terms to calculate a final matching score. This framework, also known as exact term matching, despite its simplicity, serves as a foundation for many IR systems. A variety of traditional IR methods fall into this framework and they mostly differ in different weighting (e.g., tf-idf) and term normalization (e.g., dogs to dog) schemes.</p>
<p>In the exact term matching, we represent a query and a document by a set of their constituent terms, that is, <span class="math notranslate nohighlight">\(q = \{t^{q}_1,...,t^q_M\}\)</span> and <span class="math notranslate nohighlight">\(d = \{t^{d}_1,...,t^d_M\}\)</span>. The matching score between <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span> with respect to a vocabulary <span class="math notranslate nohighlight">\(V\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
S(q, d)= \sum_{t\in V} f(t)\cdot\mathbb{1}(t\in q\cap d) = \sum_{t \in q \cap d} f(t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is some function of a term and its associated statistics, the three most important of which are</p>
<ul class="simple">
<li><p>Term frequency (how many times a term occurs in a document);</p></li>
<li><p>Document frequency (the number of documents that contain at least once instance of the term);</p></li>
<li><p>Document length (the length of the document that the term occurs in).</p></li>
</ul>
<p>Exact term match framework estimates document relevance based on the count of only the query terms in the document. The position of these occurrences and relationship with other terms in the document are ignored.</p>
<p>BM25 are based on exact matching of query and document words, which
limits the in- formation available to the ranking model and may lead to problems such
vocabulary mismatch</p>
</section>
<section id="tf-idf-vector-space-model">
<h3><span class="section-number">23.3.2. </span>TF-IDF Vector Space Model<a class="headerlink" href="#tf-idf-vector-space-model" title="Link to this heading">#</a></h3>
<p>In the vector space model, we represent each query or document by a vector in a high dimensional space. The vector representation has the dimensionality equal to the vocabulary size, and in which each vector component corresponds to a term in the vocabulary of the collection. This query vector representation stands in contrast to the term vector representation of the previous section, which included only the terms appearing in the query. Given a query vector and a set of document vectors, one for each document in the collection, we rank the documents by computing a similarity measure between the query vector and each document</p>
<p>The most commonly used similarity scoring function for a document vector <span class="math notranslate nohighlight">\(\vec{d}\)</span> and a query vector <span class="math notranslate nohighlight">\(\vec{q}\)</span> is the cosine similarity <span class="math notranslate nohighlight">\(\operatorname{Sim}(\vec{d}, \vec{q})\)</span> is computed as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Sim}(\vec{d}, \vec{q})=\frac{\vec{d}}{|\vec{d}|} \cdot \frac{\vec{q}}{|\vec{q}|}.
\]</div>
<p>The component value associated with term <span class="math notranslate nohighlight">\(t\)</span> is typically the product of term frequency <span class="math notranslate nohighlight">\(tf(t)\)</span> and inverse document frequency <span class="math notranslate nohighlight">\(idf(t)\)</span>. In addition, cosine similarity has a length normalization component that implicitly handles issues related to document length.</p>
<p>Over the years there have been a number of popular variants for both the TF and the IDF functions been proposed and evaluated. A basic version of <span class="math notranslate nohighlight">\(tf(t)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
tf(t,d)= \begin{cases}\log \left(f_{t, d}\right)+1 &amp; \text { if } f_{t, d}&gt;0 \\ 0 &amp; \text { otherwise. }\end{cases}
\end{split}\]</div>
<p>where  <span class="math notranslate nohighlight">\(f_{t, d}\)</span> is the <strong>actual</strong> term frequency count of <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>.
Here the basic intuition is that a term appearing many times in a document should be assigned a higher weight for that document, and the its value should not necessarily increase linearly with the actual term frequency <span class="math notranslate nohighlight">\(f_{t, d}\)</span>, hence the <strong>logarithm is used to proxy the saturation effect</strong>. Although two occurrences of a term should be given more weight than one occurrence, they shouldn’t necessarily be given twice the weight.</p>
<p>A common <span class="math notranslate nohighlight">\(idf(t)\)</span> functions is given by</p>
<div class="math notranslate nohighlight">
\[
idf(t)=\log \left(N / N_{t}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_t\)</span> is the number of documents in the corpus that contain the term <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(N\)</span> is the total number of documents. Here the basic intuition behind the <span class="math notranslate nohighlight">\(idf\)</span> functions is that a term appearing in many documents should be assigned a lower weight than a term appearing in few documents.</p>
</section>
<section id="bm25">
<h3><span class="section-number">23.3.3. </span>BM25<a class="headerlink" href="#bm25" title="Link to this heading">#</a></h3>
<p>One of the most widely adopted exact matching method is called <strong>BM25</strong> (short for Okapi BM25)<span id="id9">[<a class="reference internal" href="#id1473" title="W Bruce Croft, Donald Metzler, and Trevor Strohman. Search engines: Information retrieval in practice. Volume 520. Addison-Wesley Reading, 2010.">CMS10</a>, <a class="reference internal" href="#id1468" title="Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc, 2009.">RZ09</a>, <a class="reference internal" href="#id1393" title="Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: reproducible ranking baselines using lucene. Journal of Data and Information Quality (JDIQ), 10(4):1–20, 2018.">YFL18</a>]</span>. BM25 combines overlapping terms, term-frequency (TF), inverse document frequency (IDF), and document length into following formula</p>
<div class="math notranslate nohighlight">
\[
BM25(q, d)=\sum_{t_{q} \in q\cap d} i d f\left(t_{q}\right) \cdot \frac{t f\left(t_{q}, d\right) \cdot\left(k_{1}+1\right)}{t f\left(t_{q}, d\right)+k_{1} \cdot\left(1-b+b \cdot \frac{|d|}{a v g d l}\right)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(tf(t_q, d)\)</span> is the query’s term frequency in the document <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(|d|\)</span> is the length (in terms of words) of document <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(avgdl\)</span> is the average length of documents in the collection <span class="math notranslate nohighlight">\(D\)</span>, and <span class="math notranslate nohighlight">\(k_{1}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are parameters that are usually tuned on a validation dataset. In practice, <span class="math notranslate nohighlight">\(k_{1}\)</span> is sometimes set to some default value in the range <span class="math notranslate nohighlight">\([1.2,2.0]\)</span> and <span class="math notranslate nohighlight">\(b\)</span> as <span class="math notranslate nohighlight">\(0.75\)</span>. The <span class="math notranslate nohighlight">\(i d f(t)\)</span> is computed as,</p>
<div class="math notranslate nohighlight">
\[
idf(t)=\log \frac{N-N_t+0.5}{N_t+0.5} \Leftarrow \log \frac{\text{Number of documents}}{\text{Number of documents with term } t}.
\]</div>
<p>At first sight, BM25 looks quite like a traditional <span class="math notranslate nohighlight">\(tf\times idf\)</span> weight - a product of two components, one based on <span class="math notranslate nohighlight">\(tf\)</span> and one on <span class="math notranslate nohighlight">\(idf\)</span>.
Intuitively, a document <span class="math notranslate nohighlight">\(d\)</span> has a higher BM25 score if</p>
<ul class="simple">
<li><p>Many query terms also frequently occur in the document;</p></li>
<li><p>These frequent co-occurring terms have larger idf values (i.e., they are not common terms).</p></li>
</ul>
<p>However, there is one significant difference. The <span class="math notranslate nohighlight">\(tf\)</span> component in the BM25 uses <strong>some saturation mechanism to discount the impact of frequent terms in a document</strong> when the document length is long.</p>
<p>BM25 does not concerns with word semantics, that is whether the word is
a noun or a verb, or the meaning of each word. It is only sensitive to <strong>word frequency</strong> (i.e., which are common words and which are rare words), and <strong>the document length</strong>. If one query contains both common words and rare words, this method puts more weight on the rare words and returns documents with more rare words in the query. Besides, a term saturation mechanism is applied to decrease the matching signal when a matched word appears too frequently in the document. <strong>A document-length normalization mechanism is used to discount term weight when a document is longer than average documents in the collection.</strong></p>
<p>More specifically, two parameters in BM25, <span class="math notranslate nohighlight">\(k_1\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, are designed to perform <strong>term frequency saturation</strong>
and <strong>document-length normalization</strong>,respectively.</p>
<ul class="simple">
<li><p>The constant <span class="math notranslate nohighlight">\(k_{1}\)</span> determines how the <span class="math notranslate nohighlight">\(tf\)</span> component of the term weight changes as the frequency increases. If <span class="math notranslate nohighlight">\(k_{1}=0\)</span>, the term frequency component would be ignored and only term presence or absence would matter. If <span class="math notranslate nohighlight">\(k_{1}\)</span> is large, the term weight component would increase nearly linearly with the frequency.</p></li>
<li><p>The constant <span class="math notranslate nohighlight">\(b\)</span> regulates the impact of the length normalization, where <span class="math notranslate nohighlight">\(b=0\)</span> corresponds to no length normalization, and <span class="math notranslate nohighlight">\(b=1\)</span> is full normalization.</p></li>
</ul>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 23.2 </span> (Weighting scheme for long queries)</p>
<section class="remark-content" id="proof-content">
<p>If the query is long, then we might also use similar weighting for query terms. This is appropriate if the queries are paragraph-long information needs, but unnecessary for short queries.</p>
<div class="math notranslate nohighlight">
\[
BM25(q, d)=\sum_{t_q \in q\cap d} idf(t_q) \cdot \frac{\left(k_{1}+1\right) tf(t_q,d) }{k_{1}\left((1-b)+b \times  |d|/avgdl\right)+tf(t_q,d)} \cdot \frac{\left(k_{3}+1\right) tf(t_q,q)}{k_{2}+tf(t_q,q)}
\]</div>
<p>with <span class="math notranslate nohighlight">\(tf(t_q, q)\)</span> being the frequency of term <span class="math notranslate nohighlight">\(t\)</span> in the query <span class="math notranslate nohighlight">\(q\)</span>, and <span class="math notranslate nohighlight">\(k_{2}\)</span> being another positive tuning parameter that this time calibrates term frequency scaling of the query.</p>
</section>
</div></section>
<section id="bm25-efficient-implementation">
<h3><span class="section-number">23.3.4. </span>BM25 Efficient Implementation<a class="headerlink" href="#bm25-efficient-implementation" title="Link to this heading">#</a></h3>
<p>To efficient implementation of BM25, we can pre-compute document side term frequency and store it, which is known as <strong>Eager indexing-time scoring</strong> process <span id="id10">[<a class="reference internal" href="#id1598" title="Xing Han Lù. Bm25s: orders of magnitude faster lexical search via eager sparse scoring. arXiv preprint arXiv:2407.03618, 2024.">Lu24</a>]</span>.
This includes:</p>
<ul class="simple">
<li><p>Tokenize each document into tokens</p></li>
<li><p>Compute the number of documents containing each token and token frequency in each document.</p></li>
<li><p>Compute the idf for each token using the document frequencies</p></li>
<li><p>Compute the BM25 scores for each token in each document <span class="math notranslate nohighlight">\(BM25(t_i,d)\)</span></p></li>
</ul>
<p>During the query process, we tokenize the query <span class="math notranslate nohighlight">\(q\)</span> into tokens <span class="math notranslate nohighlight">\(t_i\)</span> and compute</p>
<div class="math notranslate nohighlight">
\[BM25(q, d) = \sum_{t_i \in q\cap d} BM25(t_i,d).\]</div>
</section>
<section id="bm25f">
<h3><span class="section-number">23.3.5. </span>BM25F<a class="headerlink" href="#bm25f" title="Link to this heading">#</a></h3>
<p><span id="id11">[<a class="reference internal" href="#id1468" title="Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc, 2009.">RZ09</a>]</span></p>
</section>
</section>
<section id="query-and-document-expansion">
<h2><span class="section-number">23.4. </span>Query and Document Expansion<a class="headerlink" href="#query-and-document-expansion" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">23.4.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>Query expansion and document expansion techniques provide two potential solutions to the inherent vocabulary mismatch problem in traditional IR systems. The core idea is to add extra relevant terms to queries and documents respectively to aid relevance matching.</p>
<p>Consider an ad-hoc search example of <em>automobile sales per year in the US</em>.</p>
<ul class="simple">
<li><p>Document expansion can be implemented by appending <em>car</em> in documents that contains the term <em>automobile</em> but not <em>car</em>. Then an exact-match retriever can fetch documents containing either <em>car</em> and <em>automobile</em>.</p></li>
<li><p>Query expansion can be accomplished by retrieving results from both the query <em>automobile sales per year in the US</em> and the query <em>car sales per year in the US</em>.</p></li>
</ul>
<p>There are many different approaches to coming up suitable terms to expand queries and documents in order to make relevance matching easier. These approaches range from traditional rule-based methods such as synonym expansion to recent learning based approaches by mining user logs. For example, augmented terms for a document can come from queries that are relevant from user click-through logs.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-queryexpansionarch">
<a class="reference internal image-reference" href="../../_images/query_expansion_arch.png"><img alt="../../_images/query_expansion_arch.png" src="../../_images/query_expansion_arch.png" style="width: 414.9px; height: 189.9px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.8 </span><span class="caption-text">Query expansion module in an IR system.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-queryexpansionarch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Both query and document expansion can be fit into typical IR architectures through an de-coupled module. A query expansion module takes an input query and output a (richer) expanded query [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-queryexpansionarch"><span class="std std-numref">Fig. 23.8</span></a>]. They are also known as query rewriters or expanders. The module might remove terms deemed unnecessary in the user’s query, for example stop words and add extra terms facilitate the engine to retrieve documents with a high recall.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-docexpansionarch">
<a class="reference internal image-reference" href="../../_images/doc_expansion_arch.png"><img alt="../../_images/doc_expansion_arch.png" src="../../_images/doc_expansion_arch.png" style="width: 504.9px; height: 225.9px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.9 </span><span class="caption-text">Document expansion module in an IR system.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-docexpansionarch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Similarly, document expansion naturally fits into the retrieval and ranking pipeline [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-docexpansionarch"><span class="std std-numref">Fig. 23.9</span></a>]. The index will be simply built upon the expanded corpus to provide a richer set of candidate documents for retrieval. The extra computation for document expansion can be all carried out offline. Therefore, it presents the same level of effectiveness like query expansion but at lower latency costs (for example, using less computationally intensive rerankers).</p>
<p>Query expansion and document expansion have different pros and cons. Main advantages of query expansions include</p>
<ul class="simple">
<li><p>Compare to document expansion, query expansion techniques can be quickly implemented and experimented without modifying the entire indexing. On the other hand, experimenting document expansion techniques can be costly and time-consuming, since the entire indexing is affected.</p></li>
<li><p>Query expansion techniques are generally more flexible. For example, it is easy to switch on or off different features at query time (for example, selectively apply expansion only to certain intents or certain query types).
The flexibility of query expansion also allow us insert an expansion module in different stages of the retrieval-ranking pipeline.</p></li>
</ul>
<p>On the other hand, one unique advantage for document expansion is that: documents are typically much longer than queries, and thus offer more context for a model to choose appropriate expansion terms. Neural based natural language generation models, like Transformers <span id="id12">[<a class="reference internal" href="#id1177" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 5998–6008. 2017.">VSP+17</a>]</span>, can benefit from richer contexts and generate cohesive natural language terms to expand original documents.</p>
</section>
<section id="document-expansion-via-query-prediction">
<h3><span class="section-number">23.4.2. </span>Document Expansion via Query Prediction<a class="headerlink" href="#document-expansion-via-query-prediction" title="Link to this heading">#</a></h3>
<p>Authors in <span id="id13">[<a class="reference internal" href="#id1442" title="Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.">NYLC19</a>]</span> proposed DocT5Query, a document expansion strategy based on a seq-to-seq natural language generation model to enrich each document.</p>
<p>For each document, the transformer generation model predicts a set of queries that are likely to be relevant to the document. Given a dataset of (query, relevant document) pairs, we use a transformer model is trained to takes the document as input and then to produce the target query<sup>[^3]</sup>.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-doc2queryarch">
<a class="reference internal image-reference" href="../../_images/Doc2query_arch.png"><img alt="../../_images/Doc2query_arch.png" src="../../_images/Doc2query_arch.png" style="width: 493.2px; height: 369.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23.10 </span><span class="caption-text">Given a document, our Doc2query model predicts a query, which is appended to the document. Expansion is applied to all documents in the corpus, which are then indexed and searched as before. Image from <span id="id14">[<a class="reference internal" href="#id1442" title="Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019.">NYLC19</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-doc2queryarch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Once the model is trained, we can use the model to predict top-<span class="math notranslate nohighlight">\(k\)</span> queries using beam search and append them to each document in the corpus.</p>
<p>Examples of query predictions on MS MARCO compared to real user queries.</p>
<ul class="simple">
<li><p>Input Document:  <em>July is the hottest month in Washington DC with an average temperature of 80F and the coldest is January at 38F with the most daily sunshine hours at 9 in July. The wettest month is May with an average of <span class="math notranslate nohighlight">\(100 \mathrm{&amp;nbsp;mm}\)</span> of rain.</em></p></li>
<li><p>Predicted Query:  <em>weather in washington dc</em></p></li>
<li><p>Target Query:  <em>what is the temperature in washington</em></p></li>
</ul>
<p>Another example:</p>
<ul class="simple">
<li><p>Input Document: <em>sex chromosome - (genetics) a chromosome that determines the sex of an individual; mammals normally have two sex chromosomes chromosome - a threadlike strand of DNA in the cell nucleus that carries the genes in a linear order; humans have 22 chromosome pairs plus two sex chromosomes.</em></p></li>
<li><p>Predicted Query: <em>what is the relationship between genes and chromosomes</em></p></li>
<li><p>Target Query: *which chromosome controls sex characteristics *</p></li>
</ul>
<p>This document expansion technique has demonstrated its effectiveness on the MS MARCO dataset when it is combined with BM25. The query prediction improves the performance from two aspects:</p>
<ul class="simple">
<li><p>Predicted queries tend to copy some words from the input document (e.g., Washington DC, chromosome), which is sort of equivalent to performing term re-weighting (i.e., increasing the importance of key terms).</p></li>
<li><p>Predicted queries might also contain words not present in the input document (e.g., weather), which can be characterized as expansion by synonyms and other related terms.</p></li>
</ul>
<p>A widely used relevance feedback algorithm was developed by Rocchio <span id="id15">[<a class="reference internal" href="#id1474" title="Joseph Rocchio. Relevance feedback in information retrieval. The Smart retrieval system-experiments in automatic document processing, pages 313–323, 1971.">Roc71</a>]</span> for vector space models.
Let <span class="math notranslate nohighlight">\(\left\{d_{1}, d_{2}, \cdots, d_{k}\right\}\)</span> be the vectors top <span class="math notranslate nohighlight">\(k\)</span> documents retrieved by SNRM in response to the query vector <span class="math notranslate nohighlight">\(q\)</span>. The updated query vector is computed as:</p>
<div class="math notranslate nohighlight">
\[
\vec{q}^{*}=\vec{q}+\alpha \frac{1}{k} \sum_{i=1}^{k} \vec{d}_{i}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> controls the weight of the feedback vector. In practice we only keep the top <span class="math notranslate nohighlight">\(t\)</span> (e.g., <span class="math notranslate nohighlight">\(t=10 -20\)</span>) terms with the highest values in the updated query vector <span class="math notranslate nohighlight">\(\vec{q}^{*}\)</span>.</p>
<p>A continued study in this line shows that replacing the transformer with more powerful seq-to-seq transformer model T5 <span id="id16">[<a class="reference internal" href="#id1395" title="Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.">RSR+19</a>]</span>  can bring further performance gain.</p>
<!-- 
### Pseudo Relevance Feedback

#### Basics

Pseudo relevance feedback (PRF) is another commonly used technique to boost the performance of traditional IR models and to reduce the effect of query-document vocabulary mismatches and improve the estimate the term weights. The interest of using PRF has been recently expanded into the neural IR models {cite}`li2018nprf`.

```{figure} ../img/chapter_application_IR/ApplicationIRSearch/PseudoRelevanceFeedback/PRF_arch.png
:name: ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:fig:prfarch
A typical architecture for pseudo relevance feedback implementation.
```

{numref}`ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:fig:prfarch` shows a typical architecture for pseudo relevance feedback implementation. There are two rounds of retrieval. In the initial retrieval, the retriever fetches a batch of relevant documents based on the original query. We can use the top-$k$ documents to expand and refine the original query. In the second round retrieval, the retriever fetches relevant documents as the final result based on the expanded query. Intuitively, the first-round retrieved documents help identify terms not present in the original query that are discriminative of relevant texts. After the expansion, the expanded query effective mitigate the vocabulary gap between original query and the corpus. 

#### Neural Pseudo Relevance Feedback (NPRF)

**Overview**
Given a query q, NPRF estimates the relevance of a target document $d$ relative to $q$ using following key procedures:
	1. Create initial retrieval result. Given a document corpus $\cD$, a simple ranking method (e.g., BM25) $\operatorname{rel}_{q}(q, d)$ is applied to each $d\in \cD$ to obtain the top-$m$ documents, denoted as $D_{q}$ for $q$.
	2. Compute document-document relevance. We extract the relevance between each $d_{q}\in D_{q}$ and the target $q$, using a neural ranking method $\operatorname{rel}_{d}(d_{q}, d)$.
	3. Compute final relevance.  The relevance scores $\operatorname{rel}_{d}(d_{q}, d)$ from previous step weighted by $rel_{q}(q, d_{q})$ to arrive at $\operatorname{rel}'_{d}\left(d_{q}, d\right)$. The weighting which serves as an estimator for the confidence of the contribution of $d_{q}$ relative to $q$. Finally, we relevance between $q$ and $d$ is given by the aggregation of these adjusted relevance scores, $$\operatorname{rel}_{D}(q, D_{q}, d) = \sum_{d_q\in D_q} \operatorname{rel}'_{d}(d_q, d).$$

```{figure} ../img/chapter_application_IR/ApplicationIRSearch/PseudoRelevanceFeedback/NPRF_arch.png
:name: ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:fig:prfarch
Overall architecture and procedure for neural pseudo relevance feedback (NPRF) implementation.
```

**Model architecture**

The NPRF framework begins with an initial ranking for the input query $q$ determined by $\operatorname{rel}_{q}(., .)$, which forms $D_{q}$, the set of the top- $m$ documents $D_{q}$. The ultimate query-document relevance score $\operatorname{rel}_{D}\left(q, D_{q}, d\right)$ is computed as follows.

Extracting document interactions. Given the target document $d$ and each feedback document $d_{q} \in D_{q}, r e l_{d}(., .)$ is used to evaluate the relevance between $d$ and $d_{q}$, resulting in $m$ real-valued relevance scores, where each score corresponds to the estimated relevance of $d$ according to one feedback document $d_{q}$.

As mentioned, two NIRMs are separately used to compute rel $_{d}\left(d_{q}, d\right)$ in our experiments. Both models take as input the cosine similarities between each pair of terms in $d_{q}$ and $d$, which are computed using pre-trained word embeddings as explained in Section 3.1. Given that both models consider only unigram matches and do not consider term dependencies, we first summarize $d_{q}$ by retaining only the top- $k$ terms according to their $t f$-idf scores, which speeds up training by reducing the document size and removing noisy terms. In our pilot experiments, the use of top- $k$ tf-idf document summarization did not influence performance. For different $d_{q} \in D_{q}$, the same model is used as $\operatorname{rel}_{d}(., .)$ for different pairs of $\left(d_{q}, d\right)$ by sharing model weights.

Combining document interactions. When determining the relevance of a target document $d$, there exist two sources of relevance signals to consider: the target document's relevance relative to the feedback documents $D_{q}$ and its relevance relative to the query $q$ itself. In this step, we combine rel ${ }_{d}\left(d_{q}, d\right)$ for each $d_{q} \in D_{q}$ into an overall feedback document relevance score

$\operatorname{rel}_{D}\left(q, D_{q}, d\right)$. When combining the relevance scores, the agreement between $q$ and each $d_{q}$ is also important, since $d_{q}$ may differ from $q$ in terms of information needs. The relevance of $d_{q}$ from the initial ranking rel ${ }_{q}\left(q, d_{q}\right)$ is employed to quantify this agreement and weight each $\operatorname{rel}_{d}\left(d_{q}, d\right)$ accordingly.

When computing such agreements, it is necessary to remove the influence of the absolute ranges of the scores from the initial ranker. For example, ranking scores from a language model (Ponte and Croft, 1998) and from BM25 (Robertson et al., 1995) can differ substantially in their absolute ranges. To mitigate this, we use a smoothed $\min -\max$ normalization to rescale $\operatorname{rel}_{q}\left(q, d_{q}\right)$ into the range $[0.5,1]$. The min-max normalization is applied by considering $\min \left(\operatorname{rel}_{q}\left(q, d_{q}\right) \mid d_{q} \in\right.$ $\left.D_{q}\right)$ and $\max \left(r e l_{q}\left(q, d_{q}\right) \mid d_{q} \in D_{q}\right)$. Hereafter, $r_{q} l_{q}\left(q, d_{q}\right)$ is used to denote this relevance score after min-max normalization for brevity. The (normalized) relevance score is smoothed and then weighted by the relevance evaluation of $d_{q}$, producing a weighted document relevance score rel $_{d}{ }^{\prime}\left(d_{q}, d\right)$ for each $d_{q} \in D_{q}$ that reflects the relevance of $d_{q}$ relative to $q$. This computation is described in the following equation.
$$
\operatorname{rel}_{d}{ }^{\prime}\left(d_{q}, d\right)=\operatorname{rel}_{d}\left(d_{q}, d\right)\left(0.5+0.5 \times \operatorname{rel}_{q}\left(q, d_{q}\right)\right)
$$
As the last step, we propose two variants for combining the $r e l_{d}{ }^{\prime}\left(d_{q}, d\right)$ for different $d_{q}$ into a single score $r e l_{D}\left(q, D_{q}, d\right)$ : (i) performing a direct summation and (ii) using a feed forward network with a hyperbolic tangent ( $\tanh$ ) non-linear activation. Namely, the first variant simply sums up the scores, whereas the second takes the ranking positions of individual feedback documents into account.

**Optimization and Training**
Each training sample consists of a query $q$, a set of $m$ feedback documents $D_{q}$, a relevant target document $d^{+}$and a non-relevant target document $d^{-}$according to the ground truth. The Adam optimizer (Kingma and $\mathrm{Ba}, 2014$ ) is used with a learning rate $0.001$ and a batch size of 20 . Training normally converges within 30 epochs, with weights uniformly initialized. A hinge loss is employed for training as shown below.
$$
\operatorname{loss}\left(q, D_{q}, d^{+}, d^{-}\right)= \max \left(0,1-\operatorname{rel}\left(q, D_{q}, d^{+}\right)+\operatorname{rel}\left(q, D_{q}, d^{-}\right)\right)
$$ -->
</section>
</section>
<section id="contextualized-term-importance">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-contextualized-term-importance"></span><h2><span class="section-number">23.5. </span>Contextualized Term Importance<a class="headerlink" href="#contextualized-term-importance" title="Link to this heading">#</a></h2>
<section id="context-aware-term-importance-deep-ct">
<h3><span class="section-number">23.5.1. </span>Context-aware Term Importance: Deep-CT<a class="headerlink" href="#context-aware-term-importance-deep-ct" title="Link to this heading">#</a></h3>
<p>In ad-hoc search, queries are mainly short and keyword based without complex grammatical structures. To be able to fetch most relevant results, it is  important to take into account term importance. For example,  given the query <em>bitcoin news</em>, a relevant document is expected to be about <em>bitcoin</em> and <em>news</em>, where the term <em>bitcoin</em> is more important than <em>news</em> in the sense that a document describing other aspects of bitcoin would be more relevant than a document describing news of other things.</p>
<p>In the traditional IR framework, term importance is calculated using inverse document frequency. A term is less important if it is a common term appearing in a large number of documents. These frequency-based term weights have been a huge success in traditional IR systems due to its simplicity and scalability. The problematic aspect is that Tf-idf determines the term importance solely based on word counts rather than the semantics. High-frequency words  does not necessarily indicate their central role to the meaning of the text, especially for short texts where the word frequency distribution is quite flat. Considering the following two passages returned for the query stomach <span id="id17">[<a class="reference internal" href="#id1429" title="Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for first stage retrieval. arXiv preprint arXiv:1910.10687, 2019.">DC19</a>]</span>, one is relevant and one is not:</p>
<ul class="simple">
<li><p>Relevant: In some cases, an upset stomach is the result of an allergic reaction to a certain type of food. It also may be caused by an irritation. Sometimes this happens from consuming too much alcohol or caffeine. Eating too many fatty foods or too much food in general may also cause an upset stomach.</p></li>
<li><p>Less relevant: All parts ofthe body (muscles , brain, heart, and liver) need energy to work. This energy comes from the food we eat. Our bodies digest the food we eat by mixing it with fluids( acids and enzymes) in the stomach. When the stomach digests food, the carbohydrate (sugars and starches) in the food breaks down into another type of sugar, called glucose.</p></li>
</ul>
<p>In both passages, the word <em>stomach</em> appear twice; but the second passage is actually off-topic. This example also suggests that the importance of a term depends on its context, which helps understand the role of the word playing in the text.</p>
<p>Authors in <span id="id18">[<a class="reference internal" href="#id1429" title="Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for first stage retrieval. arXiv preprint arXiv:1910.10687, 2019.">DC19</a>]</span> proposed DeepCT, which uses the contextual word representations from BERT to estimate term importance to improve the traditional IR approach. Specifically, given a word in a specific text, its contextualized word embedding (e.g., BERT) is used a feature vector that characterizes the word’s syntactic and semantic role in the text. Then DeepCT estimates the word’s importance score via a weighted summation:
$<span class="math notranslate nohighlight">\(
\hat{y}_{t, c}= {w} T_{t, c}+b
\)</span><span class="math notranslate nohighlight">\(
where \)</span>T_{t, c} \in \mathbb{R}^D<span class="math notranslate nohighlight">\( is token \)</span>t<span class="math notranslate nohighlight">\( 's contextualized embedding in the text \)</span>c<span class="math notranslate nohighlight">\(; and, \)</span>{w}\in \mathbb{R}^D<span class="math notranslate nohighlight">\( and \)</span>b$ are the weights and bias.</p>
<p>The model parameters of DeepCT are the weight and bias, and they can be estimated from a supervised learning task, per-token regression, given by
$<span class="math notranslate nohighlight">\(
L_{MSE}=\sum_{c} \sum_{t}\left(y_{t, c}-\hat{y}_{t, c}\right)^{2}.
\)</span>$</p>
<p>The ground truth term weight <span class="math notranslate nohighlight">\(y_{t, c}\)</span> for every word in either the query or the document are estimated in the following manner.</p>
<ul class="simple">
<li><p>The importance of a term in a document <span class="math notranslate nohighlight">\(d\)</span> is estimated by the occurrence of the term in relevant queries [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-deepcttermimportancedemo"><span class="std std-numref">Fig. 23.11</span></a>]. More formally, it is given by
$<span class="math notranslate nohighlight">\(
  QTR(t, d)=\frac{\left|Q_{d, t}\right|}{\left|Q_{d}\right|}
  \)</span><span class="math notranslate nohighlight">\(
  \)</span>Q_{d}<span class="math notranslate nohighlight">\( is the set of queries that are relevant to document \)</span>d<span class="math notranslate nohighlight">\(. \)</span>Q_{d, t}<span class="math notranslate nohighlight">\( is the subset of \)</span>Q_{d}<span class="math notranslate nohighlight">\( that contains term \)</span>t$. The intuition is that words that appear in relevant queries are more important than other words in the document.</p></li>
<li><p>The importance of a term in a document <span class="math notranslate nohighlight">\(d\)</span> is estimated by the occurrence of the term in relevant queries [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-deepcttermimportancedemo"><span class="std std-numref">Fig. 23.11</span></a>]. More formally, it is given by
$<span class="math notranslate nohighlight">\(
  TR(t, q)=\frac{\left|D_{q, t}\right|}{\left|D_{q}\right|}
  \)</span><span class="math notranslate nohighlight">\(
  \)</span>D_{q}<span class="math notranslate nohighlight">\( is the set of documents that are relevant to the query \)</span>q<span class="math notranslate nohighlight">\(. \)</span>D_{q, t}<span class="math notranslate nohighlight">\( is the subset of relevant documents that contains term \)</span>t$. The intuition is that a query term is more important if it is mentioned by more relevant documents.</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-deepcttermimportancedemo">
<img alt="docs/img/chapter_application_IR/ApplicationIRSearch/termImportance/deepCT/deepCT_term_importance_demo" src="docs/img/chapter_application_IR/ApplicationIRSearch/termImportance/deepCT/deepCT_term_importance_demo" />
<figcaption>
<p><span class="caption-number">Fig. 23.11 </span><span class="caption-text">Illustration of calculating context-aware term importance for a query (left) and a document (right). Term importance in a query is estimated from relevant documents of the query; term importance in a document is estimated from relevant queries of the document.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-deepcttermimportancedemo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="learnable-context-aware-term-importance-deep-impact">
<h4><span class="section-number">23.5.1.1. </span>Learnable Context-aware Term Importance: Deep-Impact<a class="headerlink" href="#learnable-context-aware-term-importance-deep-impact" title="Link to this heading">#</a></h4>
<p>DeepCT has an interesting “quirk”: in truth, it only learns the term frequency (tf) component of term weights, but still relies on the remaining parts of the BM25 scoring function via the gen- eration of pseudo-documents. This approach also has a weakness: it only assigns weights to terms that are already present in the document, which limits retrieval to exact match. This is an impor- tant limitation that is addressed by the use of dense representations, which are capable of capturing semantic matches.</p>
<p>Deep CT learning independent term-level scores without taking into account the term co-occurrences in the document.</p>
<p>DeepCT (Dai and Callan, 2019), which uses a transformer to learn term weights based on a re- gression model, with the supervision signal coming from the MS MARCO passage ranking test collection</p>
<p>DeepImpact brought together two key ideas: the use of document expansion to iden- tify dimensions in the sparse vector that should have non-zero weights and a term weighting model based on a pairwise loss between relevant and non- relevant texts with respect to a query. Expansion</p>
<p>DeepImpact aim at learning the final term impact jointly across all query terms occurring in a passage.
DeepImpact learns richer interaction patterns among the impacts, when compared to training each im- pact in isolation.</p>
<p>To address vocabulary mismatch, DeepImpact leverages DocT5Query to enrich every document with new terms likely to occur in queries for which the document is relevant.</p>
<p><strong>Network architecture</strong>  The overall architecture of the Deeplmpact neural network is depicted in Figure 1. Deeplmpact feeds a contextual LM encoder the original document terms (in white) and the injected expansion terms (in gray), separating both by a [SEP] separator token to distinguish both contexts. The LM encoder produces an embedding for each input term. The first occurrence of each unique term is provided as input to the impact score encoder, which is a two-layer MLP with ReLU activations. This produces a single-value <strong>positive score</strong> for each unique term in the document, representing its impact. Given a query <span class="math notranslate nohighlight">\(q\)</span>, we model the score of document <span class="math notranslate nohighlight">\(d\)</span> as simply the sum of impacts for the intersection of terms in <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span>. That is,
$<span class="math notranslate nohighlight">\(s(q, d) = \sum_{t\in q\cap d} \operatorname{ScoreEncoder}(h_t).\)</span>$</p>
<p>For each triple, two scores for the corresponding two documents are computed. The model is optimized via pairwise Softmax cross- entropy loss over the computed scores of the documents.</p>
<figure class="align-default" id="fig-deepimpacttermimportancedemo">
<img alt="docs/img/chapter_application_IR/ApplicationIRSearch/termImportance/deepImpact/deepImpact_term_importance_demo" src="docs/img/chapter_application_IR/ApplicationIRSearch/termImportance/deepImpact/deepImpact_term_importance_demo" />
<figcaption>
<p><span class="caption-number">Fig. 23.12 </span><span class="caption-text">DeepImpact architecture.</span><a class="headerlink" href="#fig-deepimpacttermimportancedemo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Quantization and Query Processing.</strong> In our approach we predict real-valued document-term scores, also called impact scores, that we store in the inverted index. Since storing a floating point value per posting would blow up the space requirements of the inverted index, we decided to store impacts in a quantized form. The quantized impact scores belong to the range of <span class="math notranslate nohighlight">\(\left[1,2^{b}-1\right]\)</span>, where <span class="math notranslate nohighlight">\(b\)</span> is the number of bits used to store each value. We experimented with b = 8 using linear quantization, and did not notice any loss in precision w.r.t. the original scores.</p>
</section>
</section>
<section id="tw-bert">
<h3><span class="section-number">23.5.2. </span>TW-BERT<a class="headerlink" href="#tw-bert" title="Link to this heading">#</a></h3>
<p><span id="id19">[<a class="reference internal" href="#id1599" title="Karan Samel, Cheng Li, Weize Kong, Tao Chen, Mingyang Zhang, Shaleen Gupta, Swaraj Khadanga, Wensong Xu, Xingyu Wang, Kashyap Kolipaka, and others. End-to-end query term weighting. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 4778–4786. 2023.">SLK+23</a>]</span></p>
<!-- 
### Contextualized Sparse Representation

#### Motivation

An important point to make here is that neural networks, particularly transformers, have not made sparse representations obsolete. Both dense and sparse learned representations clearly exploit transformers-the trick is that the latter class of techniques then "projects" the learned knowledge back into the sparse vocabulary space. This allows us to reuse decades of innovation in inverted indexes (e.g., integer coding techniques to compress inverted lists) and efficient query evaluation algorithms (e.g., smart skipping to reduce query latency): for example, the Lucene index used in our uniCOIL experiments is only $1.3 \mathrm{&nbsp;GB}$, compared to $\sim 40$ GB for COIL-tok, 26 GB for TCTColBERTv2, and 154 GB for ColBERT. We note, however, that with dense retrieval techniques, fixedwidth vectors can be approximated with binary hash codes, yielding far more compact representations with sacrificing much effectiveness (Yamada et al., 2021). Once again, no clear winner emerges at present.

The complete design space of modern information retrieval techniques requires proper accounting of the tradeoffs between output quality (effectiveness), time (query latency), and space (index size). Here, we have only focused on the first aspect. Learned representations for information retrieval are clearly the future, but the advantages and disadvantages of dense vs. sparse approaches along these dimensions are not yet fully understood. It'll be exciting to see what comes next!

#### COIL-token And Uni-COIL

The recently proposed COIL architecture (Gao et al., 2021a) presents an interesting case for this conceptual framework. Where does it belong? The authors themselves describe COIL as "a new exact lexical match retrieval architecture armed with deep LM representations". COIL produces representations for each document token that are then directly stored in the inverted index, where the term frequency usually goes in an inverted list. Although COIL is perhaps best described as the intellectual descendant of ColBERT (Khattab and Zaharia, 2020), another way to think about it within our conceptual framework is that instead of assignmodel assigns each term a vector "weight". Query evaluation in COIL involves accumulating inner products instead of scalar weights.

```{figure} ../img/chapter_application_IR/ApplicationIRSearch/ContextualizedSparseEmbedding/COIL_tok_embedding
:name: fig:coiltokembedding

```

In another interesting extension, if we reduce the token dimension of COIL to one, the model degenerates into producing scalar weights, which then becomes directly comparable to DeepCT, row (2a) and the "no-expansion" variant of DeepImpact, row (2c). These comparisons isolate the effects of different term weighting models. We dub this variant of COIL "uniCOIL", on top of which we can also add doc2query-T5, which produces a fair comparison to DeepImpact, row ( $2 \mathrm{&nbsp;d})$. The original formulation of COIL, even with a token dimension of one, is not directly amenable to retrieval using inverted indexes because weights can be negative. To address this issue, we added a ReLU operation on the output term weights of the base COIL model to force the model to generate non-negative weights. -->
</section>
</section>
<section id="sparse-retrieval-demonstrations">
<h2><span class="section-number">23.6. </span>Sparse Retrieval Demonstrations<a class="headerlink" href="#sparse-retrieval-demonstrations" title="Link to this heading">#</a></h2>
<section id="bm25-demonstration">
<h3><span class="section-number">23.6.1. </span>BM25 Demonstration<a class="headerlink" href="#bm25-demonstration" title="Link to this heading">#</a></h3>
<p>Consider a doc corpus consisting of the following documents.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>DocID</p></th>
<th class="head"><p>Doc Text</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Hammonton is a town in Atlantic County, New Jersey, United States, known as the “Blueberry Capital of the World.” As of the 2010 United States Census, the town’s population was 14,791.</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Nevada is one of the United States’ states. Its capital is Carson City. Other big cities are Las Vegas and Reno.</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>‘Ohio is one of the 50 states in the United States. Its capital is Columbus. Columbus also is the largest city in Ohio.’</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states. The federal government (including the United States military) also uses capital punishment.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Given the query <em>What is the capital of the United States</em>, which contains tokens {<em>capital, united, states</em>} after stopword removal, we can compute the following sorted list based on BM25 scores.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>BM25 Score</p></th>
<th class="head"><p>Doc Text</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.164</p></td>
<td><p>Capital</mark> punishment (the death penalty) has existed in the <mark>United</mark> <mark>States</mark> since before the <mark>United</mark> <mark>States</mark> was a country. As of 2017, <mark>capital</mark> punishment is legal in 30 of the 50 <mark>States</mark>. The federal government (including the <mark>United</mark> <mark>States</mark> military) also uses <mark>capital</mark> punishment.</p></td>
</tr>
<tr class="row-odd"><td><p>0.143</p></td>
<td><p>‘Ohio is one of the 50 <mark>States</mark> in the <mark>United</mark> <mark>States</mark>. Its <mark>capital</mark> is Columbus. Columbus also is the largest city in Ohio.’</p></td>
</tr>
<tr class="row-even"><td><p>0.139</p></td>
<td><p>Nevada is one of the <mark>United</mark> <mark>States</mark>’ <mark>states</mark>. Its <mark>capital</mark> is Carson City. Other big cities are Las Vegas and Reno.</p></td>
</tr>
<tr class="row-odd"><td><p>0.131</p></td>
<td><p>Hammonton is a town in Atlantic County, New Jersey, <mark>United</mark> <mark>States</mark>, known as the “Blueberry <mark>capital</mark> of the World.” As of the 2010 <mark>United</mark> <mark>States</mark> Census, the town’s population was 14,791.</p></td>
</tr>
<tr class="row-even"><td><p>0.120</p></td>
<td><p>Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the <mark>capital</mark> of the <mark>United</mark> <mark>States</mark>. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the <mark>United</mark> <mark>States</mark> of America.</p></td>
</tr>
</tbody>
</table>
</div>
<p>We have the following observations:</p>
<ul class="simple">
<li><p>The most relevant document (doc 0) has the lowest score due to the fact that</p>
<ul>
<li><p>Document is long</p></li>
<li><p>The word <em>capital</em> appears once, <em>united states</em> appear twice</p></li>
</ul>
</li>
<li><p>The irrelevant document (doc 4) has the highest score due to the fact that words in {<em>capital, united, states</em>} appear multiple times.</p></li>
</ul>
<p>In general BM25-based counting apporach cannot capture the semantic accurately and have a poor precision &#64; k and we need to rely on dense models like bi-encoder and cross-encoder.</p>
<div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 23.3 </span> (Stopword removal)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p>In naive sparse retrieval, stopwords in the query and documents are directly dropped.</p></li>
<li><p>Stopwords are usually coming from a curated word list and the determination of stopword is dependent on the word list creator. For example, in the <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction._stop_words</span></code>, <em>what</em> and <em>when</em> are included as stopwords.</p></li>
<li><p>In some queries, stopwords can be critical for the accurate interpretation of the query intent.</p></li>
<li><p>Instead of naively removing all the stopwords, one can use contextualized term importance model to determine the importance of stopwords in the query (see <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-contextualized-term-importance"><span class="std std-ref">Contextualized Term Importance</span></a>).</p></li>
</ul>
</section>
</div><p>For the same query, if we apply bi-encoder retrieval model (<code class="docutils literal notranslate"><span class="pre">multi-qa-MiniLM-L6-cos-v1</span></code>) to compute the similarity score with the corpus, we have the following scores.</p>
<p>The dense retriever is able to achieve semantic understanding the query and doc:</p>
<ul class="simple">
<li><p>Top 1 result is most relevant</p></li>
<li><p>Top 2 and 3 results are weakly relevant</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Biencoder Scores</p></th>
<th class="head"><p>Doc Text</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.57</p></td>
<td><p>Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.</p></td>
</tr>
<tr class="row-odd"><td><p>0.49</p></td>
<td><p>‘Ohio is one of the 50 states in the United States. Its capital is Columbus. Columbus also is the largest city in Ohio.’</p></td>
</tr>
<tr class="row-even"><td><p>0.48</p></td>
<td><p>Nevada is one of the United States’ states. Its capital is Carson City. Other big cities are Las Vegas and Reno.</p></td>
</tr>
<tr class="row-odd"><td><p>0.37</p></td>
<td><p>Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states. The federal government (including the United States military) also uses capital punishment.</p></td>
</tr>
<tr class="row-even"><td><p>0.18</p></td>
<td><p>Hammonton is a town in Atlantic County, New Jersey, United States, known as the “Blueberry Capital of the World.” As of the 2010 United States Census, the town’s population was 14,791.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="proof remark admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 23.4 </span> (Sparse and dense retriever comparison)</p>
<section class="remark-content" id="proof-content">
<p>In the above example, dense retriever shows a clear advantage over sparse retriever. However, the observation from BEIR benchmark is that BM25 remains as a stronger baseline compared to dense retriever across diverse domains. The seemingly contradictory results can be understood from the following results</p>
<ul class="simple">
<li><p>BM25 is usually performing poorly for short documents, where word count frequency are quite uniform across different words - in other words, two documents with different topics can have a similar count stats for the same query terms.</p></li>
<li><p>Dense retriever model tend to have weaker performance over long documents and in tail topic domains.</p>
<ul>
<li><p>Encoding a document into a fixed length vector (e.g., 512) will have more information loss for long documents due to context window limit (i.e., a model might truncate the part exceeding its max context length) or the model’s own capacity.</p></li>
<li><p>Tail topic (e.g., topics involve uncommon technical jargons) have a much lower presence in the training corpus, causing inferior semantic understanding of the topic.</p></li>
</ul>
</li>
</ul>
<p>Also see <a class="reference internal" href="information_retrieval_fundamentals_part2.html#ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-retriever-comparison"><span class="std std-ref">Discussion: Sparse and Dense Retrieval</span></a> for a more detailed discussion.</p>
</section>
</div></section>
</section>
<section id="note-on-bibliography-and-software">
<h2><span class="section-number">23.7. </span>Note on Bibliography and Software<a class="headerlink" href="#note-on-bibliography-and-software" title="Link to this heading">#</a></h2>
<section id="bibliography">
<h3><span class="section-number">23.7.1. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h3>
<p>For excellent reviews in neural information retrieval, see <span id="id20">[<a class="reference internal" href="#id1386" title="Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. A deep look into neural ranking models for information retrieval. Information Processing &amp; Management, 57(6):102067, 2020.">GFP+20</a>, <a class="reference internal" href="#id1440" title="Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: bert and beyond. Synthesis Lectures on Human Language Technologies, 14(4):1–325, 2021.">LNY21</a>, <a class="reference internal" href="#id1387" title="Bhaskar Mitra, Nick Craswell, and others. An introduction to neural information retrieval. Now Foundations and Trends Boston, MA, 2018.">MC+18</a>]</span></p>
<p>For traditional information retrieval, see <span id="id21">[<a class="reference internal" href="#id1449" title="Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. Information retrieval: Implementing and evaluating search engines. Mit Press, 2016.">BCC16</a>, <a class="reference internal" href="#id1473" title="W Bruce Croft, Donald Metzler, and Trevor Strohman. Search engines: Information retrieval in practice. Volume 520. Addison-Wesley Reading, 2010.">CMS10</a>, <a class="reference internal" href="#id1468" title="Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc, 2009.">RZ09</a>, <a class="reference internal" href="#id1448" title="Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information retrieval. Volume 39. Cambridge University Press Cambridge, 2008.">SchutzeMR08</a>]</span></p>
<div class="docutils container" id="id22">
<div role="list" class="citation-list">
<div class="citation" id="id1449" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">BCC16</a><span class="fn-bracket">]</span></span>
<p>Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. <em>Information retrieval: Implementing and evaluating search engines</em>. Mit Press, 2016.</p>
</div>
<div class="citation" id="id1407" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">CFWB17</a><span class="fn-bracket">]</span></span>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. <em>arXiv preprint arXiv:1704.00051</em>, 2017.</p>
</div>
<div class="citation" id="id1473" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CMS10<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id21">2</a>)</span>
<p>W Bruce Croft, Donald Metzler, and Trevor Strohman. <em>Search engines: Information retrieval in practice</em>. Volume 520. Addison-Wesley Reading, 2010.</p>
</div>
<div class="citation" id="id1429" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DC19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id17">1</a>,<a role="doc-backlink" href="#id18">2</a>)</span>
<p>Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for first stage retrieval. <em>arXiv preprint arXiv:1910.10687</em>, 2019.</p>
</div>
<div class="citation" id="id1178" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCLT18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</div>
<div class="citation" id="id1392" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">GDC21</a><span class="fn-bracket">]</span></span>
<p>Luyu Gao, Zhuyun Dai, and Jamie Callan. Coil: revisit exact lexical match in information retrieval with contextualized inverted list. <em>arXiv preprint arXiv:2104.07186</em>, 2021.</p>
</div>
<div class="citation" id="id944" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">GBCB16</a><span class="fn-bracket">]</span></span>
<p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. <em>Deep learning</em>. Volume 1. MIT press Cambridge, 2016.</p>
</div>
<div class="citation" id="id1386" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">GFP+20</a><span class="fn-bracket">]</span></span>
<p>Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. A deep look into neural ranking models for information retrieval. <em>Information Processing &amp; Management</em>, 57(6):102067, 2020.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">HHG+13</a><span class="fn-bracket">]</span></span>
<p>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In <em>Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>, 2333–2338. 2013.</p>
</div>
<div class="citation" id="id1398" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">HSLW19</a><span class="fn-bracket">]</span></span>
<p>Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. <em>arXiv preprint arXiv:1905.01969</em>, 2019.</p>
</div>
<div class="citation" id="id1440" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">LNY21</a><span class="fn-bracket">]</span></span>
<p>Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: bert and beyond. <em>Synthesis Lectures on Human Language Technologies</em>, 14(4):1–325, 2021.</p>
</div>
<div class="citation" id="id1399" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">LETC21</a><span class="fn-bracket">]</span></span>
<p>Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for text retrieval. <em>Transactions of the Association for Computational Linguistics</em>, 9:329–345, 2021.</p>
</div>
<div class="citation" id="id1598" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Lu24</a><span class="fn-bracket">]</span></span>
<p>Xing Han Lù. Bm25s: orders of magnitude faster lexical search via eager sparse scoring. <em>arXiv preprint arXiv:2407.03618</em>, 2024.</p>
</div>
<div class="citation" id="id1388" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">MC19</a><span class="fn-bracket">]</span></span>
<p>Bhaskar Mitra and Nick Craswell. An updated duet model for passage re-ranking. <em>arXiv preprint arXiv:1903.07666</em>, 2019.</p>
</div>
<div class="citation" id="id1387" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">MC+18</a><span class="fn-bracket">]</span></span>
<p>Bhaskar Mitra, Nick Craswell, and others. <em>An introduction to neural information retrieval</em>. Now Foundations and Trends Boston, MA, 2018.</p>
</div>
<div class="citation" id="id1383" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">MDC17</a><span class="fn-bracket">]</span></span>
<p>Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations of text for web search. In <em>Proceedings of the 26th International Conference on World Wide Web</em>, 1291–1299. 2017.</p>
</div>
<div class="citation" id="id1446" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">MNCC16</a><span class="fn-bracket">]</span></span>
<p>Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. A dual embedding space model for document ranking. <em>arXiv preprint arXiv:1602.01137</em>, 2016.</p>
</div>
<div class="citation" id="id1442" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NYLC19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. <em>arXiv preprint arXiv:1904.08375</em>, 2019.</p>
</div>
<div class="citation" id="id1395" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">RSR+19</a><span class="fn-bracket">]</span></span>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>arXiv preprint arXiv:1910.10683</em>, 2019.</p>
</div>
<div class="citation" id="id1468" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RZ09<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id11">2</a>,<a role="doc-backlink" href="#id21">3</a>)</span>
<p>Stephen Robertson and Hugo Zaragoza. <em>The probabilistic relevance framework: BM25 and beyond</em>. Now Publishers Inc, 2009.</p>
</div>
<div class="citation" id="id1474" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">Roc71</a><span class="fn-bracket">]</span></span>
<p>Joseph Rocchio. Relevance feedback in information retrieval. <em>The Smart retrieval system-experiments in automatic document processing</em>, pages 313–323, 1971.</p>
</div>
<div class="citation" id="id1599" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">SLK+23</a><span class="fn-bracket">]</span></span>
<p>Karan Samel, Cheng Li, Weize Kong, Tao Chen, Mingyang Zhang, Shaleen Gupta, Swaraj Khadanga, Wensong Xu, Xingyu Wang, Kashyap Kolipaka, and others. End-to-end query term weighting. In <em>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 4778–4786. 2023.</p>
</div>
<div class="citation" id="id1448" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">SchutzeMR08</a><span class="fn-bracket">]</span></span>
<p>Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. <em>Introduction to information retrieval</em>. Volume 39. Cambridge University Press Cambridge, 2008.</p>
</div>
<div class="citation" id="id1400" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">TSJ+21</a><span class="fn-bracket">]</span></span>
<p>Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. Improving document representations by generating pseudo query embeddings for dense retrieval. <em>arXiv preprint arXiv:2105.03599</em>, 2021.</p>
</div>
<div class="citation" id="id1177" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">VSP+17</a><span class="fn-bracket">]</span></span>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Advances in neural information processing systems</em>, 5998–6008. 2017.</p>
</div>
<div class="citation" id="id1393" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">YFL18</a><span class="fn-bracket">]</span></span>
<p>Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: reproducible ranking baselines using lucene. <em>Journal of Data and Information Quality (JDIQ)</em>, 10(4):1–20, 2018.</p>
</div>
</div>
</div>
</section>
<section id="software">
<h3><span class="section-number">23.7.2. </span>Software<a class="headerlink" href="#software" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/facebookresearch/faiss/wiki/">Faiss</a> is a recently developed computational library for efficient similarity search and clustering of dense vectors.</p>
<!-- \printbibliography
\end{refsection}

[^1]: Semantic matching means words (or phrases) have similar meaning despite they are different words

[^2]: This is also known as query agnostic problem {cite}`tang2021improving`.

[^3]: To avoid excessive memory usage, we truncate each document to 400 tokens and queries to 100 tokens.

[^4]: Let $L = \sum_j q_j\log p_j$, where $p_j = \exp(z_j)/\sum_j\exp(z_j)$. Use $\log q_i = z_i - \log (\sum_j \exp(z_j))$, we have
	```{math}
	\begin{align*}
	\frac{\partial L}{\partial z_i} &= \sum_j q_j (\delta_{ij} - p_j) \\
	& =(q_i - p_i)
	\end{align*}
	```

[^5]: \url{https://en.wikipedia.org/wiki/GNU_Aspell}

[^6]: https://en.wikipedia.org/wiki/Hunspell

[^7]: \url{https://microsoft.github.io/msmarco/TREC-Deep-Learning.html} -->
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_application_IR"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_prompt/advanced_prompt.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Advanced Prompting Techniques</p>
      </div>
    </a>
    <a class="right-next"
       href="information_retrieval_fundamentals_part2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">24. </span>Information Retrieval and Dense Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-information-retrieval">23.1. Overview of Information Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ad-hoc-retrieval">23.1.1. Ad-hoc Retrieval</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-domain-question-answering">23.1.2. Open-domain Question Answering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-ir-systems">23.1.3. Modern IR Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-opportunities-in-ir-systems">23.1.4. Challenges And Opportunities In IR Systems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-understanding-and-rewriting">23.1.4.1. Query Understanding And Rewriting</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-match-and-semantic-match">23.1.4.2. Exact Match And Semantic Match</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#robustness-to-document-variations">23.1.4.3. Robustness To Document Variations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency">23.1.4.4. Computational Efficiency</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-ranking-evaluation-metrics">23.2. Text Ranking Evaluation Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">23.2.1. Precision And Recall</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-discounted-cumulative-gain-ndcg">23.2.2. Normalized Discounted Cumulative Gain (NDCG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-metrics">23.2.3. Online Metrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-sparse-ir-fundamentals">23.3. Traditional Sparse IR Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-match-framework">23.3.1. Exact Match Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-vector-space-model">23.3.2. TF-IDF Vector Space Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25">23.3.3. BM25</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25-efficient-implementation">23.3.4. BM25 Efficient Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25f">23.3.5. BM25F</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#query-and-document-expansion">23.4. Query and Document Expansion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">23.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-expansion-via-query-prediction">23.4.2. Document Expansion via Query Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextualized-term-importance">23.5. Contextualized Term Importance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-aware-term-importance-deep-ct">23.5.1. Context-aware Term Importance: Deep-CT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learnable-context-aware-term-importance-deep-impact">23.5.1.1. Learnable Context-aware Term Importance: Deep-Impact</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tw-bert">23.5.2. TW-BERT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-retrieval-demonstrations">23.6. Sparse Retrieval Demonstrations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bm25-demonstration">23.6.1. BM25 Demonstration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-on-bibliography-and-software">23.7. Note on Bibliography and Software</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">23.7.1. Bibliography</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">23.7.2. Software</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>