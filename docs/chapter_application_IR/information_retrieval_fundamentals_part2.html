
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>25. Information Retrieval and Dense Models &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_application_IR/information_retrieval_fundamentals_part2';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="26. Application of LLM in IR (WIP)" href="application_LLM_in_IR.html" />
    <link rel="prev" title="24. Information Retrieval and Sparse Retrieval" href="information_retrieval_fundamentals_part1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">14. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">20. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">21. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">22. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">23. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="information_retrieval_fundamentals_part1.html">24. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">25. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="application_LLM_in_IR.html">26. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="conversational_IR.html">27. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">28. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">29. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Information Retrieval and Dense Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-dense-models">25.1. Semantic Dense Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">25.1.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-architecture-paradigms">25.1.2. Two Architecture Paradigms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classic-representation-based-learning">25.1.3. Classic Representation-based Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dssm">25.1.3.1. DSSM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-dssm">25.1.3.2. CNN-DSSM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfomer-retrievers-and-rerankers">25.2. Transfomer Retrievers and Rerankers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">25.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bi-encoder-retriever">25.2.2. Bi-Encoder Retriever</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-encoder-for-point-wise-ranking">25.2.3. Cross-Encoder For Point-wise Ranking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#duo-bert-for-pairwise-ranking">25.2.4. Duo-BERT For Pairwise Ranking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multistage-retrieval-and-ranking-pipeline">25.2.5. Multistage Retrieval and Ranking Pipeline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dc-bert">25.2.6. DC-BERT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-attribute-and-multi-task-modeling">25.2.7. Multi-Attribute and Multi-task Modeling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-vector-retrievers">25.3. Multi-Vector Retrievers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">25.3.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colbert">25.3.2. ColBERT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">25.3.2.1. Overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding">25.3.2.2. Encoding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#late-interaction">25.3.2.3. Late Interaction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-re-ranking">25.3.2.4. Retrieval &amp; Re-ranking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">25.3.2.5. Evaluation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-clusters-as-pseudo-query-embeddings">25.3.3. Semantic Clusters As Pseudo Query Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-level-representation-and-retrieval-colbert">25.3.4. Token-level Representation and Retrieval (ColBert)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colbert-v2">25.3.5. Colbert v2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ranker-training">25.4. Ranker Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">25.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">25.4.2. Training Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training-objective-functions">25.4.3. Model Training Objective Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-regression-objective">25.4.3.1. Pointwise Regression Objective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-ranking-objective">25.4.3.2. Pointwise Ranking Objective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-ranking-via-triplet-loss">25.4.3.3. Pairwise Ranking via Triplet Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#n-pair-loss">25.4.3.4. N-pair Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#n-pair-dual-loss">25.4.3.5. N-pair Dual Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#doc-doc-n-pair-loss">25.4.3.6. Doc-Doc N-pair Loss</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-sampling-strategies">25.5. Training Data Sampling Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles">25.5.1. Principles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-sampling-methods-i-heuristic-methods">25.5.2. Negative Sampling Methods I: Heuristic Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-negatives-and-in-batch-negatives">25.5.2.1. Random Negatives and In-batch Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#popularity-based-negative-sampling">25.5.2.2. Popularity-based Negative Sampling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-aware-negative-sampling">25.5.2.3. Topic-aware Negative Sampling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-sampling-methods-ii-model-based-methods">25.5.3. Negative Sampling Methods II: Model-based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#static-hard-negative-examples">25.5.3.1. Static Hard Negative Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-hard-negative-mining">25.5.3.2. Dynamic Hard Negative Mining</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-batch-large-scale-negatives">25.5.3.3. Cross-Batch Large-Scale Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum-negatives">25.5.3.4. Momentum Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-positives">25.5.3.5. Hard Positives</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label-denoising">25.5.4. Label Denoising</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-negatives">25.5.4.1. False Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-positives">25.5.4.2. False Positives</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-augmentation">25.5.5. Data Augmentation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation">25.6. Knowledge Distillation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">25.6.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation-training-framework">25.6.2. Knowledge Distillation Training Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-distillation-strategies">25.6.3. Example Distillation Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bi-encoder-teacher-distillation">25.6.3.1. Bi-encoder Teacher Distillation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-encoder-embedding-similarity-distillation">25.6.3.2. Cross-Encoder Embedding Similarity Distillation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-teacher-distillation">25.6.3.3. Ensemble Teacher Distillation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-for-retrieval">25.7. Pretraining for Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contriever">25.7.1. Contriever</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-sparse-and-dense-retrieval">25.8. Discussion: Sparse and Dense Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-size-and-embedding-dimensionality-impact">25.8.1. Index Size and Embedding Dimensionality Impact</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-nearest-neighbor-search">25.9. Approximate Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">25.9.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-quantization">25.9.2. Vector Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-representation-and-storage">25.9.2.1. Approximate Representation And Storage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-distances-using-quantized-codes">25.9.2.2. Approximating Distances Using Quantized Codes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#product-quantization">25.9.3. Product Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#from-vector-quantization-to-product-quantization">25.9.3.1. From Vector Quantization To Product Quantization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">25.9.3.2. Approximating Distances Using Quantized Codes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-non-exhaustive-nearest-neighbor-search">25.9.4. Approximate Non-exhaustive Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-quantization-and-inverted-file-indexing">25.9.4.1. Hierarchical Quantization And Inverted File Indexing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-datasets">25.10. Benchmark Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ms-marco">25.10.1. MS MARCO</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-ranking-task">25.10.1.1. Document Ranking Task</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#passage-ranking-task">25.10.1.2. Passage Ranking Task</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terc">25.10.2. TERC</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trec-deep-learning-track">25.10.2.1. TREC-deep Learning Track</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trec-car">25.10.2.2. TREC-CAR</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-question-nq">25.10.3. Natural Question (NQ)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entity-questions">25.10.4. Entity Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beir">25.10.5. BEIR</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lotte">25.10.6. LoTTE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mteb">25.10.7. MTEB</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-on-bibliography-and-software">25.11. Note On Bibliography And Software</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">25.11.1. Bibliography</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">25.11.2. Software</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="information-retrieval-and-dense-models">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2"></span><h1><span class="section-number">25. </span>Information Retrieval and Dense Models<a class="headerlink" href="#information-retrieval-and-dense-models" title="Link to this heading">#</a></h1>
<section id="semantic-dense-models">
<span id="index-0"></span><h2><span class="section-number">25.1. </span>Semantic Dense Models<a class="headerlink" href="#semantic-dense-models" title="Link to this heading">#</a></h2>
<section id="motivation">
<h3><span class="section-number">25.1.1. </span>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h3>
<p>For ad-hoc search, traditional exact-term matching models (e.g., BM25) are playing critical roles in both traditional IR systems [<a class="reference internal" href="information_retrieval_fundamentals_part1.html#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-traditionalirengine"><span class="std std-numref">Fig. 24.5</span></a>] and modern multi-stage pipelines [<a class="reference internal" href="information_retrieval_fundamentals_part1.html#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrieverankingarch"><span class="std std-numref">Fig. 24.6</span></a>]. Unfortunately, exact-term matching inherently suffers from the vocabulary mismatch problem due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries.</p>
<p>Early latent semantic models such as <strong>latent semantic analysis (LSA)</strong> illustrated the idea of identifying semantically relevant documents for a query when lexical matching is insufficient. However, their effectiveness in addressing the language discrepancy between documents and search queries are limited by their weak modeling capacity (i.e., simple, linear models). Also, these model parameters are typically learned via the unsupervised learning, i.e., by grouping different terms that occur in a similar context into the same semantic cluster.</p>
<p>The introduction of deep neural networks for semantic modeling and retrieval was pioneered in <span id="id1">[<a class="reference internal" href="#id114" title="Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management, 2333–2338. 2013.">HHG+13</a>]</span>. Recent deep learning model utilize the neural networks with large learning capacity and user-interaction data for supervised learning, which has led to significance performance gain over LSA.  Similarly in the field of OpenQA <span id="id2">[<a class="reference internal" href="#id1463" title="Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.">KOuguzM+20</a>]</span>, whose first stage is to retrieve relevant passages that might contain the answer, semantic-based retrieval has also demonstrated performance gains over traditional retrieval methods.</p>
</section>
<section id="two-architecture-paradigms">
<h3><span class="section-number">25.1.2. </span>Two Architecture Paradigms<a class="headerlink" href="#two-architecture-paradigms" title="Link to this heading">#</a></h3>
<p>The current neural  architecture paradigms for IR can be categorized into two classes: <strong>representation-based</strong> and <strong>interaction-based</strong> [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-twoparadigms"><span class="std std-numref">Fig. 25.1</span></a>].</p>
<p>In the representation-based architecture, a query and a document are encoded independently into two embedding vectors, then their relevance is estimated based on a single similarity score between the two embedding vectors.</p>
<p>Here we would like to make a critical distinction on symmetric vs. asymmetric encoding:</p>
<ul class="simple">
<li><p>For <strong>symmetric encoding</strong>, the query and the entries in the corpus are typically of the similar length and have the same amount of content and they are encoded using the same network. Symmetric encoding is used for symmetric semantic search. An example would be searching for similar questions. For instance, the query could be <em>How to learn Python online?</em> and the entry that satisfies the search is like <em>How to learn Python on the web?</em>.</p></li>
<li><p>For <strong>asymmetric encoding</strong>, we usually have a short query (like a question or some keywords) and we would like to find a longer paragraph answering the query; they are encoded using two different networks. An example would be information retrieval. The entry is typically a paragraph or a web-page.</p></li>
</ul>
<p>In the interaction-based architecture, instead of directly encoding <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span> into individual embeddings, term-level interaction features across the query and the document are first constructed. Then a deep neural network is used to extract high-level matching features from the interactions and produce a final relevance score.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-twoparadigms">
<img alt="../../_images/Two_paradigms.png" src="../../_images/Two_paradigms.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.1 </span><span class="caption-text">Two common architectural paradigms in semantic retrieval learning: representation-based learning (left) and interaction-based learning (right).</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-twoparadigms" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>These two architectures have different strengths in modeling relevance and final model serving. For example, a representation-based model architecture makes it possible to pre-compute and cache document representations offline, greatly reducing the online computational load per query. However, the pre-computation of query-independent document representations often miss term-level matching features that are critical to construct high-quality retrieval results. On the other hand, interaction-based architectures are often good at capturing the fine-grained matching feature between the query and the document.</p>
<p>Since interaction-based models can model interactions between word pairs in queries and document, they are effective for <strong>re-ranking</strong>, but are cost-prohibitive for first-stage retrieval as the expensive document-query interactions must be computed online for all ranked documents.</p>
<p>Representation-based models enable low-latency, full-collection retrieval with a dense index. By representing queries and documents with dense vectors, retrieval is reduced to nearest neighbor search, or a maximum inner product search (MIPS) <span id="id3">[]</span> problem if similarity is represented by an inner product.</p>
<p>In recent years, there has been increasing effort on accelerating maximum inner product and nearest neighbor search, which led to high-quality implementations of libraries for nearest neighbor search such as hnsw <span id="id4">[<a class="reference internal" href="#id1505" title="Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824–836, 2018.">MY18</a>]</span>, FAISS <span id="id5">[<a class="reference internal" href="#id1506" title="Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019.">JDJegou19</a>]</span>, and SCaNN <span id="id6">[<a class="reference internal" href="#id1507" title="Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, 3887–3896. PMLR, 2020.">GSL+20</a>]</span>.</p>
</section>
<section id="classic-representation-based-learning">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-classicrepresentationlearning"></span><h3><span class="section-number">25.1.3. </span>Classic Representation-based Learning<a class="headerlink" href="#classic-representation-based-learning" title="Link to this heading">#</a></h3>
<section id="dssm">
<h4><span class="section-number">25.1.3.1. </span>DSSM<a class="headerlink" href="#dssm" title="Link to this heading">#</a></h4>
<p>Deep structured semantic model (DSSM) <span id="id7">[<a class="reference internal" href="#id114" title="Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management, 2333–2338. 2013.">HHG+13</a>]</span> improves the previous latent semantic models in two aspects: 1) DSSM is supervised learning based on labeled data, while latent semantic models are unsupervised learning; 2) DSSM utilize deep neural networks to capture more semantic meanings.</p>
<p>The high-level architecture of DSSM is illustrated in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dssm"><span class="std std-numref">Fig. 25.2</span></a>. First, we represent a query and a document (only its title) by a sparse vector, respectively. Second, we apply a non-linear projection to map the query and the document sparse vectors to two low-dimensional embedding vectors in a common semantic space. Finally, the relevance of each document given the query is calculated as the cosine similarity between their embedding vectors in that semantic space.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dssm">
<a class="reference internal image-reference" href="../../_images/dssm.png"><img alt="../../_images/dssm.png" src="../../_images/dssm.png" style="width: 589.2px; height: 325.20000000000005px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.2 </span><span class="caption-text">The architecture of DSSM. Two MLP encoders with shared parameters are used to encode a query and a document into dense vectors. Query and document are both represented by term vectors. The final relevance score is computed via dot product between the query vector and the document vector.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dssm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To represent word features in the query and the documents, DSSM adopt a word level sparse term vector representation with letter 3-gram vocabulary, whose size is approximately <span class="math notranslate nohighlight">\(30k \approx 30^3\)</span>. Here 30 is the approximate number of alphabet letters. This is also known as a letter trigram word hashing technique. In other words, both query and the documents will be represented by sparse vectors with dimensionality of <span class="math notranslate nohighlight">\(30k\)</span>.</p>
<p>The usage of letter 3-gram vocabulary has multiple benefits compared to the full vocabulary:</p>
<ul class="simple">
<li><p>Avoid OOV problem with finite-size vocabulary or term vector dimensionality.</p></li>
<li><p>The use of letter n-gram can capture morphological meanings of words.</p></li>
</ul>
<p>One problem of this method is collision, i.e., two different
words could have the same letter n-gram vector representation because this is a bag-of-words representation that does not take into account orders. But the collision probability is rather low.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1663">
<caption><span class="caption-number">Table 25.1 </span><span class="caption-text">Word hashing token size and collision numbers as a function of the vocabulary size and the type of letter ngrams.</span><a class="headerlink" href="#id1663" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Word Size</p></th>
<th class="head"><p>Letter-Bigram</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Letter-Trigram</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p>Token Size</p></td>
<td><p>Collision</p></td>
<td><p>Token Size</p></td>
<td><p>Collision</p></td>
</tr>
<tr class="row-odd"><td><p>40k</p></td>
<td><p>1107</p></td>
<td><p>18</p></td>
<td><p>10306</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>500k</p></td>
<td><p>1607</p></td>
<td><p>1192</p></td>
<td><p>30621</p></td>
<td><p>22</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Training</strong>. The neural network model is trained on the clickthrough data to map a query and its relevant document to vectors that are similar to each other and vice versa. The click-through logs consist of a list of queries and their clicked documents. It is assumed that a query is relevant, at least partially, to the documents that are clicked on for that query.</p>
<p>The semantic relevance score between a query <span class="math notranslate nohighlight">\(q\)</span> and a document <span class="math notranslate nohighlight">\(d\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
R(q, d)=\operatorname{Cosine}\left(y_{Q}, y_{D}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(E_{q}\)</span> and <span class="math notranslate nohighlight">\(E_{q}\)</span> are the embedding vectors of the query and the document, respectively. The conditional probability of a document being relevant to a given query is now defined through a Softmax function</p>
<div class="math notranslate nohighlight">
\[
P(d \mid q)=\frac{\exp (\gamma R(q, d))}{\sum_{d^{\prime} \in D} \exp \left(\gamma R\left(q, d^{\prime}\right)\right)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a smoothing factor as a hyperparameter. <span class="math notranslate nohighlight">\(D\)</span> denotes the set of candidate documents to be ranked. While <span class="math notranslate nohighlight">\(D\)</span> should ideally contain all possible documents in the corpus, in practice, for each query <span class="math notranslate nohighlight">\(q\)</span>, <span class="math notranslate nohighlight">\(D\)</span> is approximated by including the clicked document <span class="math notranslate nohighlight">\(d^{+}\)</span> and four randomly selected un-clicked documents.</p>
<p>In training, the model parameters are estimated to maximize the likelihood of the clicked documents given the queries across the training set. Equivalently, we need to minimize the following loss function</p>
<div class="math notranslate nohighlight">
\[
L=-\sum_{(q, d^+)}\log P\left(d^{+} \mid q\right).
\]</div>
<p><strong>Evaluation</strong> DSSM is compared with baselines of traditional IR models like TF-IDF, BM25, and LSA. Specifically, the best performing DNN-based semantic model, L-WH DNN, uses three hidden layers, including the layer with the Letter-trigram-based Word Hashing, and an output layer, and is discriminatively trained on query-title pairs.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Models</p></th>
<th class="head"><p>NDCG&#64;1</p></th>
<th class="head"><p>NDCG&#64;3</p></th>
<th class="head"><p>NDCG&#64;10</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TF-IDF</p></td>
<td><p>0.319</p></td>
<td><p>0.382</p></td>
<td><p>0.462</p></td>
</tr>
<tr class="row-odd"><td><p>BM25</p></td>
<td><p>0.308</p></td>
<td><p>0.373</p></td>
<td><p>0.455</p></td>
</tr>
<tr class="row-even"><td><p>LSA</p></td>
<td><p>0.298</p></td>
<td><p>0.372</p></td>
<td><p>0.455</p></td>
</tr>
<tr class="row-odd"><td><p>L-WH DNN</p></td>
<td><p><strong>0.362</strong></p></td>
<td><p><strong>0.425</strong></p></td>
<td><p><strong>0.498</strong></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="cnn-dssm">
<h4><span class="section-number">25.1.3.2. </span>CNN-DSSM<a class="headerlink" href="#cnn-dssm" title="Link to this heading">#</a></h4>
<p>DSSM treats a query or a document as a bag of words, the fine-grained contextual structures embedding in the word order are lost. The DSSM-CNN<span id="id8">[<a class="reference internal" href="#id1485" title="Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM international conference on conference on information and knowledge management, 101–110. 2014.">SHG+14</a>]</span> [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-cnndssm"><span class="std std-numref">Fig. 25.3</span></a>] directly represents local contextual features at the word n-gram level; i.e., it projects each raw word n-gram to a low dimensional feature vector where semantically similar word <span class="math notranslate nohighlight">\(\mathrm{n}\)</span> grams are projected to vectors that are close to each other in this feature space.</p>
<p>Moreover, instead of simply summing all local word-n-gram features evenly, the DSSM-CNN performs a max pooling operation to select the highest neuron activation value across all word n-gram features at each dimension. This amounts to extract the sentence-level salient semantic concepts.</p>
<p>Meanwhile, for any sequence of words, this operation forms a fixed-length sentence level feature vector, with the same dimensionality as that of the local word n-gram features.</p>
<p>Given the letter-trigram based word representation, we represent a word-n-gram by concatenating the letter-trigram vectors of each word, e.g., for the <span class="math notranslate nohighlight">\(t\)</span>-th word-n-gram at the word-ngram layer, we have:</p>
<div class="math notranslate nohighlight">
\[
l_{t}=\left[f_{t-d}^{T}, \ldots, f_{t}^{T}, \ldots, f_{t+d}^{T}\right]^{T}, \quad t=1, \ldots, T
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{t}\)</span> is the letter-trigram representation of the <span class="math notranslate nohighlight">\(t\)</span>-th word, and <span class="math notranslate nohighlight">\(n=2 d+1\)</span> is the size of the contextual window. In our experiment, there are about <span class="math notranslate nohighlight">\(30K\)</span> unique letter-trigrams observed in the training set after the data are lower-cased and punctuation removed. Therefore, the letter-trigram layer has a dimensionality of <span class="math notranslate nohighlight">\(n \times 30 K\)</span>.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-cnndssm">
<a class="reference internal image-reference" href="../../_images/CNN_DSSM.png"><img alt="../../_images/CNN_DSSM.png" src="../../_images/CNN_DSSM.png" style="width: 645.6px; height: 392.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.3 </span><span class="caption-text">The architecture of CNN-DSSM. Each term together with its left and right contextual words are encoded together into term vectors.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-cnndssm" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="transfomer-retrievers-and-rerankers">
<h2><span class="section-number">25.2. </span>Transfomer Retrievers and Rerankers<a class="headerlink" href="#transfomer-retrievers-and-rerankers" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">25.2.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>BERT (Bidirectional Encoder Representations from Transformers) <span id="id9">[<a class="reference internal" href="#id1232" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span> and its transformer variants <span id="id10">[<a class="reference internal" href="#id1460" title="Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. arXiv preprint arXiv:2106.04554, 2021.">LWLQ21</a>]</span> represent the state-of-the-art modeling strategies in a broad range of natural language processing tasks. The application of BERT in information retrieval and ranking was pioneered by <span id="id11">[<a class="reference internal" href="#id1451" title="Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019.">NC19</a>, <a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span>. The fundamental characteristics of BERT architecture is self-attention. By pretraining BERT on large scale text data, BERT encoder can produce contextualized embeddings can better capture semantics of different linguistic units. By adding additional prediction head to the BERT backbone, such BERT encoders can be fine-tuned to retrieval related tasks.</p>
<p>In general, there are two mainstream architectures in using BERT for information retrieval and ranking tasks [<span id="id12">[]</span>].</p>
<ul class="simple">
<li><p>Bi-encoder, also known as dual-encoder or twin-tower models, employ two separate encoders to generate independent embeddings for each input text segment. The relevance label is predicted using similarity scores between the query embedding and the doc embedding.</p></li>
<li><p>Cross-encoder, process both input text segments together within a single encoder. This joint encoding allows the model to capture richer relationships and dependencies between the text segments, leading to higher accuracy in tasks that require a deeper understanding of the semantic similarity or relatedness between the inputs.</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-biencoder-cross-encoder-arch">
<a class="reference internal image-reference" href="../../_images/biencoder_bert_arch.png"><img alt="../../_images/biencoder_bert_arch.png" src="../../_images/biencoder_bert_arch.png" style="width: 887.4px; height: 373.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.4 </span><span class="caption-text">(Left) The Bi-Encoder architecture for document relevance ranking. The query and document inputs are passed to query encoder and document encoder separately. The [CLS] embedding for query encoder and the doc encoder are used to compute similiarity score, as the proxy of relevance score. (Right) The Cross-Encoder architecture for document relevance ranking. The input is the concatenation of the query token sequence and the candidate document token sequence. Once the input sequence is passed through the model, we use the [CLS] embedding as input to a single layer neural network to obtain a posterior probability <span class="math notranslate nohighlight">\(p_{i}\)</span> of the candidate <span class="math notranslate nohighlight">\(d_{i}\)</span> being relevant to query <span class="math notranslate nohighlight">\(q\)</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-biencoder-cross-encoder-arch" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="bi-encoder-retriever">
<h3><span class="section-number">25.2.2. </span>Bi-Encoder Retriever<a class="headerlink" href="#bi-encoder-retriever" title="Link to this heading">#</a></h3>
<p>Using Bi-Encoder as retriever was first explored in passage retrieval <strong>RepBERT</strong> <span id="id13">[<a class="reference internal" href="#id1662" title="Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. Repbert: contextualized text embeddings for first-stage retrieval. arXiv preprint arXiv:2006.15498, 2020.">ZML+20</a>]</span> and OpenDomain QA <strong>DPR</strong> <span id="id14">[<a class="reference internal" href="#id1463" title="Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.">KOuguzM+20</a>]</span>.</p>
<p>In RepBERT, the query encoder and document encoder shares the same weight. The encoding is the mean of the last hidden states of input tokens. In DPR, the query encoder and document encoder are separate encoder, and the text encoding is taking the representation at the [CLS].</p>
<p>The relevance score between a query and a passage is expressed as the similarity between the query and the passage embeddings, given by</p>
<div class="math notranslate nohighlight">
\[\operatorname{sim}(q, p)=E_Q(q)^{\top} E_P(p).\]</div>
<p>Loss Function The goal of training is to make the embedding inner products of relevant pairs of queries and documents larger than those of irrelevant pairs. Let <span class="math notranslate nohighlight">\(\left(q, d_1^{+}, \ldots, d_m^{+}, d_{m+1}^{-}, \ldots, d_n^{-}\right)\)</span>be one instance of the input training batch. The instance contains one query <span class="math notranslate nohighlight">\(q, m\)</span> relevant (positive) documents and <span class="math notranslate nohighlight">\(n-m\)</span> irrelevant (negative) documents. We adopt MultiLabelMarginLoss [16] as the loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(q, d_1^{+}, \ldots, d_m^{+}, d_{m+1}^{-}, \ldots, d_n^{-}\right)=\frac{1}{n} \cdot \sum_{1 \leq i \leq m, m&lt;j \leq n} \max \left(0,1-\left(\operatorname{Rel}\left(q, d_i^{+}\right)-\operatorname{Rel}\left(q, d_j^{-}\right)\right)\right)
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left\langle q_i, p_i^{+}, p_{i, 1}^{-}, \cdots, p_{i, n}^{-}\right\rangle\right\}_{i=1}^m\)</span> be the training data that consists of <span class="math notranslate nohighlight">\(m\)</span> instances. Each instance contains one question <span class="math notranslate nohighlight">\(q_i\)</span> and one relevant (positive) passage <span class="math notranslate nohighlight">\(p_i^{+}\)</span>, along with <span class="math notranslate nohighlight">\(n\)</span> irrelevant (negative) passages <span class="math notranslate nohighlight">\(p_{i, j}^{-}\)</span>. We optimize the loss function as the negative <span class="math notranslate nohighlight">\(\log\)</span> likelihood of the positive passage:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; L\left(q_i, p_i^{+}, p_{i, 1}^{-}, \cdots, p_{i, n}^{-}\right) \\
= &amp; -\log \frac{e^{\operatorname{sim}\left(q_i, p_i^{+}\right)}}{e^{\operatorname{sim}\left(q_i, p_i^{+}\right)}+\sum_{j=1}^n e^{\operatorname{sim}\left(q_i, p_{i, j}^{-}\right)}}
\end{aligned}
\end{split}\]</div>
</section>
<section id="cross-encoder-for-point-wise-ranking">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-monobert"></span><h3><span class="section-number">25.2.3. </span>Cross-Encoder For Point-wise Ranking<a class="headerlink" href="#cross-encoder-for-point-wise-ranking" title="Link to this heading">#</a></h3>
<p>The first application of BERT in document retrieval is using BERT as a cross encoder, where the query token sequence and the document token sequence are concatenated via [SEP] token and encoded together. This architecture [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-biencoder-cross-encoder-arch"><span class="std std-numref">Fig. 25.4</span></a>], called <strong>mono-BERT</strong>, was first proposed by <span id="id15">[<a class="reference internal" href="#id1451" title="Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019.">NC19</a>, <a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span>.</p>
<p>To meet the token sequence length constraint of a BERT encoder (e.g., 512), we might need to truncate the query (e.g, not greater than 64 tokens) and the candidate document token sequence such that the total concatenated token sequence have a maximum length of 512 tokens.</p>
<p>Once the input sequence is passed through the model, we use the [CLS] embedding as input to a single layer neural network to obtain a posterior <strong>binary classification probability</strong> <span class="math notranslate nohighlight">\(p_{i}\)</span> of the candidate <span class="math notranslate nohighlight">\(d_{i}\)</span> being relevant to query <span class="math notranslate nohighlight">\(q\)</span>. The posterior probability can be used to rank documents.</p>
<p>The training data can be represented by a collections of triplets <span class="math notranslate nohighlight">\((q, J_P^q, J_N^q), q\in Q\)</span>, where <span class="math notranslate nohighlight">\(Q\)</span> is the set of queries, <span class="math notranslate nohighlight">\(J_{P}^q\)</span> is the set of indexes of the relevant candidates associated with query <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(J_{N}^q\)</span> is the set of indexes of the nonrelevant candidates.</p>
<p>The encoder can be fine-tuned using cross-entropy loss:</p>
<div class="math notranslate nohighlight">
\[
L_{\text {mono-BERT}}=-\sum_{q\in Q}( \sum_{j \in J_{P}^q} \log \left(p_{j}\right)-\sum_{j \in J_{N}^q} \log \left(1-p_{j}\right) )
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_j\)</span> is the probability that <span class="math notranslate nohighlight">\(q\)</span> is relevant to passage <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>During training, each batch can consist of a query and its candidate documents (include both positive and negative) produced by previous retrieval layers.</p>
</section>
<section id="duo-bert-for-pairwise-ranking">
<h3><span class="section-number">25.2.4. </span>Duo-BERT For Pairwise Ranking<a class="headerlink" href="#duo-bert-for-pairwise-ranking" title="Link to this heading">#</a></h3>
<p>Mono-BERT can be characterized as a <em>pointwise</em> approach for ranking. Within the <em>framework of learning to rank</em>, <span id="id16">[<a class="reference internal" href="#id1451" title="Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019.">NC19</a>, <a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span> also proposed duo-BERT, which is  a <em>pairwise</em> ranking approach. In this pairwise approach, the duo-BERT ranker model estimates the probability <span class="math notranslate nohighlight">\(p_{i, j}\)</span> of the candidate <span class="math notranslate nohighlight">\(d_{i}\)</span> being more relevant than <span class="math notranslate nohighlight">\(d_{j}\)</span> with respect to query <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p>The duo-BERT architecture [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-duobertarch"><span class="std std-numref">Fig. 25.5</span></a>] takes the concatenation of the query <span class="math notranslate nohighlight">\(q\)</span>, the candidate document <span class="math notranslate nohighlight">\(d_{i}\)</span>, and the candidate document <span class="math notranslate nohighlight">\(d_{j}\)</span> as the input. We also need to truncate the query, candidates <span class="math notranslate nohighlight">\(d_{i}\)</span> and <span class="math notranslate nohighlight">\(d_{j}\)</span> to proper lengths (e.g., 62 , 223 , and 223 tokens, respectively), so the entire sequence will have at most 512 tokens.</p>
<p>Once the input sequence is passed through the model, we use the [CLS] embedding as input to a single layer neural network to obtain a posterior probability <span class="math notranslate nohighlight">\(p_{i,j}\)</span>. This posterior probability can be used to rank documents <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> with respect to each other. If there are <span class="math notranslate nohighlight">\(k\)</span> candidates for query <span class="math notranslate nohighlight">\(q\)</span>, there will be <span class="math notranslate nohighlight">\(k(k-1)\)</span> passes to compute all the pairwise probabilities.</p>
<p>The model can be fine-tune using with the following loss per query.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
L_{\text {duo }}=&amp;-\sum_{i \in J_{P}, j \in J_{N}} \log \left(p_{i, j}\right) \\
	&amp;-\sum_{i \in J_{N}, j \in J_{P}} \log \left(1-p_{i, j}\right)
\end{align*}\end{split}\]</div>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-duobertarch">
<a class="reference internal image-reference" href="../../_images/duo_bert_arch.png"><img alt="../../_images/duo_bert_arch.png" src="../../_images/duo_bert_arch.png" style="width: 629.1px; height: 319.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.5 </span><span class="caption-text">The duo-BERT architecture takes the concatenation of the query and two candidate documents as the input. Once the input sequence is passed through the model, we use the [CLS] embedding as input to a single layer neural network to obtain a posterior probability that the first document is more relevant than the second document.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-duobertarch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>At inference time, the obtained <span class="math notranslate nohighlight">\(k(k -1)\)</span> pairwise probabilities are used to produce the final document relevance ranking given the query. Authors in <span id="id17">[<a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span> investigate five different aggregation methods (SUM, BINARY, MIN, MAX, and SAMPLE) to produce the final ranking score.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\operatorname{SUM}:  s_{i} &amp;=\sum_{j \in J_{i}} p_{i, j} \\
	\operatorname{BINARY}: s_{i} &amp;=\sum_{j \in J_{i}} \mathbf{1}_{p_{i, j} &gt; 0.5} \\
	\operatorname{MIN}: s_{i}  &amp;=\min _{j \in J_{i}} p_{i, j} \\
	\operatorname{MAX}: s_{i} &amp;=\max _{j \in J_{i}} p_{i, j} \\
	\operatorname{SAMPLE}: s_{i}&amp;=\sum_{j \in J_{i}(m)} p_{i, j}
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(J_i = \{1 &lt;= j &lt;= k, j\neq i\}\)</span> and <span class="math notranslate nohighlight">\(J_i(m)\)</span> is <span class="math notranslate nohighlight">\(m\)</span> randomly sampled elements from <span class="math notranslate nohighlight">\(J_i\)</span>.</p>
<p>The SUM method measures the pairwise agreement that candidate <span class="math notranslate nohighlight">\(d_{i}\)</span> is more relevant than the rest of the candidates <span class="math notranslate nohighlight">\(\left\{d_{j}\right\}_{j \neq i^{*}}\)</span>. The BINARY method resembles majority vote. The Min (MAX) method measures the relevance of <span class="math notranslate nohighlight">\(d_{i}\)</span> only against its strongest (weakest) competitor. The SAMPLE method aims to decrease the high inference costs of pairwise computations via sampling. Comparison studies using MS MARCO dataset suggest that SUM and BINARY give the best results.</p>
</section>
<section id="multistage-retrieval-and-ranking-pipeline">
<h3><span class="section-number">25.2.5. </span>Multistage Retrieval and Ranking Pipeline<a class="headerlink" href="#multistage-retrieval-and-ranking-pipeline" title="Link to this heading">#</a></h3>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multistageretrievalrankingbert">
<a class="reference internal image-reference" href="../../_images/multistage_retrieval_ranking_bert.png"><img alt="../../_images/multistage_retrieval_ranking_bert.png" src="../../_images/multistage_retrieval_ranking_bert.png" style="width: 535.6px; height: 229.20000000000002px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.6 </span><span class="caption-text">Illustration of a three-stage retrieval-ranking architecture using BM25, monoBERT and duoBERT. Image from <span id="id18">[<a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multistageretrievalrankingbert" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>With BERT variants of different ranking capability, we can construct a multi-stage ranking architecture to select a handful of most relevant document from a large collection of candidate documents given a query. Consider a typical architecture comprising a number of stages from <span class="math notranslate nohighlight">\(H_0\)</span> ot <span class="math notranslate nohighlight">\(H_N\)</span>. <span class="math notranslate nohighlight">\(H_0\)</span> is a exact-term matching stage using from an inverted index. <span class="math notranslate nohighlight">\(H_0\)</span> stage take billion-scale document as input and output thousands of candidates <span class="math notranslate nohighlight">\(R_0\)</span>. For stages from <span class="math notranslate nohighlight">\(H_1\)</span> to <span class="math notranslate nohighlight">\(H_N\)</span>, each stage <span class="math notranslate nohighlight">\(H_{n}\)</span> receives a ranked list <span class="math notranslate nohighlight">\(R_{n-1}\)</span> of candidates from the previous stage and output candidate list <span class="math notranslate nohighlight">\(R_n\)</span>. Typically <span class="math notranslate nohighlight">\(|R_n| \ll |R_{n-1}|\)</span> to enable efficient retrieval.</p>
<p>An example three-stage retrieval-ranking system is shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multistageretrievalrankingbert"><span class="std std-numref">Fig. 25.6</span></a>. In the first stage <span class="math notranslate nohighlight">\(H_{0}\)</span>, given a query <span class="math notranslate nohighlight">\(q\)</span>, the top candidate documents <span class="math notranslate nohighlight">\(R_{0}\)</span> are retrieved using BM25. In the second stage <span class="math notranslate nohighlight">\(H_{1}\)</span>, monoBERT produces a relevance score <span class="math notranslate nohighlight">\(s_{i}\)</span> for each pair of query <span class="math notranslate nohighlight">\(q\)</span> and candidate <span class="math notranslate nohighlight">\(d_{i} \in R_{0}.\)</span> The top candidates with respect to these relevance scores are passed to the last stage <span class="math notranslate nohighlight">\(H_{2}\)</span>, in which duoBERT computes a relevance score <span class="math notranslate nohighlight">\(p_{i, j}\)</span> for each triple <span class="math notranslate nohighlight">\(\left(q, d_{i}, d_{j}\right)\)</span>. The final list of candidates <span class="math notranslate nohighlight">\(R_{2}\)</span> is formed by re-ranking the candidates according to these scores .</p>
<p><strong>Evaluation.</strong> Different multistage architecture configurations are evaluated using the MS MARCO dataset. We have following observations:</p>
<ul class="simple">
<li><p>Using a single stage of BM25 yields the worst performance.</p></li>
<li><p>Adding an additional monoBERT significantly improve the performance over the single BM25 stage architecture.</p></li>
<li><p>Adding the third component duoBERT only yields a diminishing gain.</p></li>
</ul>
<p>Further, the author found that	employing the technique of Target Corpus Pre-training (TCP)\ gives additional performance gain. Specifically, the BERT backbone will undergo a two-phase pre-training. In the first phase, the model is pre-trained using the original setup, that is Wikipedia (2.5B words) and the Toronto Book corpus ( 0.8B words) for one million iterations. In the second phase, the model is further pre-trained on the MS MARCO corpus.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Dev</p></th>
<th class="head"><p>Eval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Anserini (BM25)</p></td>
<td><p>18.7</p></td>
<td><p>19.0</p></td>
</tr>
<tr class="row-odd"><td><p>+ monoBERT</p></td>
<td><p>37.2</p></td>
<td><p>36.5</p></td>
</tr>
<tr class="row-even"><td><p>+ monoBERT + duoBERT<sub>MAX</sub></p></td>
<td><p>32.6</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>+ monoBERT + duoBERT<sub>MIN</sub></p></td>
<td><p>37.9</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>+ monoBERT + duoBERT<sub>SUM</sub></p></td>
<td><p>38.2</p></td>
<td><p>37.0</p></td>
</tr>
<tr class="row-odd"><td><p>+ monoBERT + duoBERT<sub>BINARY</sub></p></td>
<td><p>38.3</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>+ monoBERT + duoBERT<sub>SUM</sub> + TCP</p></td>
<td><p>39.0</p></td>
<td><p>37.9</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="dc-bert">
<h3><span class="section-number">25.2.6. </span>DC-BERT<a class="headerlink" href="#dc-bert" title="Link to this heading">#</a></h3>
<p>One way to improve the computational efficiency of cross-encoder is to employ bi-encoders for partial separate encoding and then employ an additional shallow module for cross encoding. One example is the architecture shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dcbert"><span class="std std-numref">Fig. 25.7</span></a>, which is called <strong>DC-BERT</strong> and proposed in <span id="id19">[<a class="reference internal" href="#id1476" title="Ping Nie, Yuyu Zhang, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. Dc-bert: decoupling question and document for efficient contextual encoding. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, 1829–1832. 2020.">NZG+20</a>]</span>. The overall architecture of DC-BERT  consists of a dual-BERT component for decoupled encoding, a Transformer component for question-document interactions, and a binary classifier component for document relevance scoring.</p>
<p>The document encoder can be run offline to pre-encodes all documents and caches all term representations. During online inference, we only need to run the BERT query encodes online. Then the obtained contextual term representations are fed into high-layer Transformer interaction layer.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dcbert">
<a class="reference internal image-reference" href="../../_images/DC_bert.png"><img alt="../../_images/DC_bert.png" src="../../_images/DC_bert.png" style="width: 590.4px; height: 501.29999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.7 </span><span class="caption-text">The overall architecture of DC-BERT [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dcbert"><span class="std std-numref">Fig. 25.7</span></a>] consists of a dual-BERT component for decoupled encoding, a Transformer component for question-document interactions, and a classifier component for document relevance scoring.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-dcbert" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Dual-BERT component</strong>. DC-BERT contains two pre-trained BERT models to independently encode the question and each retrieved document. During training, the parameters of both BERT models are fine-tuned to optimize the learning objective.</p>
<p><strong>Transformer component.</strong> The dual-BERT components produce contextualized embeddings for both the query token sequence and the document token sequence. Then we add global position embeddings <span class="math notranslate nohighlight">\(\mathbf{E}_{P_{i}} \in \mathbb{R}^{d}\)</span> and segment embedding again to re-encode the position information and segment information (i.e., query vs document). Both the global position and segment embeddings are initialized from pre-trained BERT, and will be fine-tuned. The number of Transformer layers <span class="math notranslate nohighlight">\(K\)</span> is configurable to trade-off between the model capacity and efficiency. The Transformer layers are initialized by the last <span class="math notranslate nohighlight">\(K\)</span> layers of pre-trained BERT, and are fine-tuned during the training.</p>
<p><strong>Classifier component.</strong> The two CLS token output from the Transformer layers will be fed into a linear binary classifier to predict whether the retrieved document is relevant to the query. Following previous work (Das et al., 2019; Htut et al., 2018; Lin et al., 2018), we employ paragraph-level distant supervision to gather labels for training the classifier, where a paragraph that contains the exact ground truth answer span is labeled as a positive example. We parameterize the binary classifier as a MLP layer on top of the Transformer layers:</p>
<div class="math notranslate nohighlight">
\[
p\left(q_{i}, d_{j}\right)=\sigma\left(\operatorname{Linear}\left(\left[o_{[C L S]} ; o_{[C L S]}^{\prime}\right]\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(q_{i}, d_{j}\right)\)</span> is a pair of question and retrieved document, and <span class="math notranslate nohighlight">\(o_{[C L S]}\)</span> and <span class="math notranslate nohighlight">\(o_{[C L S]}^{\prime}\)</span> are the Transformer output encodings of the [CLS] token of the question and the document, respectively. The MLP parameters are updated by minimizing the cross-entropy loss.</p>
<p>DC-BERT uses one Transformer layer for question-document interactions. Quantized BERT is a 8bit-Integer model. DistilBERT is a compact BERT model with 2 Transformer layers.</p>
<p>We first compare the retriever speed. DC-BERT achieves over 10x speedup over the BERT-base retriever, which demonstrates the efficiency of this method. Quantized BERT has the same model architecture as BERT-base, leading to the minimal speedup. DistilBERT achieves about 6x speedup with only 2 Transformer layers, while BERT-base uses 12 Transformer layers.</p>
<p>With a 10x speedup, DC-BERT still achieves similar retrieval performance compared to BERT- base on both datasets. At the cost of little speedup, Quantized BERT also works well in ranking documents. DistilBERT performs significantly worse than BERT-base, which shows the limitation of the distilled BERT model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>SQuAD</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Natural Questions</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td><p>PTB&#64;10</p></td>
<td><p>Speedup</p></td>
<td><p>P&#64;10</p></td>
<td><p>Speedup</p></td>
</tr>
<tr class="row-odd"><td><p>BERT-base</p></td>
<td><p>71.5</p></td>
<td><p>1.0x</p></td>
<td><p>65.0</p></td>
<td><p>1.0x</p></td>
</tr>
<tr class="row-even"><td><p>Quantized BERT</p></td>
<td><p>68.0</p></td>
<td><p>1.1x</p></td>
<td><p>64.3</p></td>
<td><p>1.1x</p></td>
</tr>
<tr class="row-odd"><td><p>DistilBERT</p></td>
<td><p>56.4</p></td>
<td><p>5.7x</p></td>
<td><p>60.6</p></td>
<td><p>5.7x</p></td>
</tr>
<tr class="row-even"><td><p>DC-BERT</p></td>
<td><p>70.1</p></td>
<td><p>10.3x</p></td>
<td><p>63.5</p></td>
<td><p>10.3x</p></td>
</tr>
</tbody>
</table>
</div>
<p>To further investigate the impact of our model architecture design, we compare the performance of DC-BERT and its variants, including 1) DC-BERT-Linear, which uses linear layers instead of Transformers for interaction; and 2) DC-BERT-LSTM, which uses LSTM and bi- linear layers for interactions following previous work (Min et al., 2018). We report the results in Table 3. Due to the simplistic architecture of the interaction layers, DC-BERT-Linear achieves the best speedup but has significant performance drop, while DC-BERT-LSTM achieves slightly worse performance and speedup than DC-BERT.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Retriever Model</p></th>
<th class="head"><p>Retriever P&#64;10</p></th>
<th class="head"><p>Retriever Speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DC-BERT-Linear</p></td>
<td><p>57.3</p></td>
<td><p>43.6x</p></td>
</tr>
<tr class="row-odd"><td><p>DC-BERT-LSTM</p></td>
<td><p>61.5</p></td>
<td><p>8.2x</p></td>
</tr>
<tr class="row-even"><td><p>DC-BERT</p></td>
<td><p>63.5</p></td>
<td><p>10.3x</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="multi-attribute-and-multi-task-modeling">
<h3><span class="section-number">25.2.7. </span>Multi-Attribute and Multi-task Modeling<a class="headerlink" href="#multi-attribute-and-multi-task-modeling" title="Link to this heading">#</a></h3>
<p>We can extend cross-encoder to take into multiple-attributes from query side and document side, as well as generating multiple predictive outputs for different tasks [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multitask-multiattribute"><span class="std std-numref">Fig. 25.8</span></a>].</p>
<p>For example, query side attributes could include</p>
<ul class="simple">
<li><p>Query text</p></li>
<li><p>Query’s language (produced by a cheapter language detection model)</p></li>
<li><p>User’s location and region</p></li>
<li><p>Other query side signals (e.g., key concept groups in the query, document signals from historical queries)
Document side attributes could include</p></li>
<li><p>Organic contents with semantic markers (e.g., [T] for Title)</p></li>
<li><p>Other derived signals from documents (e.g., puesedo queries, historical queries, etc.)
Other high level signals suitable late stage fusion</p></li>
<li><p>Document refreshness attribute (for intent to search latest news)</p></li>
<li><p>Document spamness attributes</p></li>
</ul>
<p>After feature fusion (e.g., via concatination), we can separate MLP head for different tasks</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multitask-multiattribute">
<a class="reference internal image-reference" href="../../_images/multitask_multiattribute_arch.png"><img alt="../../_images/multitask_multiattribute_arch.png" src="../../_images/multitask_multiattribute_arch.png" style="width: 747.9px; height: 389.7px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.8 </span><span class="caption-text">An representative cross-encoder that is extended to take into account multiple-attributes from query side and document side. There are multiple outputs for <strong>multi-tasking</strong>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-multitask-multiattribute" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="multi-vector-retrievers">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-multivector"></span><h2><span class="section-number">25.3. </span>Multi-Vector Retrievers<a class="headerlink" href="#multi-vector-retrievers" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3><span class="section-number">25.3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<p>In classic representation-based learning for semantic retrieval [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-classicrepresentationlearning"><span class="std std-numref">Section 25.1.3</span></a>], we use two encoders (i.e., bi-encoders) to separately encoder a query and a candidate document into two dense vectors in the embedding space, and then a score function, such as cosine similarity, to produce the final relevance score. In this paradigm, there is a single global, static representation for each query and each document. Specifically, the document’s embedding remain the same regardless of the document length, the content structure of document (e.g., multiple topics) and the variation of queries that are relevant to the document. It is very common that a document with hundreds of tokens might contain several distinct subtopics, some important semantic information might be easily missed or biased by each other when compressing a document into a dense vector.  As such, this simple bi-encoder structure may cause serious information loss when used to encode documents. <sup>[^2]</sup></p>
<p>On the other hand,  cross-encoders based on BERT variants [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-classicrepresentationlearning"><span class="std std-numref">Section 25.1.3</span></a>] utilize multiple self-attention layers not only to extract contextualized features from queries and documents but also capture the interactions between them. Cross-encoders only produce intermediate representations that take a pair of query and document as the joint input. While BERT-based cross-encoders brought significant performance gain,  they are computationally prohibitive and impractical for online inference.</p>
<p>In this section, we focus on different strategies <span id="id20">[<a class="reference internal" href="#id1452" title="Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019.">HSLW19</a>, <a class="reference internal" href="#id1453" title="Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329–345, 2021.">LETC21</a>, <a class="reference internal" href="#id1454" title="Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. Improving document representations by generating pseudo query embeddings for dense retrieval. arXiv preprint arXiv:2105.03599, 2021.">TSJ+21</a>]</span> to encode documents by multi-vector representations, which enriches the single vector representation produced by a bi-encoder. With additional computational overhead, these strategies can gain much improvement of the encoding quality while retaining the fast retrieval strengths of Bi-encoder.</p>
</section>
<section id="colbert">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-colbert"></span><h3><span class="section-number">25.3.2. </span>ColBERT<a class="headerlink" href="#colbert" title="Link to this heading">#</a></h3>
<section id="id21">
<h4><span class="section-number">25.3.2.1. </span>Overview<a class="headerlink" href="#id21" title="Link to this heading">#</a></h4>
<p>ColBERT <span id="id22">[<a class="reference internal" href="#id1484" title="Omar Khattab and Matei Zaharia. Colbert: efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 39–48. 2020.">KZ20</a>]</span> is another example architecture that consists of an early separate encoding phase and a late interaction phase, as shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-colbert"><span class="std std-numref">Fig. 25.9</span></a>. ColBERT employs a single BERT model for both query and document encoders but distinguish input sequences that correspond to queries and documents by prepending a special token [Q] to queries and another token [D] to documents.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-colbert">
<a class="reference internal image-reference" href="../../_images/Col_bert.png"><img alt="../../_images/Col_bert.png" src="../../_images/Col_bert.png" style="width: 581.4px; height: 325.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.9 </span><span class="caption-text">The architecture of ColBERT, which consists of an early separate encoding phase and a late interaction phase.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-colbert" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="encoding">
<h4><span class="section-number">25.3.2.2. </span>Encoding<a class="headerlink" href="#encoding" title="Link to this heading">#</a></h4>
<p>The query Encoder take query tokens as the input. Note that if a query is shorter than a pre-defined number <span class="math notranslate nohighlight">\(N_q\)</span>, it will be padded with BERT’s special [mask] tokens up to length <span class="math notranslate nohighlight">\(N_q\)</span>; otherwise, only the first <span class="math notranslate nohighlight">\(N_q\)</span> tokens will be kept. It is found that the mask token padding serves as some sort of query augmentation and brings perform gain. In additional, a [Q] token is placed right after BERT’s sequence start token [CLS]. The query encoder then computes a contextualized representation for the query tokens.</p>
<p>The document encoder has a very similar architecture. A [D] token is placed right after BERT’s sequence start token [CLS]. Note that after passing through the encoder, embeddings correponding to punctuation symbols are filtered out.</p>
<p>Given BERT’s representation of each token, an additional linear layer with no activation is used to reduce the dimensionality reduction. The reduced dimensionality <span class="math notranslate nohighlight">\(m\)</span> is set much smaller than BERT’s fixed hidden dimension.</p>
<p>Finally, given <span class="math notranslate nohighlight">\(q= q_{1} \ldots q_{l}\)</span> and <span class="math notranslate nohighlight">\(d=d_{1} \ldots d_{n}\)</span>, an additional CNN layer is used to allow each embedding vector to interact with its neighbor, yielding  the bags of embeddings <span class="math notranslate nohighlight">\(E_{q}\)</span> and <span class="math notranslate nohighlight">\(E_{d}\)</span> in the following manner.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;E_{q}:=\operatorname{Normalize}\left(\operatorname{Linear}\left(\operatorname{BERT}\left([Q] q_{0} q_{1} \ldots q_{l} \# \# \ldots \#\right)\right)\right) \\
	&amp;E_{d}:=\operatorname{Filter}\left(\operatorname{Normalize}\left(\operatorname{Linear}\left(\operatorname{BERT}\left([D] d_{0} d_{1} \ldots d_{n} \right)\right)\right)\right)
\end{align*}\end{split}\]</div>
<p>Here # refers to the [Mask] tokens and <span class="math notranslate nohighlight">\(\operatorname{Normalize}\)</span> denotes <span class="math notranslate nohighlight">\(L_2\)</span> length normalization.</p>
</section>
<section id="late-interaction">
<h4><span class="section-number">25.3.2.3. </span>Late Interaction<a class="headerlink" href="#late-interaction" title="Link to this heading">#</a></h4>
<p>In the late interaction phase, every query embedding interacts with all document embeddings via a MaxSimilarity operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms.</p>
<p>Formally, the final similarity score between the <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-eq-colbert-score">
<span class="eqno">(25.1)<a class="headerlink" href="#equation-ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-eq-colbert-score" title="Link to this equation">#</a></span>\[
S_{q, d} =\sum_{i \in I_q} \max _{j \in I_d} E_{q_{i}} \cdot E_{d_{j}}^{T},
\]</div>
<p>where <span class="math notranslate nohighlight">\(I_q = \{1,...,l\}\)</span>, <span class="math notranslate nohighlight">\(I_d = \{1, ..., n\}\)</span> are the index sets for query token embeddings and document token embeddings, respectively.
ColBERT is differentiable end-to-end and we can fine-tune the BERT encoders and train from scratch the additional parameters (i.e., the linear layer and the <span class="math notranslate nohighlight">\([Q]\)</span> and <span class="math notranslate nohighlight">\([D]\)</span> markers’ embeddings). Notice that the final aggregation interaction mechanism has no trainable parameters.</p>
</section>
<section id="retrieval-re-ranking">
<h4><span class="section-number">25.3.2.4. </span>Retrieval &amp; Re-ranking<a class="headerlink" href="#retrieval-re-ranking" title="Link to this heading">#</a></h4>
<p>The retrieval and re-ranking using ColBert consists of three steps:</p>
<ul class="simple">
<li><p><strong>Token retrieving</strong> doc token candidates from index via query token embedding, with doc token’s source canddiate being the doc candidates,</p></li>
<li><p><strong>Gathering</strong> all token vectors for doc candidates,</p></li>
<li><p><strong>Scoring</strong> the candidate documents using all its token embeddings</p></li>
</ul>
<p>The first retrieval step further consists of two-steps:</p>
<ul class="simple">
<li><p><strong>each query token</strong> (out of <span class="math notranslate nohighlight">\(N_q\)</span> tokens) first retrieves top <span class="math notranslate nohighlight">\(k'\)</span> (e.g., <span class="math notranslate nohighlight">\(k'=k/2\)</span>) document IDs using approximate nearest neighbor (ANN) search. See more in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-ann-search"><span class="std std-ref">Approximate Nearest Neighbor Search</span></a>.</p></li>
<li><p><strong>Merge</strong> <span class="math notranslate nohighlight">\(N_q \times k'\)</span> documents ID to get <span class="math notranslate nohighlight">\(K\)</span> unique documents as the retrieval result.</p></li>
</ul>
<p>After retrieving top-<span class="math notranslate nohighlight">\(k\)</span> documents given a query <span class="math notranslate nohighlight">\(q\)</span>, the next step is score these <span class="math notranslate nohighlight">\(k\)</span> documents as the **re-ranking **step. Specifically, with a query <span class="math notranslate nohighlight">\(q\)</span> represented by a bag contextualized embeddings <span class="math notranslate nohighlight">\(E_q\)</span> (a 2D matrix) and we further gather the document representations into a 3-dimensional tensor <span class="math notranslate nohighlight">\(D\)</span> consisting of <span class="math notranslate nohighlight">\(k\)</span> document matrices. For each query and document pair, we compute its score according to <a class="reference internal" href="#equation-ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-eq-colbert-score">(25.1)</a>.</p>
</section>
<section id="evaluation">
<h4><span class="section-number">25.3.2.5. </span>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h4>
<p>The <strong>retrieval</strong> performance of ColBERT is evaluated on MS MARCO dataset. Compared with traditional exact term matching retrieval BM25, ColBERT has shortcomings in terms of latency but MRR is significantly better.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1664">
<caption><span class="caption-number">Table 25.2 </span><span class="caption-text">E2E retrieval results on MS MARCO</span><a class="headerlink" href="#id1664" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>MRR&#64;10(Dev)</p></th>
<th class="head"><p>MRR&#64;10 (Local Eval)</p></th>
<th class="head"><p>Latency (ms)</p></th>
<th class="head"><p>Recall&#64;50</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BM25 (Anserini)</p></td>
<td><p>18.7</p></td>
<td><p>19.5</p></td>
<td><p>62</p></td>
<td><p>59.2</p></td>
</tr>
<tr class="row-odd"><td><p>doc2query</p></td>
<td><p>21.5</p></td>
<td><p>22.8</p></td>
<td><p>85</p></td>
<td><p>64.4</p></td>
</tr>
<tr class="row-even"><td><p>DeepCT</p></td>
<td><p>24.3</p></td>
<td><p>-</p></td>
<td><p>62 (est.)</p></td>
<td><p>69[2]</p></td>
</tr>
<tr class="row-odd"><td><p>docTTTTTquery</p></td>
<td><p>27.7</p></td>
<td><p>28.4</p></td>
<td><p>87</p></td>
<td><p>75.6</p></td>
</tr>
<tr class="row-even"><td><p>ColBERT L2 (BM25 + re-rank)</p></td>
<td><p>34.8</p></td>
<td><p>36.4</p></td>
<td><p>-</p></td>
<td><p>75.3</p></td>
</tr>
<tr class="row-odd"><td><p>ColBERTL2 (retrieval &amp; re-rank)</p></td>
<td><p>36.0</p></td>
<td><p>36.7</p></td>
<td><p>458</p></td>
<td><p>82.9</p></td>
</tr>
</tbody>
</table>
</div>
<p>Similarly, we can evaluate ColBERT’s re-ranking performance against some strong baselines, such as BERT cross encoders <span id="id23">[<a class="reference internal" href="#id1451" title="Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019.">NC19</a>, <a class="reference internal" href="#id1474" title="Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019.">NYCL19</a>]</span>. ColBERT has demonstrated significant benefits in reducing latency with little cost of re-ranking performance.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1665">
<caption><span class="caption-number">Table 25.3 </span><span class="caption-text">Re-ranking results on MS MARCO using candidates produced by BM25.</span><a class="headerlink" href="#id1665" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>MRR&#64;10 (Dev)</p></th>
<th class="head"><p>MRR&#64;10 (Eval)</p></th>
<th class="head"><p>Re-ranking Latency (ms)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BM25 (official)</p></td>
<td><p>16.7</p></td>
<td><p>16.5</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>KNRM</p></td>
<td><p>19.8</p></td>
<td><p>19.8</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>Duet</p></td>
<td><p>24.3</p></td>
<td><p>24.5</p></td>
<td><p>22</p></td>
</tr>
<tr class="row-odd"><td><p>fastText+ConvKNRM</p></td>
<td><p>29.0</p></td>
<td><p>27.7</p></td>
<td><p>28</p></td>
</tr>
<tr class="row-even"><td><p>BERT base</p></td>
<td><p>34.7</p></td>
<td><p>-</p></td>
<td><p>10,700</p></td>
</tr>
<tr class="row-odd"><td><p>BERT large</p></td>
<td><p>36.5</p></td>
<td><p>35.9</p></td>
<td><p>32,900</p></td>
</tr>
<tr class="row-even"><td><p>ColBERT (over BERT base)</p></td>
<td><p>34.9</p></td>
<td><p>34.9</p></td>
<td><p>61</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="semantic-clusters-as-pseudo-query-embeddings">
<h3><span class="section-number">25.3.3. </span>Semantic Clusters As Pseudo Query Embeddings<a class="headerlink" href="#semantic-clusters-as-pseudo-query-embeddings" title="Link to this heading">#</a></h3>
<p>The primary limitation of Bi-encoder is information loss when we condense the document into a query agnostic dense vector representation. Authors in <span id="id24">[<a class="reference internal" href="#id1454" title="Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. Improving document representations by generating pseudo query embeddings for dense retrieval. arXiv preprint arXiv:2105.03599, 2021.">TSJ+21</a>]</span> proposed the idea of representing a document by its semantic salient fragments [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-pseudoqueryembeddings"><span class="std std-numref">Fig. 25.10</span></a>]. These semantic fragments can be modeled by token embedding vector clusters in the embedding space. By performing clustering algorithms (e.g., k-means) on token embeddings, the generated centroids can be used as a document’s multi-vector presentation. Another interpretation is that these centroids can be viewed as multiple potential queries corresponding to the input document; as such, we can call them <em>pseudo query embeddings</em>.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-pseudoqueryembeddings">
<a class="reference internal image-reference" href="../../_images/pseudo_query_embeddings.png"><img alt="../../_images/pseudo_query_embeddings.png" src="../../_images/pseudo_query_embeddings.png" style="width: 580.5px; height: 469.79999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.10 </span><span class="caption-text">Deriving semantic clusters by clustering token-level embedding vectors. Semantic cluster centroids are used as multi-vector document representation, or as pseudo query embeddings. The final relevance score between a query and a document is computed using attententive pooling of centroid cluster and doc product.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-pseudoqueryembeddings" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There are a couple of steps to calculate the final relevance score between a query and a document. First, we encode the query <span class="math notranslate nohighlight">\(q\)</span> into a dense vector <span class="math notranslate nohighlight">\(e_q\)</span> (via the CLS token embedding) and the document <span class="math notranslate nohighlight">\(d\)</span> into multiple vectors via token level encoding and clustering <span class="math notranslate nohighlight">\(c_1,...,c_K\)</span>. Second, the query-conditional document representation <span class="math notranslate nohighlight">\(e_d\)</span> is obtained by attending to the centroids using <span class="math notranslate nohighlight">\(e_q\)</span> as the key. Finally, we can compute the similarity score via dot product between <span class="math notranslate nohighlight">\(e_q\)</span> and <span class="math notranslate nohighlight">\(e_d\)</span>.</p>
<p>In summary, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
a_{j}&amp;=\operatorname{Softmax}\left(e_{q} \cdot c_{j}\right) \\
	e_{d}&amp;=\sum_{j=1}^{k} a_{j} c_{j} \\
	y &amp;=e_{q} \cdot e_{d}
\end{align*}\end{split}\]</div>
<p>In practice, we can save the centroid embeddings in memory and retrieve them using the real queries.</p>
</section>
<section id="token-level-representation-and-retrieval-colbert">
<h3><span class="section-number">25.3.4. </span>Token-level Representation and Retrieval (ColBert)<a class="headerlink" href="#token-level-representation-and-retrieval-colbert" title="Link to this heading">#</a></h3>
<p>To enrich the representations of the documents produced by Bi-encoder, some researchers extend the original Bi-encoder by employing more delicate structures like later-interaction.</p>
<p>ColBERT [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-colbert"><span class="std std-numref">Section 25.3.2</span></a>] can be viewed a token-level multi-vector representation encoder for both queries and documents. Token-level representations for documents can be pre-computed offline. During online inference, late interactions of the query’s multi-vectors representation and the document’s  multi-vectors representation are used to  improve the robustness of dense retrieval, as compared to inner products of single-vector representations. Specifically,</p>
<p>Formally, given <span class="math notranslate nohighlight">\(q= q_{1} \ldots q_{l}\)</span> and <span class="math notranslate nohighlight">\(d=d_{1} \ldots d_{n}\)</span> and their token level embeddings <span class="math notranslate nohighlight">\(\{E_{q_1},\ldots E_{q_l}\}\)</span> and <span class="math notranslate nohighlight">\(\{E_{d_1},...,E_{d_n}\}\)</span> and the final similarity score between the <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
S_{q, d} =\sum_{i \in I_q} \max _{j \in I_d} E_{q_{i}} \cdot E_{d_{j}}^{T},
\]</div>
<p>where <span class="math notranslate nohighlight">\(I_q = \{1,...,l\}\)</span>, <span class="math notranslate nohighlight">\(I_d = \{1, ..., n\}\)</span> are the index sets for query token embeddings and document token embeddings, respectively.</p>
<p>While this method has shown signficant improvement over bi-encoder methods, it has a main disadvantage of high storage requirements. For example, ColBERT requires storing all the WordPiece token vectors of each text in the corpus.</p>
</section>
<section id="colbert-v2">
<h3><span class="section-number">25.3.5. </span>Colbert v2<a class="headerlink" href="#colbert-v2" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="ranker-training">
<h2><span class="section-number">25.4. </span>Ranker Training<a class="headerlink" href="#ranker-training" title="Link to this heading">#</a></h2>
<section id="id25">
<h3><span class="section-number">25.4.1. </span>Overview<a class="headerlink" href="#id25" title="Link to this heading">#</a></h3>
<p>Unlike in classification or regression, the main goal of a ranker<span id="id26">[<a class="reference internal" href="#id1568" title="Qingyao Ai, Xuanhui Wang, Sebastian Bruch, Nadav Golbandi, Michael Bendersky, and Marc Najork. Learning groupwise multivariate scoring functions using deep neural networks. In Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval, 85–92. 2019.">AWB+19</a>]</span> is not to assign a label or a value to individual items, but to produce an ordering of the items in that list in such a way that the utility of the entire list is maximized.</p>
<p>In other words, in ranking we are more concerned with the relative ordering items, instead of predicting the numerical value or label of an individual item.</p>
<p><strong>Pointwise ranking</strong> transforms the ranking problem into a regression problem. Given a certain Query, ranking amounts to</p>
<ul class="simple">
<li><p>Predict the relevance score between the document to the query</p></li>
<li><p>Order the document list based on its relevance score with the query.</p></li>
</ul>
<p><strong>Pairwise Ranking</strong><span id="id27">[<a class="reference internal" href="#id1559" title="Christopher JC Burges. From ranknet to lambdarank to lambdamart: an overview. Learning, 11(23-581):81, 2010.">Bur10</a>]</span>, instead of predicting the absolute relevance, learns to predict the relative order of documents. This method is particularly useful in scenarios where the absolute relevance scores are less important than the relative ordering of items,</p>
<p><strong>Listwise ranking</strong> <span id="id28">[<a class="reference internal" href="#id1606" title="Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine learning, 129–136. 2007.">CQL+07</a>]</span> considers the entire list of items simultaneously when training a ranking model. Unlike pairwise or pointwise methods, listwise ranking directly optimizes ranking metrics such as NDCG or MAP, which better aligns the training objective with the evaluation criteria used in information retrieval tasks. This approach can capture more complex relationships between items in a list and often leads to better performance in real-world ranking scenarios, though it may be computationally more expensive than other ranking methods.</p>
<!-- 
\begin{remark}[training loss function and score function]\hfill
- RankNet, LambdaRank are methods with pairwise loss function and univariate score function
- Biencoders with N-pair loss function are methods with list-wise loss function and univariate score function
- Cross-encoder with Binary cross entropy loss are methods with point-wise loss and univariate score function

\end{remark} -->
</section>
<section id="training-data">
<h3><span class="section-number">25.4.2. </span>Training Data<a class="headerlink" href="#training-data" title="Link to this heading">#</a></h3>
<p>In a typical model learning setting, we construct training data from user search log, which contains queries issued by users and the documents they clicked after issuing the query. The basic assumption is that a query and a document are relevant if the user clicked the document.</p>
<p>Model learning in information retrieval typically falls into the category of contrastive learning. The query and the clicked document form a positive example; the query and irrelevant documents form negative examples. For retrieval problems, it is often the case that positive examples are available explicitly, while negative examples are unknown and need to be selected from an extremely large pool. The strategy of selecting negative examples plays an important role in determining quality of the encoders. In the most simple case, we randomly select unclicked documents as irrelevant document, or negative example. We defer the discussion of advanced negative example selecting strategy to <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-negativesamplingstrategies"><span class="std std-numref">Section 25.5</span></a>.</p>
<p>When there is a shortage of annotation data or click behavior data, we can also leverage weakly supervised data for training <span id="id29">[<a class="reference internal" href="#id1439" title="Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural ranking models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 65–74. 2017.">DZS+17</a>, <a class="reference internal" href="#id1514" title="Dany Haddad and Joydeep Ghosh. Learning more from less: towards strengthening weak supervision for ad-hoc retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 857–860. 2019.">HG19</a>, <a class="reference internal" href="#id1515" title="Yifan Nie, Alessandro Sordoni, and Jian-Yun Nie. Multi-level abstraction convolutional model with weak supervision for information retrieval. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 985–988. 2018.">NSN18</a>, <a class="reference internal" href="#id1513" title="Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision. arXiv preprint arXiv:2112.07708, 2021.">RSL+21</a>]</span>. In the weakly supervised data, labels or signals are obtained from an unsupervised ranking model, such as BM25. For example, given a query, relevance scores for all documents can be computed efficiently using BM25. Documents with highest scores can be used as positive examples and documents with lower scores can be used as negatives or hard negatives.</p>
</section>
<section id="model-training-objective-functions">
<h3><span class="section-number">25.4.3. </span>Model Training Objective Functions<a class="headerlink" href="#model-training-objective-functions" title="Link to this heading">#</a></h3>
<section id="pointwise-regression-objective">
<h4><span class="section-number">25.4.3.1. </span>Pointwise Regression Objective<a class="headerlink" href="#pointwise-regression-objective" title="Link to this heading">#</a></h4>
<p>The idea of pointwise regression objective is to model the numerical relevance score for a given query-document. During inference time, the relevance scores between a set of candidates and a given query can be predicted and ranked.</p>
<p>During training, given a set of query-document pairs <span class="math notranslate nohighlight">\(\left(q_{i}, d_{i, j}\right)\)</span> and their corresponding relevance score <span class="math notranslate nohighlight">\(y_{i, j} \in [0, 1]\)</span> and their prediction <span class="math notranslate nohighlight">\(f(q_i,d_{i,j})\)</span>. A pointwise regression objective tries to optimize a model to predict the relevance score via minimization</p>
<div class="math notranslate nohighlight">
\[L = -\sum_{i} \sum_{j} (f(q_i,d_{i,j})) - y_{i,j})^2.
\]</div>
<p>Using a regression objective offer flexible for the user to model different levels of relevance between queries and documents. However, such flexibility also comes with** a requirement that the target relevance score should be accurate in absolute scale.**
While human annotated data might provide absolute relevance score, human annotation data is expensive and small scale. On the other hand, absolute relevance scores that are approximated by click data can be noisy and less optimal for regression objective. To make** training robust to label noises**, one can consider <strong>Pairwise ranking objectives.</strong> This particularly important in weak supervision scenario with noisy label. Using the ranking objective alleviates this issue by forcing the 	model to learn a preference function rather than reproduce absolute scores.</p>
</section>
<section id="pointwise-ranking-objective">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-pointwise-ranking-loss"></span><h4><span class="section-number">25.4.3.2. </span>Pointwise Ranking Objective<a class="headerlink" href="#pointwise-ranking-objective" title="Link to this heading">#</a></h4>
<p>The idea of pointwise ranking objective is to simplify a ranking problem to a binary classification problem. Specifically, given a set of query-document pairs <span class="math notranslate nohighlight">\(\left(q_{i}, d_{i, j}\right)\)</span> and their corresponding relevance label <span class="math notranslate nohighlight">\(y_{i, j} \in \{0, 1\}\)</span>, where 0 denotes irrelevant and 1 denotes relevant. A pointwise learning objective tries to optimize a model to predict the relevance label.</p>
<p>A commonly used pointwise loss functions is the binary Cross Entropy loss:</p>
<div class="math notranslate nohighlight">
\[
L=-\sum_{i} \sum_{j} y_{i, j} \log \left(p\left(q_{i}, d_{i, j}\right)\right)+\left(1-y_{i, j}\right) \log \left(1-p\left(q_{i}, d_{i, j}\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(p\left(q_{i}, d{i, j}\right)\)</span> is the predicted probability of document <span class="math notranslate nohighlight">\(d_{i,j}\)</span> being relevant to query <span class="math notranslate nohighlight">\(q_i\)</span>.</p>
<p>The advantages of pointwise ranking objectives are two-fold. First, pointwise ranking objectives are computed based on each query-document pair <span class="math notranslate nohighlight">\(\left(q_{i}, d_{i, j}\right)\)</span> separately, which makes it simple and easy to scale. Second, the outputs of neural models learned with pointwise loss functions often have real meanings and value in practice. For instance, in sponsored search, the predicted the relevance probability can be used in ad bidding, which is more important than creating a good result list in some application scenarios.</p>
<p>In general, however, pointwise ranking objectives are considered to be less effective in ranking tasks. Because pointwise loss functions consider no document preference or order information, they do not guarantee to produce the best ranking list when the model loss reaches the global minimum. Therefore, better ranking paradigms that directly optimize document ranking based on pairwise loss functions and even listwise loss functions.</p>
</section>
<section id="pairwise-ranking-via-triplet-loss">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-triplet-loss"></span><h4><span class="section-number">25.4.3.3. </span>Pairwise Ranking via Triplet Loss<a class="headerlink" href="#pairwise-ranking-via-triplet-loss" title="Link to this heading">#</a></h4>
<p>Pointwise ranking loss aims to optimize the model to directly predict relevance between query and documents on absolute score. From embedding optimization perspective, it train the neural query/document encoders to produce similar embedding vectors for a query and its relevant document and dissimilar embedding vectors for a query and its irrelevant documents.</p>
<p>On the other hand, pairwise ranking objectives focus on <strong>optimizing the relative preferences between documents rather than predicting their relevance labels.</strong> In contrast to pointwise methods where the final ranking loss is the sum of loss on each document, pairwise loss functions are computed based on the different combination of document pairs.</p>
<p>One of the most common pairwise ranking loss function is the <strong>triplet loss</strong>. Let <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left\langle q_{i}, d_{i}^{+}, d_{i}^{-}\right\rangle\right\}_{i=1}^{m}\)</span> be the training data organized into <span class="math notranslate nohighlight">\(m\)</span> triplets. Each triplet contains one query <span class="math notranslate nohighlight">\(q_{i}\)</span> and one relevant document <span class="math notranslate nohighlight">\(d_{i}^{+}\)</span>, along with one irrelevant (negative) documents <span class="math notranslate nohighlight">\(d_{i}^{-}\)</span>. Negative documents are typically randomly sampled from a large corpus or are strategically constructed [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-negativesamplingstrategies"><span class="std std-numref">Section 25.5</span></a>].
Visualization of the learning process in the embedding space is shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-triplet"><span class="std std-numref">Fig. 25.11</span></a>. Triplet loss helps guide the encoder networks to pull relevant query and document closer and push irrelevant query and document away.</p>
<p>The loss function is given by</p>
<div class="math notranslate nohighlight">
\[L =- \sum_{\left\langle q_{i}, d_{i}^{+}, d_{i}^{-}\right\rangle}\max(0,  (\operatorname{Sim}(q_i, d_i^+) - \operatorname{Sim}(q_i, d^-_i)) - m)\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{Sim}(q, d)\)</span> is the similarity score produced by the network between the query and the document and <span class="math notranslate nohighlight">\(m\)</span> is a hyper-parameter adjusting the margin. Clearly, if we would like to make <span class="math notranslate nohighlight">\(L\)</span> small, we need to make  <span class="math notranslate nohighlight">\(\operatorname{Sim}(q_i, d_i^+) - \operatorname{Sim}(q_i, d^-_i) &gt; m\)</span>. Commonly used <span class="math notranslate nohighlight">\(\operatorname{Sim}\)</span> functions include <strong>dot product</strong> or <strong>Cosine similarity</strong> (i.e., length-normalized dot product), which are related to distance calculation in the Euclidean space and hyperspherical surface.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-triplet">
<img alt="../../_images/triplet.png" src="../../_images/triplet.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.11 </span><span class="caption-text">The illustration of the learning process (in the embedding space) using triplet loss.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-triplet" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Triplet loss can also operating in the angular space</p>
<div class="math notranslate nohighlight">
\[
\operatorname{sim}(q, d)=1-\arccos \left(\frac{\psi_{\beta}(q) \cdot \psi_{\alpha}(d)}{\left\|\psi_{\beta}(q)\right\|\left\|\psi_{\alpha}(d)\right\|}\right) / \pi
\]</div>
<p>As illustrated in Figure 1, the training objective is to score the positive example <span class="math notranslate nohighlight">\(d^{+}\)</span>by at least the margin <span class="math notranslate nohighlight">\(\mu\)</span> higher than the negative one <span class="math notranslate nohighlight">\(d^{-}\)</span>. As part of our loss function, we use the triplet margin objective:</p>
<div class="math notranslate nohighlight">
\[
l\left(q, d^{+}, d^{-}\right):=-\max \left(0, \operatorname{sim}\left(q, d^{+}\right)-\operatorname{sim}\left(q, d^{-}\right)-\mu\right)
\]</div>
</section>
<section id="n-pair-loss">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-n-pair-loss"></span><h4><span class="section-number">25.4.3.4. </span>N-pair Loss<a class="headerlink" href="#n-pair-loss" title="Link to this heading">#</a></h4>
<p>Triplet loss optimize the neural by encouraging positive pair <span class="math notranslate nohighlight">\((q_i, d^+_i)\)</span> to be more similar than its negative pair <span class="math notranslate nohighlight">\((q_i, d^+_i)\)</span>. One improvement is to encourage <span class="math notranslate nohighlight">\(q_i\)</span> to be more similar <span class="math notranslate nohighlight">\(d^+_i\)</span> compared to <span class="math notranslate nohighlight">\(n\)</span> negative examples <span class="math notranslate nohighlight">\( d_{i, 1}^{-}, \cdots, d_{i, n}^{-}\)</span>, instead of just one negative example. This is known as N-pair loss <span id="id30">[<a class="reference internal" href="#id1488" title="Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. Advances in neural information processing systems, 2016.">Soh16</a>]</span>, and it is typically more robust than triplet loss.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left\langle q_{i}, d_{i}^{+}, D_i^-\right\rangle\right\}_{i=1}^{m}\)</span>, where <span class="math notranslate nohighlight">\(D_i^- = \{d_{i, 1}^{-}, \cdots, d_{i, n}^{-}\}\)</span> are a set of negative examples (i.e., irrelevant document) with respect to query <span class="math notranslate nohighlight">\(q_i\)</span>,  be the training data that consists of <span class="math notranslate nohighlight">\(m\)</span> examples. Each example contains one query <span class="math notranslate nohighlight">\(q_{i}\)</span> and one relevant document <span class="math notranslate nohighlight">\(d_{i}^{+}\)</span>, along with <span class="math notranslate nohighlight">\(n\)</span> irrelevant (negative) documents <span class="math notranslate nohighlight">\(d_{i, j}^{-}\)</span>. The <span class="math notranslate nohighlight">\(n\)</span> negative documents are typically randomly sampled from a large corpus or are strategically constructed [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-negativesamplingstrategies"><span class="std std-numref">Section 25.5</span></a>].</p>
<p>Visualization of the learning process in the embedding space is shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-npairloss"><span class="std std-numref">Fig. 25.12</span></a>. Like triplet loss, N-pair loss helps guide the encoder networks to pull relevant query and document closer and push irrelevant query and document away. Besides that, when there are are negatives are involved in the N-pair loss, their repelling to each other appears to help the learning of generating more uniform embeddings<span id="id31">[<a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html#id1438" title="Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, 9929–9939. PMLR, 2020.">WI20</a>]</span>.</p>
<p>The loss function is given by</p>
<div class="math notranslate nohighlight">
\[L =-\sum_{\left\langle q_{i}, d_{i}^{+}, D_{i}^{-}\right\rangle}\log \frac{\exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{+}}\right))}{\exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{+}}\right))+\sum_{d^-_i\in D^-} \exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{-}}\right))}\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{Sim}(e_q, e_d)\)</span> is the similarity score function taking query embedding <span class="math notranslate nohighlight">\(e_q\)</span> and document embedding <span class="math notranslate nohighlight">\(e_d\)</span> as the input.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-npairloss">
<img alt="../../_images/N_pair_loss.png" src="../../_images/N_pair_loss.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.12 </span><span class="caption-text">The illustration of the learning process (in the embedding space) using N-pair loss.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-npairloss" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="n-pair-dual-loss">
<h4><span class="section-number">25.4.3.5. </span>N-pair Dual Loss<a class="headerlink" href="#n-pair-dual-loss" title="Link to this heading">#</a></h4>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-npairlossdual">
<img alt="../../_images/N_pair_loss_dual.png" src="../../_images/N_pair_loss_dual.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.13 </span><span class="caption-text">The illustration of the learning process (in the embedding space) using N-pair dual loss.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-npairlossdual" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The N-pair loss uses query as the anchor to adjust the distribution of document vectors in the embedding space. Authors in <span id="id32">[<a class="reference internal" href="#id1492" title="Yizhi Li, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. More robust dense retrieval with contrastive dual learning. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, 287–296. 2021.">LLXL21</a>]</span> proposed that document can also be as the anchor to adjust the distribution of query vectors in the embedding space. This leads to loss functions consisting of two parts</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
L &amp;= L_{prime} + L_{dual} \\
	L_{prime} &amp;=-\sum_{\left\langle q_{i}, d_{i}^{+}, D_{i}^{-}\right\rangle}\log \frac{\exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{+}}\right))}{\exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{+}}\right))+\sum_{d^-_i\in D^-} \exp(\operatorname{Sim}\left(e_{q_{i}}, e_{d_{i}^{-}}\right))} \\
	L_{dual} &amp;=-\sum_{\left\langle d_{i}, q_{i}^{+}, Q_{i}^{-}\right\rangle}\log \frac{\exp(\operatorname{Sim}\left(e_{d_{i}}, e_{q_{i}^{+}}\right))}{\exp(\operatorname{Sim}\left(e_{d_{i}}, e_{q_{i}^{+}}\right))+\sum_{q^-_i\in Q^-} \exp(\operatorname{Sim}\left(e_{d_{i}}, e_{q_{i}^{-}}\right))}
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{Sim}(e_q, e_d)\)</span> is a <strong>symmetric</strong> similarity score function for the query and the document embedding vectors, <span class="math notranslate nohighlight">\(L_{prime}\)</span> is the N-pair loss, and <span class="math notranslate nohighlight">\(L_{dual}\)</span> is the N-pair dual loss.</p>
<p>To compute dual loss, we need to prepare training data <span class="math notranslate nohighlight">\(\mathcal{D}_{dual}=\left\{\left\langle d_{i}, q_{i}^{+}, Q_i^-\right\rangle\right\}_{i=1}^{m}\)</span>, where <span class="math notranslate nohighlight">\(Q_i^- = \{q_{i, 1}^{-}, \cdots, q_{i, n}^{-}\}\)</span> are a set of negative queries examples (i.e., irrelevant query) with respect to document <span class="math notranslate nohighlight">\(d_i\)</span>. Each example contains one document <span class="math notranslate nohighlight">\(d_{i}\)</span> and one relevant query <span class="math notranslate nohighlight">\(d_{i}^{+}\)</span>, along with <span class="math notranslate nohighlight">\(n\)</span> irrelevant (negative) queries <span class="math notranslate nohighlight">\(q_{i, j}^{-}\)</span>.</p>
</section>
<section id="doc-doc-n-pair-loss">
<h4><span class="section-number">25.4.3.6. </span>Doc-Doc N-pair Loss<a class="headerlink" href="#doc-doc-n-pair-loss" title="Link to this heading">#</a></h4>
<figure class="align-default" id="id33">
<img alt="../../_images/N_pair_doc_doc.png" src="../../_images/N_pair_doc_doc.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.14 </span><span class="caption-text">The illustration of the learning process (in the embedding space) using Doc-Doc N-pair loss.</span><a class="headerlink" href="#id33" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Besiding use above prime and dual loss to capture robust query doc relationship, we can also improve robustness of document representation by considering doc-doc relations. Particularly,</p>
<ul class="simple">
<li><p>When there are multiple positive documents associated with the same query, we use loss function encourage their representation embedding to stay close.</p></li>
<li><p>For positive and negative documents associated with the same query, we use loss function encourage their representation embedding to stay far apart.</p></li>
</ul>
<p>The loss function is given by</p>
<div class="math notranslate nohighlight">
\[L =-\sum_{\left\langle q_{i}, d_{i}^{+}, d_{i'}^{+} \in D_{i}^{+}, D_{i}^{-}\right\rangle}\log \frac{\exp(\operatorname{Sim}\left(e_{d_{i}}, e_{d_{i'}}\right))}{\exp(\operatorname{Sim}\left(e_{d_{i}^+}, e_{d_{i}^{+}}\right))+\sum_{d^-_i\in D^-} \exp(\operatorname{Sim}\left(e_{d_{i}^+}, e_{d_{i}^{-}}\right))}\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{Sim}(e_{d_1}, e_{d_2})\)</span> is the similarity score function taking document embeddings <span class="math notranslate nohighlight">\(e_{d_1}\)</span> and <span class="math notranslate nohighlight">\(e_{d_2}\)</span> as the input.</p>
</section>
</section>
</section>
<section id="training-data-sampling-strategies">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-negativesamplingstrategies"></span><h2><span class="section-number">25.5. </span>Training Data Sampling Strategies<a class="headerlink" href="#training-data-sampling-strategies" title="Link to this heading">#</a></h2>
<section id="principles">
<h3><span class="section-number">25.5.1. </span>Principles<a class="headerlink" href="#principles" title="Link to this heading">#</a></h3>
<p>From the ranking perspective, both retrieval and re-ranking requires the generation of some order on the input samples. For example, given a query <span class="math notranslate nohighlight">\(q\)</span> and a set of candidate documents <span class="math notranslate nohighlight">\((d_1,...,d_N)\)</span>. We need the model to produce an order list <span class="math notranslate nohighlight">\(d_2 \succ d_3 ... \succ d_k\)</span> according to their relevance to the query.</p>
<p>To train a model to produce the expected results during inference, we should ensure the training data distribution to matched with the inference time data distribution.
Particularly, the inference time the candidate document distribution and ranking granularity  differ vastly for retrieval tasks and re-ranking tasks [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrievalrerankingtask"><span class="std std-numref">Fig. 25.15</span></a>]. Specifically,</p>
<ul class="simple">
<li><p>For the retrieval task, we typically need to identify top <span class="math notranslate nohighlight">\(k (k=1000-10000)\)</span> relevant documents from the entire document corpus. This is achieved by ranking all documents in the corpus with respect to the relevance of the query.</p></li>
<li><p>For the re-ranking task, we need to identify the top <span class="math notranslate nohighlight">\(k (k=10)\)</span> most relevant documents from the relevant documents generated by the retrieval task.</p></li>
</ul>
<p>Clearly, features most useful in the retrieval task (i.e., distinguish relevant from irrelevant) are often not the same as the features most useful in re-ranking task (i.e., distinguish most relevant from less relevant). Therefore, the training samples for retrieval and re-ranking need to be constructed differently.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrievalrerankingtask">
<img alt="../../_images/retrieval_reranking_task.png" src="../../_images/retrieval_reranking_task.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.15 </span><span class="caption-text">Retrieval tasks and re-ranking tasks are faced with different the candidate document distribution and ranking granularity.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-retrievalrerankingtask" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Constructing the proper training data distribution is more challenging to retrieval stage than the re-ranking stage. In re-ranking stage, data in the training and inference phases are both the documents from previous retrieval stages. In the retrieval stage, we need to construct training examples in a mini-batch fashion in a way that each batch approximates the distribution in the inference phase as close as possible.</p>
<p>This section will mainly focus on constructing training examples for retrieval model training in an efficient and effective way. Since the number of negative examples (i.e., irrelevant documents) significantly outnumber the number of positive examples. Constructing training examples particularly boil down to constructing proper negative examples.</p>
</section>
<section id="negative-sampling-methods-i-heuristic-methods">
<h3><span class="section-number">25.5.2. </span>Negative Sampling Methods I: Heuristic Methods<a class="headerlink" href="#negative-sampling-methods-i-heuristic-methods" title="Link to this heading">#</a></h3>
<!-- #### Overview

The essence of the negative sampling algorithm is to set or adjust the sampling distribution during negative sampling based on certain methods. According to the way the negative sampling algorithm sets the sampling distribution, we can divide the current negative sampling algorithms into two categories: Heuristic Negative Sampling Algorithms and Model-based Negative Sampling Algorithms.

In {cite}`karpukhin2020dense`, there are three different types of negatives: (1) Random: any random passage from the corpus; (2) BM25: top passages returned by BM25 which don’t contain the answer but match most question tokens; (3) Gold: positive passages paired with other questions which appear in the training set.

One approach to improving the effectiveness of single-vector bi-encoders is hard negative mining, by training with carefully selected negative examples that emphasize discrimination between relevant and non-relevant texts.

both large in-batch negative sampling and asynchronous ANN index updates are computationally demanding.

Compared with the two heuristic algorithms mentioned above, the model-based negative sampling algorithm is easier to pick high-quality negative examples, and it is also the more cutting-edge sampling algorithm at present. Here are several model-based negative sampling algorithms: -->
<section id="random-negatives-and-in-batch-negatives">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-in-batch-negatives"></span><h4><span class="section-number">25.5.2.1. </span>Random Negatives and In-batch Negatives<a class="headerlink" href="#random-negatives-and-in-batch-negatives" title="Link to this heading">#</a></h4>
<p><strong>Random negative sampling</strong> is the most basic negative sampling algorithm. The algorithm uniformly sample documents from the corpus and treat it as a negative. Clearly, random negatives can generate negatives that are <strong>too easy</strong> for the model. For example, a negative document that is topically different from the query. These easy negatives lower the learning efficiency, that is, each batch produces limited information gain to update the model. Still, random negatives are widely used because of its simplicity.</p>
<p>In practice, random negatives are implemented as in-batch negatives.  In the contrastive learning framework with N-pair loss [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-n-pair-loss"><span class="std std-numref">Section 25.4.3.4</span></a>], we construct a mini-batch of query-doc examples like <span class="math notranslate nohighlight">\(\{(q_1, d_1^+, d_{1,1}^-, d_{1,M}^-), ..., (q_N, d_N^+, d_{N,1}^-, d_{N,M}^M)\}\)</span>, Naively implementing N-pair loss would increase computational cost from constructing sufficient negative documents corresponding to each query. In-batch negatives<span id="id34">[<a class="reference internal" href="#id1463" title="Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.">KOuguzM+20</a>]</span> is trick to reuse positive documents associated with other queries as extra negatives [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchnegatives"><span class="std std-numref">Fig. 25.16</span></a>]. The critical assumption here is that queries in a mini-batch are vastly different semantically, and positive documents from other queries would be confidently used as negatives. The assumption is largely true since each mini-batch is randomly sampled from the set of all training queries, in-batch negative document are usually true negative although they might not be hard negatives.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchnegatives">
<img alt="../../_images/in_batch_negatives.png" src="../../_images/in_batch_negatives.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.16 </span><span class="caption-text">The illustration of using in-batch negatives in contrastive learning.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchnegatives" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Specifically, assume that we have <span class="math notranslate nohighlight">\(N\)</span> queries in a mini-batch and each one is associated with a relevant  positive document. By using positive document of other queries, each query will have an additional <span class="math notranslate nohighlight">\(N - 1\)</span> negatives.</p>
<p>Formally, we can define our batch-wise loss function as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}:=\sum_{1 \leq i \leq N}\left(\sum_{1 \leq j \leq N} l\left(q_{i}, d_{i}^{+}, d_{j}^{-}\right)+\sum_{1 \leq k \leq N, k \neq i} l\left(q_{i}, d_{i}^{+}, d_{k}^{+}\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(l\left(q_{i}, d_{i}^{+}, d_{j}^{-}\right)\)</span> is the loss function for a triplet.</p>
<p>In-batch negative offers an efficient implementation for random negatives. Another way to mitigate the inefficient learning issue is simply use large batch size (&gt;4,000) [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-large-scale-negatives"><span class="std std-ref">Cross-Batch Large-Scale Negatives</span></a>].</p>
</section>
<section id="popularity-based-negative-sampling">
<h4><span class="section-number">25.5.2.2. </span>Popularity-based Negative Sampling<a class="headerlink" href="#popularity-based-negative-sampling" title="Link to this heading">#</a></h4>
<p><strong>Popularity-based negative sampling</strong> use document popularity as the sampling weight to sample negative documents. The popularity of a document can be defined as some combination of click, dwell time, quality, etc. Compared to random negative sampling, this algorithm replaces the uniform distribution with a popularity-based sampling distribution, which can be pre-computed offline.</p>
<p>The major rationale of using popularity-based negative examples is to improve representation learning. <strong>Popular negative documents represent a harder negative compared to a unpopular negative since they tend to have to a higher chance of being more relevant</strong>; that is, lying closer to query in the embedding space. If the model is trained to distinguish these harder cases, the over learned representations will be likely improved.</p>
<p>Popularity-based negative sampling is also used in word2vec training <span id="id35">[<a class="reference internal" href="#id1191" title="Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111–3119. 2013.">MSC+13</a>]</span>. For example, the probability to sample a word <span class="math notranslate nohighlight">\(w_i\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
P\left(w_i\right)=\frac{f\left(w_i\right)^{3 / 4}}{\sum_{j=0}^n\left(f\left(w_j\right)^{3 / 4}\right)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(w)\)</span> is the frequency of word <span class="math notranslate nohighlight">\(w\)</span>. This equation, compared to linear popularity, has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.</p>
</section>
<section id="topic-aware-negative-sampling">
<h4><span class="section-number">25.5.2.3. </span>Topic-aware Negative Sampling<a class="headerlink" href="#topic-aware-negative-sampling" title="Link to this heading">#</a></h4>
<p>In-batch random negatives would often consist of  topically-different documents, leaving little information gain for the training. To improve the information gain from a single random batch, we can constrain the queries and their relevant document are drawn from a similar topic<span id="id36">[<a class="reference internal" href="#id1458" title="Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 113–122. 2021.">HofstatterLY+21</a>]</span>.</p>
<p>The procedures are</p>
<ul class="simple">
<li><p>Cluster queries using query embeddings produced by basic query encoder.</p></li>
<li><p>Sample queries and their relevant documents from a randomly picked cluster. A relevant document of a query form the negative of the other query.</p></li>
</ul>
<p>Since queries are topically similar, the formed in-batch negatives are harder examples than randomly formed in-batch negative, therefore delivering more information gain each batch.</p>
<p>Note that here we group queries into clusters by their embedding similarity, which allows grouping queries without lexical overlap. We can also consider lexical similarity between queries as additional signals to predict query similarity.</p>
</section>
</section>
<section id="negative-sampling-methods-ii-model-based-methods">
<h3><span class="section-number">25.5.3. </span>Negative Sampling Methods II: Model-based Methods<a class="headerlink" href="#negative-sampling-methods-ii-model-based-methods" title="Link to this heading">#</a></h3>
<section id="static-hard-negative-examples">
<h4><span class="section-number">25.5.3.1. </span>Static Hard Negative Examples<a class="headerlink" href="#static-hard-negative-examples" title="Link to this heading">#</a></h4>
<p>Deep model improves the encoded representation of queries and documents by contrastive learning, in which the model learns to distinguish positive examples and negative examples. A simple random sampling strategy tend to produce a large quantity of easy negative examples since easy negative examples make up the majority of negative examples. Here by easy negative examples, we mean a document that can be easily judged to be irrelevant to the query. For example, the document and the query are in completely different topics.</p>
<p>The model learning from easy negative example can quickly plateau since easy negative examples produces vanishing gradients to update the model. An improvement strategy is to supply additional hard negatives with randomly sampled negatives. In the simplest case, hard negatives can be selected based on a traditional BM25model <span id="id37">[<a class="reference internal" href="#id1463" title="Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.">KOuguzM+20</a>, <a class="reference internal" href="#id1451" title="Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019.">NC19</a>]</span> or other efficient dense retriever: hard negatives are those have a high relevant score to the query but they are not relevant.</p>
<p>As illustrated in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-impacthardnegativeonretrieval"><span class="std std-numref">Fig. 25.17</span></a>, a model trained only with easy negatives can fail to distinguish fairly relevant documents from irrelevant examples; on the other hand, a model trained with some hard negatives can learn better representations:</p>
<ul class="simple">
<li><p>Positive document embeddings are more aligned <span id="id38">[<a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html#id1438" title="Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, 9929–9939. PMLR, 2020.">WI20</a>]</span>; that is, they are lying closer with respect to each other.</p></li>
<li><p>Fairly relevant and irrelevant documents are more separated in the embedding space and thus a better decision boundary for relevant and irrelevant documents.</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-impacthardnegativeonretrieval">
<a class="reference internal image-reference" href="../../_images/Impact_hard_negative_on_retrieval.png"><img alt="../../_images/Impact_hard_negative_on_retrieval.png" src="../../_images/Impact_hard_negative_on_retrieval.png" style="width: 701.1px; height: 353.7px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.17 </span><span class="caption-text">Illustration of importance of negative hard examples, which helps learning better representations to distinguish irrelevant and fairly relevant documents.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-impacthardnegativeonretrieval" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In generating these negative examples, the negative-generation model (e.g., BM25) and the model under training are de-coupled; that is the negative-generation model is not updated during training and the hard examples are static. Despite this simplicity, static hard negative examples introduces two shortcomings:</p>
<ul class="simple">
<li><p>Distribution mismatch, the negatives generated by the static model might quickly become less hard since the target model is constantly evolving.</p></li>
<li><p>The generated negatives can have a higher risk of being false negatives to the target model because negative-generation model and the target model are two different models.</p></li>
</ul>
</section>
<section id="dynamic-hard-negative-mining">
<h4><span class="section-number">25.5.3.2. </span>Dynamic Hard Negative Mining<a class="headerlink" href="#dynamic-hard-negative-mining" title="Link to this heading">#</a></h4>
<p>Dynamic hard negative mining is a scheme first proposed in ANCE<span id="id39">[<a class="reference internal" href="#id1481" title="Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.">XXL+20</a>]</span>. The core idea is to use the target model at <strong>previous checkpoint</strong> as the negative-generation model [<code class="xref std std-numref docutils literal notranslate"><span class="pre">ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:fig:ancenegativesamplingdemo</span></code>], instead of only using in-batch local negatives.
Specifically, checkpoints from previous epoch iteration is used to retrieve top candidates. These candidates, excluding labeled positives, are used as hard negatives. As shown in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-ancenegativesamplingembedding"><span class="std std-numref">Fig. 25.18</span></a>, these mined hard negatives are lying rather closer to the postive compared random negatives as well as BM25 negatives.</p>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:fig:ancenegativesamplingdemo</span></code> shows the workflow for dynamic negative mining. However, this negative mining approach is rather computationally demanding since corpus index need updates at every checkpoint.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-ancenegativesamplingembedding">
<a class="reference internal image-reference" href="../../_images/ANCE_negative_sampling_embedding_space_demo.png"><img alt="../../_images/ANCE_negative_sampling_embedding_space_demo.png" src="../../_images/ANCE_negative_sampling_embedding_space_demo.png" style="width: 433.8px; height: 293.85px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.18 </span><span class="caption-text">T-SNE representations of query, relevant documents, negative training instances
from BM25 (BM25 Neg) or randomly sampled (Rand Neg), and testing negatives (DR Neg) in dense retrieval. Image from <span id="id40">[<a class="reference internal" href="#id1481" title="Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.">XXL+20</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-ancenegativesamplingembedding" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-ancenegativesampling">
<a class="reference internal image-reference" href="../../_images/ANCE_negative_sampling_demo.png"><img alt="../../_images/ANCE_negative_sampling_demo.png" src="../../_images/ANCE_negative_sampling_demo.png" style="width: 662.85px; height: 268.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.19 </span><span class="caption-text">Dynamic hard negative sampling from ANCE asynchronous training framework. Negatives are drawn from index produced using models at the previous checkpoint. Image from <span id="id41">[<a class="reference internal" href="#id1481" title="Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.">XXL+20</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-ancenegativesampling" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>RocketQA <span id="id42">[<a class="reference internal" href="#id1480" title="Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.">QDL+20</a>]</span> follows similar idea in ANCE, but further leverages cross-encoder at the re-ranking stage  to generate de-noised hard negatives:</p>
<ul class="simple">
<li><p>Top-ranked passages from the retriever’s output, excluding the labeled positive passages, are used as hard negatives.</p></li>
<li><p>This will bring false negatives since annotators usually only annotate a few top-retrieved passages, therefore the cross-encoder ranker needs to get involve to remove false negatives.</p></li>
</ul>
</section>
<section id="cross-batch-large-scale-negatives">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-large-scale-negatives"></span><h4><span class="section-number">25.5.3.3. </span>Cross-Batch Large-Scale Negatives<a class="headerlink" href="#cross-batch-large-scale-negatives" title="Link to this heading">#</a></h4>
<p>Fundamentally, we want large-scale negatives to better sample the underlying continuous, highdimensional embedding space. In-batch negatives offers an efficient way to construct many negatives during training; however, the number of negatives are limited by GPU memory that determines the batch size. During multiple GPU training [examplified by <strong>RocketQA</strong><span id="id43">[<a class="reference internal" href="#id1480" title="Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.">QDL+20</a>]</span>], in-batch negatives can be generalized to cross-batch negatives to generate large-scale negatives.</p>
<p>Specifically,</p>
<ul class="simple">
<li><p>We first compute the document embeddings within each single GPU, and then share these documents embeddings among all the GPUs.</p></li>
<li><p>Beside in-batch negatives, all documents representations from other GPUs are used as the additional negatives for each query.</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-crossnegative-demo">
<a class="reference internal image-reference" href="../../_images/cross_batch_negatives.png"><img alt="../../_images/cross_batch_negatives.png" src="../../_images/cross_batch_negatives.png" style="width: 496.79999999999995px; height: 322.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.20 </span><span class="caption-text">The comparison of in-batch negative and cross-batch negative during multi-gpu training. Image from <span id="id44">[<a class="reference internal" href="#id1480" title="Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.">QDL+20</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-crossnegative-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="momentum-negatives">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-momentum-negatives"></span><h4><span class="section-number">25.5.3.4. </span>Momentum Negatives<a class="headerlink" href="#momentum-negatives" title="Link to this heading">#</a></h4>
<p>Even in the single-GPU training setting, we can leverage queue to construct large-scale negatives [<strong>MoCo</strong> <span id="id45">[<a class="reference internal" href="#id78" title="Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. 2020. URL: https://arxiv.org/abs/2003.04297, arXiv:2003.04297.">CFGH20</a>]</span>].</p>
<p>Fundamentally, we want the negatives are coming from the same or similar encoder so that their comparisons in the contrastive learning are consistent.</p>
<p>MoCo leverages an additional momentum network, parameterized by<span class="math notranslate nohighlight">\(\theta_k\)</span>, to generate representations that are used as negatives for the main network. The parameters of the key network does not update from graident descent, instead, it is updated from the parameters of the main network network by using a exponential moving average:</p>
<div class="math notranslate nohighlight">
\[
\theta_k \leftarrow m \theta_k+(1-m) \theta_q,
\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the momentum parameter that takes its value in <span class="math notranslate nohighlight">\([0,1]\)</span>. A queue is used to enque representations from the momentum network, which also exits old batch after exceeding queue size. The size of the queue controls the number of negative examples that the main network can see. One example application of Moco is <span id="id46">[<a class="reference internal" href="#id1661" title="Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.">ICH+21</a>]</span> <a class="reference external" href="https://github.com/facebookresearch/contriever">code</a></p>
</section>
<section id="hard-positives">
<h4><span class="section-number">25.5.3.5. </span>Hard Positives<a class="headerlink" href="#hard-positives" title="Link to this heading">#</a></h4>
<p>In the retrieval model query-doc training data, it is usually filled with <strong>easy positives</strong>, that is query and relevant documents have all query term exact matched. During hybrid retrieval system, as the goal of dense retrieval is to complement sparse retrieval (which relies on exact term matching), it is beneficial to enrich training samples with <strong>hard positives</strong>, that is query and relevant document does not have all query term exact matched, particularly important query terms. With easy and hard positives, we can design currilumn learning to help model improve its semantic retrieval ability.</p>
</section>
</section>
<section id="label-denoising">
<h3><span class="section-number">25.5.4. </span>Label Denoising<a class="headerlink" href="#label-denoising" title="Link to this heading">#</a></h3>
<section id="false-negatives">
<h4><span class="section-number">25.5.4.1. </span>False Negatives<a class="headerlink" href="#false-negatives" title="Link to this heading">#</a></h4>
<p>Hard negative examples produced from static or dynamic negative mining methods are effective to improve the encoder’s performance. However, when selecting hard negatives with a less powerful model (e.g., BM25), we are also running the risk of introduce more false negatives (i.e., negative examples are actually positive) than a random sampling approach. Authors in <span id="id47">[<a class="reference internal" href="#id1480" title="Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191, 2020.">QDL+20</a>]</span> proposed to utilize a well-trained, complex  model (e.g., a cross-encoder) to determine if an initially retrieved hard-negative is a false negative. Such models are more powerful for capturing semantic similarity among query and documents. Although they are less ideal for deployment and inference purpose due to high computational cost, they are suitable for filtering. From the initially retrieved hard-negative documents, we can filter out documents that are actually relevant to the query. The resulting documents can be used as denoised hard negatives.</p>
</section>
<section id="false-positives">
<h4><span class="section-number">25.5.4.2. </span>False Positives<a class="headerlink" href="#false-positives" title="Link to this heading">#</a></h4>
<p>Because of the noise in the labeling process (e.g., based on click data), it is also possible that a positive labeled document turns out to be irrelevant. To reduce false positive examples, one can develop more robust labeling process and merge labels from multiple sources of signals.</p>
</section>
</section>
<section id="data-augmentation">
<h3><span class="section-number">25.5.5. </span>Data Augmentation<a class="headerlink" href="#data-augmentation" title="Link to this heading">#</a></h3>
<p>To alleviate the issue of limited labeled training data for bi-encoder, one can leverage the following strategy:</p>
<ul class="simple">
<li><p>Use existing bi-encoder to retrieve top-<span class="math notranslate nohighlight">\(k\)</span> passages</p></li>
<li><p>Use cross-encoder or LLM to denoise generated queries by predicting relevance label, and only pseudo-label positive and negative pair with high-confidence scores.</p></li>
</ul>
<p>In the case that we want to adapt a generic retrieval models to a highly specialized domain (e.g., medical, law, scientific), we can consider  a LLM-based approach<span id="id48">[<a class="reference internal" href="#id77" title="Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.">DZM+22</a>]</span>, PROMPTAGATOR, to enhance task-specific retrievers.</p>
<p>As shown in the <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-fig-promptagator-demo"><span class="std std-numref">Fig. 25.21</span></a>, PROMPTAGATOR consists of three components:</p>
<ul class="simple">
<li><p>Prompt-based query generation, a task-specific prompt will be combined with a LLM to produce queries for all documents or passages.</p></li>
<li><p>Consistency filtering, which cleans the generated data based on round-trip consistency - query should be answered by the passage from which the query was generated.</p></li>
<li><p>Retriever training, in which a retriever will be trained using the filtered synthetic data.</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-fig-promptagator-demo">
<a class="reference internal image-reference" href="../../_images/promptagator_training1.png"><img alt="../../_images/promptagator_training1.png" src="../../_images/promptagator_training1.png" style="width: 765.8px; height: 225.39999999999998px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.21 </span><span class="caption-text">Illustration of PROMPTAGATOR, which generates synthetic data using LLM. Synthetic data, after consistency filtering, is used to train a retriever in labeled data scarcity domain. Image from <span id="id49">[<a class="reference internal" href="#id77" title="Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.">DZM+22</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-fig-promptagator-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="knowledge-distillation">
<h2><span class="section-number">25.6. </span>Knowledge Distillation<a class="headerlink" href="#knowledge-distillation" title="Link to this heading">#</a></h2>
<section id="id50">
<h3><span class="section-number">25.6.1. </span>Introduction<a class="headerlink" href="#id50" title="Link to this heading">#</a></h3>
<p>Knowledge distillation aims to transfer knowledge from a well-trained, high-performing yet cumbersome teacher model to a lightweight student model with significant performance loss. Knowledge distillation has been a widely adopted method to achieve efficient neural network architecture, thus reducing overall inference costs, including memory requirements as well as inference latency. Typically, the teacher model can be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as dropout. The student model uses the distilled knowledge from the teacher network as additional learning cues. The resulting student model is computationally inexpensive and has accuracy better than directly training it from scratch.</p>
<p>As such, tor retrieval and ranking systems, knowledge distillation is a desirable approach to develop efficient models to meet the high requirement on both accuracy and latency.</p>
<p>For example, one can distill knowledge from a more powerful cross-encoder (e.g., BERT cross-encoder in <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-monobert"><span class="std std-numref">Section 25.2.3</span></a>) to a computational efficient bi-encoders. Empirically, this two-step procedure might be more effective than directly training a bi-encoder from scratch.</p>
<p>In this section, we first review the principle of knowledge distillation. Then we go over a couple examples to demonstrate the application of knowledge distillation in developing retrieval and ranking models.</p>
</section>
<section id="knowledge-distillation-training-framework">
<h3><span class="section-number">25.6.2. </span>Knowledge Distillation Training Framework<a class="headerlink" href="#knowledge-distillation-training-framework" title="Link to this heading">#</a></h3>
<p>In the classic knowledge distillation framework <span id="id51">[<a class="reference internal" href="#id1304" title="Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.">HVD15</a>, <a class="reference internal" href="#id1479" title="Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019.">TLL+19</a>]</span>[<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-teacherstudentdistillationscheme"><span class="std std-numref">Fig. 25.22</span></a>], the fundamental principle is that the teacher model produces soft label <span class="math notranslate nohighlight">\(q\)</span> for each input feature <span class="math notranslate nohighlight">\(x\)</span>. Soft label <span class="math notranslate nohighlight">\(q\)</span> can be viewed as a softened probability vector distributed over class labels of interest.
Soft targets contain valuable information on the rich similarity structure over the data. Use MNIST classification as an example, a reasonable soft target will tell that 2 looks more like 3 than 9. These soft targets can be viewed as a strategy to mitigate the over-confidence issue and reduce gradient variance when we train neural networks using one-hot hard labels. Similar mechanism is leveraged in smooth label to improves model generalization.</p>
<p>Allows the smaller Student model to be trained on much smaller data than the original cumbersome model and with a much higher learning rate</p>
<p>Specifically, the logits <span class="math notranslate nohighlight">\(z\)</span> from the techer model are outputted to generate soft labels via</p>
<div class="math notranslate nohighlight">
\[
q_{i}^T=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the temperature parameter controlling softness of the probability vector, and the sum is over the entire label space. When <span class="math notranslate nohighlight">\(T=1\)</span>, it is equivalent to standard Softmax function. As <span class="math notranslate nohighlight">\(T\)</span> grows, <span class="math notranslate nohighlight">\(q\)</span> become softer and approaches uniform distribution <span class="math notranslate nohighlight">\(T=\infty\)</span>. On the other hand, as <span class="math notranslate nohighlight">\(T\to 0\)</span>, the <span class="math notranslate nohighlight">\(q\)</span> approaches a one-hot hard label.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-teacherstudentdistillationscheme">
<a class="reference internal image-reference" href="../../_images/teacher_student_distillation_scheme.png"><img alt="../../_images/teacher_student_distillation_scheme.png" src="../../_images/teacher_student_distillation_scheme.png" style="width: 377.09999999999997px; height: 378.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.22 </span><span class="caption-text">The classic teacher-student knowledge distillation framework.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-teacherstudentdistillationscheme" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The loss function for the student network training is a weighted sum of the hard label based cross entropy loss and soft label based KL divergence. The rationale of KL (Kullback-Leibler) divergence is to use the softened probability vector from the teacher model to guide the learning of the student network. Minimizing the KL divergence constrains the student model’s probabilistic outputs to match soft targets of the teacher model.</p>
<p>The loss function is formally given by</p>
<div class="math notranslate nohighlight">
\[L =(1-\alpha) L_{C E}\left(p, y\right)+\alpha T^{2} {L}_{K L}\left(p^T, q^T\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(L_{CE}\)</span> is the regular cross entropy loss between predicted probability vector <span class="math notranslate nohighlight">\(p\)</span> and the one-hot label vector</p>
<div class="math notranslate nohighlight">
\[{L}_{CE}\left(p, y\right) =-\sum_{j}{y}_{j} \log {p}_{j};\]</div>
<p><span class="math notranslate nohighlight">\(L_{KL}\)</span> is the KL divergence loss between the softened predictions at temperature <span class="math notranslate nohighlight">\(T\)</span> from the student and the teacher networks, respectively:</p>
<div class="math notranslate nohighlight">
\[{L}_{KL}\left({p}^T, q^{T}\right) = -\sum_{j} {q}_{j}^{T} \log \frac{p_{j}^T}{{q}_{j}^{T}}.\]</div>
<p>Note that the same high temperature is used to produce distributions from the student model.</p>
<p>Note that <span class="math notranslate nohighlight">\(L_{KL}(p^T, q^T) = L_{CE}(p^T, q^T) + H(q^T, q^T)\)</span>, with <span class="math notranslate nohighlight">\(H(q^T, q^T)\)</span> being the entropy of probability vector <span class="math notranslate nohighlight">\(q^T\)</span> and remaining as a constant during the training. As a result, we also often reduce the total loss to</p>
<div class="math notranslate nohighlight">
\[L =(1-\alpha) L_{C E}\left(p, y\right)+\alpha T^{2} {L}_{CE}\left(p^T, q^T\right).\]</div>
<p>Finally, the multiplier <span class="math notranslate nohighlight">\(T^2\)</span> is used to re-scale the gradient of KL loss and <span class="math notranslate nohighlight">\(\alpha\)</span> is a scalar controlling the weight contribution to each loss.</p>
<p>Besides using softened probability vector and KL divergence loss to guide the student learning process, we can also use MSE loss between the logits from the teacher and the student networks. Specifically,
$<span class="math notranslate nohighlight">\(L_{MSE} = ||z^{(T)} - z^{(S)}||^2\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(z^{(T)}\)</span> and <span class="math notranslate nohighlight">\(z^{(S)}\)</span> are logits from the teacher and the student network, respectively.</p>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 25.1 </span> (connections between MSE loss and KL loss)</p>
<section class="remark-content" id="proof-content">
<p>In <span id="id52">[<a class="reference internal" href="#id1304" title="Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.">HVD15</a>]</span>, given a single sample input feature <span class="math notranslate nohighlight">\(x\)</span>, the gradient of <span class="math notranslate nohighlight">\({L}_{KL}\)</span> with respect to <span class="math notranslate nohighlight">\(z_{k}^{(S)}\)</span> is as follows:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial {L}_{KL}}{\partial {z}_{k}^{s}}=T\left(p_{k}^{T}-{q}_{k}^{T}\right).
\]</div>
<p>When <span class="math notranslate nohighlight">\(T\)</span> goes to <span class="math notranslate nohighlight">\(\infty\)</span>, using the approximation <span class="math notranslate nohighlight">\(\exp \left({z}_{k}/ T\right) \approx 1+{z}_{k} / T\)</span>, the gradient is simplified to:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial {L}_{KL}}{\partial {z}_{k}^{(S)}} \approx T\left(\frac{1+z_{k}^{(S)} / T}{K+\sum_{j} {z}_{j}^{(S)} / T}-\frac{1+{z}_{k}^{(T)} / T}{K+\sum_{j} {z}_{j}^{(T)} / T}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the number of classes.</p>
<p>Here, by assuming the zero-mean teacher and student logit, i.e., <span class="math notranslate nohighlight">\(\sum_{j} {z}_{j}^{(T)}=0\)</span> and <span class="math notranslate nohighlight">\(\sum_{j} {z}_{j}^{(S)}=0\)</span>, and hence <span class="math notranslate nohighlight">\(\frac{\partial {L}_{K L}}{\partial {z}_{k}^{(S)}} \approx \frac{1}{K}\left({z}_{k}^{(S)}-{z}_{k}^{(T)}\right)\)</span>. This indicates that minimizing <span class="math notranslate nohighlight">\({L}_{KL}\)</span> is equivalent to minimizing the mean squared error <span class="math notranslate nohighlight">\({L}_{MSE}\)</span>, under a sufficiently large temperature <span class="math notranslate nohighlight">\(T\)</span> and the zero-mean logit assumption for both the teacher and the student.</p>
</section>
</div></section>
<section id="example-distillation-strategies">
<h3><span class="section-number">25.6.3. </span>Example Distillation Strategies<a class="headerlink" href="#example-distillation-strategies" title="Link to this heading">#</a></h3>
<!-- #### Single Cross-encoder Teacher Distillation

```{figure} ../img/chapter_application_IR/ApplicationIRSearch/KnowledgeDistllation/cross_encoder_distillation.png
:scale: 30%
:name: fig:crossencoderdistillation

``` -->
<section id="bi-encoder-teacher-distillation">
<h4><span class="section-number">25.6.3.1. </span>Bi-encoder Teacher Distillation<a class="headerlink" href="#bi-encoder-teacher-distillation" title="Link to this heading">#</a></h4>
<p>Authors in <span id="id53">[<a class="reference internal" href="#id1457" title="Wenhao Lu, Jian Jiao, and Ruofei Zhang. Twinbert: distilling knowledge to twin-structured bert models for efficient retrieval. arXiv preprint arXiv:2002.06275, 2020.">LJZ20</a>, <a class="reference internal" href="#id1456" title="Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. Distilling knowledge for fast retrieval-based chat-bots. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2081–2084. 2020.">VTGS20</a>]</span> pioneered the strategy of distilling powerful BERT cross-encoder into BERT bi-encoder to retain the benefits of the two model architectures: the accuracy of cross-encoder and the efficiency of bi-encoder.</p>
<p>Knowledge distillation follows the classic soft label framework. Bi-encoder student model training can use pointwise ranking loss, which is equivalent to binary relevance classification problem given a query and a candidate document. More formally, given training examples <span class="math notranslate nohighlight">\((q_i, d_i)\)</span> and their labels <span class="math notranslate nohighlight">\(y_i\in \{0, 1\}\)</span>. The BERT cross-encoder as teacher model to produce soft targets for irrelevance label and relevance label.</p>
<p>Although cross-encoder teacher can offer accurate soft labels, it cannot directly extend to the <strong>in-batch negatives</strong> technique and <strong>N-pair loss</strong> [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-n-pair-loss"><span class="std std-numref">Section 25.4.3.4</span></a>] when training the student model. The reason is that query and document embedding cannot be computed separately from a cross-encoder. <strong>Implementing in-batch negatives using cross-encoder requires exhaustive computation</strong> on all combinations between a query and possible documents, which amount to <span class="math notranslate nohighlight">\(|B|^2\)</span> (<span class="math notranslate nohighlight">\(|B|\)</span> is the batch size) query-document pairs.</p>
<p>Authors in <span id="id54">[<a class="reference internal" href="#id1478" title="Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), 163–173. 2021.">LYL21</a>]</span> proposed to leverage bi-encoder variant such as Col-BERT [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-colbert"><span class="std std-numref">Section 25.3.2</span></a>] as a teacher model, which is more feasible to perform exhaustive comparisons between queries and passages since they are passed through the encoder independently [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchdistillation"><span class="std std-numref">Fig. 25.23</span></a>].</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchdistillation">
<a class="reference internal image-reference" href="../../_images/in_batch_distillation.png"><img alt="../../_images/in_batch_distillation.png" src="../../_images/in_batch_distillation.png" style="width: 693.6px; height: 341.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.23 </span><span class="caption-text">Compared to cross-encoder teacher, bi-encoder teacher computes query and document embeddings independents, which enables the application of the in-batch negative trick. Image from <span id="id55">[<a class="reference internal" href="#id1478" title="Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), 163–173. 2021.">LYL21</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-inbatchdistillation" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="cross-encoder-embedding-similarity-distillation">
<h4><span class="section-number">25.6.3.2. </span>Cross-Encoder Embedding Similarity Distillation<a class="headerlink" href="#cross-encoder-embedding-similarity-distillation" title="Link to this heading">#</a></h4>
<p>While bi-encoder teacher can offer efficiency in producing on-the-fly teacher scores, it sacrifaces the interaction modeling abiity from cross-encoders. On the other hand, directly using cross-encoder to produce binary classficiation logics as the distillation target does not fully leverage other useful information in the teacher model.</p>
<p>To mitigate this, one can</p>
<ul class="simple">
<li><p>Having a spealized cross-encoder teacher to produce query and document embeddings</p></li>
<li><p>Enforce closeness between student and teacher on query/doc embedding vectors (e.g., via cosine similarity distance)</p></li>
<li><p>Enforce closeness between student and teacher on query-doc embedding similarity scores (e.g., via MSE)</p></li>
</ul>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-crossencoderdistillation">
<a class="reference internal image-reference" href="../../_images/cross_encoder_distillation.png"><img alt="../../_images/cross_encoder_distillation.png" src="../../_images/cross_encoder_distillation.png" style="width: 711.9px; height: 209.7px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.24 </span><span class="caption-text">Illustration of leveraging rich information from a cross-encoder teacher for knowledge distillation.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-crossencoderdistillation" title="Link to this image">#</a></p>
</figcaption>
</figure>
 <!-- One can see that such cross-encoder teacher is designed for retrieval purpose, which can be train -->
</section>
<section id="ensemble-teacher-distillation">
<h4><span class="section-number">25.6.3.3. </span>Ensemble Teacher Distillation<a class="headerlink" href="#ensemble-teacher-distillation" title="Link to this heading">#</a></h4>
<p>As we have seen in previous sections, large Transformer based models such as BERT cross-encoders or bi-encoders are popular choices of teacher models when we perform knowledge distillation. These fine-tuned BERT models often show high performance variances across different runs. From ensemble learning perspective, using an ensemble of models as a teacher model could potentially not only achieves better distillation results, but also reduces the performance variances.</p>
<p>The critical challenge arising from distilling an ensemble teacher model vs a single teacher model is how to reconcile soft target labels generated by different teacher models.</p>
<p>Authors in <span id="id56">[<a class="reference internal" href="#id1498" title="Honglei Zhuang, Zhen Qin, Shuguang Han, Xuanhui Wang, Michael Bendersky, and Marc Najork. Ensemble distillation for bert-based ranking models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, 131–136. 2021.">ZQH+21</a>]</span> propose following method to fuse scores and labels. Formally, consider query <span class="math notranslate nohighlight">\(q_{i}\)</span>, its <span class="math notranslate nohighlight">\(j\)</span>-th candidate document <span class="math notranslate nohighlight">\(d_{i j}\)</span>, and <span class="math notranslate nohighlight">\(K\)</span> teacher models. Let the predicted ranking score by the <span class="math notranslate nohighlight">\(k\)</span>-th teacher be represented as <span class="math notranslate nohighlight">\(\hat{s}_{i j}^{(k)}\)</span>.</p>
<p>The simplest aggregated teacher label is to directly use the mean score, namely</p>
<div class="math notranslate nohighlight">
\[
s_{i j}=\frac{1}{K}\hat{s}_{i j}^{(k)}.
\]</div>
<p>The simple average scheme would work poorly when teacher models can have outputs with very different scales. A more robust way to fuse scores is to reciprocal rank, given by</p>
<div class="math notranslate nohighlight">
\[
s_{i j}^{(k)}=\frac{1}{K} \sum_{k=1}^{K}\frac{1}{C+\hat{r}_{i j}^{(k)}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{r}_{i j}^{(k)}\)</span> is the predicted rank of the <span class="math notranslate nohighlight">\(j\)</span>-th candidate text by the <span class="math notranslate nohighlight">\(k\)</span>-th teacher, and <span class="math notranslate nohighlight">\(C\)</span> is the constant as model hyperparameters.</p>
<p>With the fused score, softened probability vector can be obtained by taking Softmax with temperature as the scaling factor.</p>
</section>
</section>
</section>
<section id="pretraining-for-retrieval">
<h2><span class="section-number">25.7. </span>Pretraining for Retrieval<a class="headerlink" href="#pretraining-for-retrieval" title="Link to this heading">#</a></h2>
<section id="contriever">
<h3><span class="section-number">25.7.1. </span>Contriever<a class="headerlink" href="#contriever" title="Link to this heading">#</a></h3>
<p>Contriever <span id="id57">[<a class="reference internal" href="#id1661" title="Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.">ICH+21</a>]</span> explores the limits of contrastive pre-training to learn dense text retrievers. Key tecchniques include positive example generation from unlabeled text, large-scale negative sampling, and pretraining text selection and preparation.</p>
<p>Two <strong>positive example generation</strong> techniques are considered:</p>
<ul class="simple">
<li><p><strong>Inverse Cloze Task</strong> - Given a sequence of text <span class="math notranslate nohighlight">\(\left(w_1, \ldots, w_n\right)\)</span>, ICT samples a span <span class="math notranslate nohighlight">\(\left(w_a, \ldots, w_b\right)\)</span>, where <span class="math notranslate nohighlight">\(1 \leq a \leq b \leq n\)</span>, uses the tokens of the span as the query and the complement <span class="math notranslate nohighlight">\(\left(w_1, \ldots, w_{a-1}, w_{b+1}, \ldots, w_n\right)\)</span> as the positive example.</p></li>
<li><p><strong>Random Cropping</strong> - Samples independently two spans from a document to form a positive pair.</p></li>
</ul>
<p>To enable the construction of <strong>large-scale negative samples</strong>, MoCo technique (also see <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-sec-momentum-negatives"><span class="std std-ref">Momentum Negatives</span></a>) is used to train models with a large number of negative examples. Number of negative examples are ranging from 2,048 to 131,072.</p>
<p><strong>Pretraining dataset</strong> consists of Wikipedia and CCNet <span id="id58">[<a class="reference internal" href="#id1660" title="Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.">WLC+19</a>]</span>. CCNet extracts clean text from Common Crawl wet files and cleans them using a pretrained 5-gram model pretrained on Wikipedia over 18 different languages - by filtering perplexity lower than an given threshold. The model filters out bad quality texts such as code or tables. <strong>FastText</strong> is used for language identification and deduplication using hash of the content.</p>
<p>Key observations are:</p>
<ul class="simple">
<li><p>Neural networks trained without supervision using contrastive learning exhibit good retrieval performance, which are competitive with BM25 (albeit not state-of-the-art).</p></li>
<li><p>The number of negatives leads to better retrieval performance, especially in the unsupervised setting. However, this effect is not equally strong for all datasets.</p></li>
<li><p>These results can be further improved by fine-tuning on the supervised MS MARCO dataset, leading to strong results, in particular for recall&#64;100.</p></li>
</ul>
</section>
</section>
<section id="discussion-sparse-and-dense-retrieval">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-retriever-comparison"></span><h2><span class="section-number">25.8. </span>Discussion: Sparse and Dense Retrieval<a class="headerlink" href="#discussion-sparse-and-dense-retrieval" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Aspects</p></th>
<th class="head text-center"><p>BM25</p></th>
<th class="head text-center"><p>Bi-Encoder</p></th>
<th class="head text-center"><p>Cross-Encoder</p></th>
<th class="head text-center"><p>Late-Interaction BiEncoder (Colbert)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Speed</p></td>
<td class="text-center"><p>Fast</p></td>
<td class="text-center"><p>Fast</p></td>
<td class="text-center"><p>Slow</p></td>
<td class="text-center"><p>Medium</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Training needed</p></td>
<td class="text-center"><p>N</p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>Y</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>OOD Generation</p></td>
<td class="text-center"><p>Strong</p></td>
<td class="text-center"><p>Weak</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Medium</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Semantic Understanding</p></td>
<td class="text-center"><p>Weak</p></td>
<td class="text-center"><p>Strong</p></td>
<td class="text-center"><p>Very strong</p></td>
<td class="text-center"><p>Very strong</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Scability</p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>N</p></td>
<td class="text-center"><p>Medium</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Performance</p></td>
<td class="text-center"><p>Consistent medium</p></td>
<td class="text-center"><p>Decent for in-domain</p></td>
<td class="text-center"><p>Strong</p></td>
<td class="text-center"><p>Strong</p></td>
</tr>
</tbody>
</table>
</div>
<p>In terms of semantic understanding, cross-Encoder sets the upperbound for Late-Interaction Bi-Encoder model.</p>
<p>The importance of Out-of-distribution generation, a robust retriever should have reliable performance on tail topic and tail queries.</p>
<p>When dense model encode a query or document into a fixed length vector, it usually have the following limitations:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Aspects</p></th>
<th class="head text-center"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>When a query is a short and tail-ish, query embedding is of low quality as the model barely gets exposed to such query during training.</p></td>
<td class="text-center"><p>one word query <em>laresar</em> is a navigation query looking for a Chinese high-tech company.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>When a query is long, specific, and invovles multi-concept with complex relationships or modifiers, the query embedding is insensitive to variations of concept relationships</p></td>
<td class="text-center"><p><em>The cause of climate change and its impact on marine life in the Altantic sea</em> vs <em>The cause of climate change and its impact on marine life</em> will have high cosine similar embeddings</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p><em>What are some good rap songs to dance to?</em> vs <em>What are some of the best rap songs?</em> have high similarity score; but they mean different things.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><em>What are the types of immunity?</em> vs <em>What are the different types of immunity in our body?</em> - the former one is more broad, including immunity in the socieity sense.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Long queries are semantically similar but have many different lexical terms</p></td>
<td class="text-center"><p><em>What would a Trump presidency mean for current international master’s students on an F1 visa?</em> vs <em>How will a Trump presidency affect the students presently in US or planning to study in US?</em> will have relatively low similarity scores</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><em>Why does China block sanctions at the UN against the Masood Azhar?</em> vs <em>Why does China support Masood Azhar?</em> have low similarity score even if they have the same meaning.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Different query intent due to slight variation on grammar, word order, and word choice</p></td>
<td class="text-center"><p><em>How do I prevent breast cancer?</em> vs <em>Is breast cancer preventable?</em> have different intent but high similarity score.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><em>How can I transfer money from Skrill to a PayPal account?</em> vs <em>How can I send money from my PayPal account to my Skrill account to withdraw?</em> have high similarity score, but they mean different directions. Similarly, <em>‘How do I switch from Apple Music to Spotify?’</em> vs <em>Should I switch from Spotify to Apple Music?</em></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Queries involving external knowledge that is not in the model</p></td>
<td class="text-center"><p><em>How do we prepare for UPSC?</em> and <em>How do I prepare for civil service?</em> have low similarity score despite the fact that UPSC and civil service mean the same exam in India</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><em>How competitive is the hiring process at Republic Bank?</em> vs <em>How competitive is the hiring process at S &amp; T Bank?</em> have high similarity score but Republic Bank and S &amp; T bank are different entities.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>When a query’s intent shall be interpreted deeply</p></td>
<td class="text-center"><p><em>My mom wished that I would die</em> has the intent of seeking relationship consultation.</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p>When a document is long and being compressed into a single dense vector, there will be innevitably information loss.</p></li>
</ul>
<section id="index-size-and-embedding-dimensionality-impact">
<h3><span class="section-number">25.8.1. </span>Index Size and Embedding Dimensionality Impact<a class="headerlink" href="#index-size-and-embedding-dimensionality-impact" title="Link to this heading">#</a></h3>
<p>In dense retrieval, query and documents are compressed into low-dimensionality, denoted by <span class="math notranslate nohighlight">\(k\)</span>, space. Query and document similarity are computed using length-normalized vectors. Geometrically, each query and documents are residing on the surface of a <span class="math notranslate nohighlight">\(k\)</span>-dimension hyper-space. Intuitively, the larger the index size <span class="math notranslate nohighlight">\(n\)</span> and the smaller <span class="math notranslate nohighlight">\(k\)</span>, irrelevant documents more likely to be returned.
Authors from <span id="id59">[<a class="reference internal" href="#id1655" title="Nils Reimers and Iryna Gurevych. The curse of dense low-dimensional information retrieval for large index sizes. arXiv preprint arXiv:2012.14210, 2020.">RG20</a>]</span> show that, both theorically and emprically,</p>
<ul class="simple">
<li><p>The probability of retrieving an irrelevant doc will increase with index size <span class="math notranslate nohighlight">\(n\)</span>;</p></li>
<li><p>The probability of retrieving an irrelevant doc will increase with dimensionality <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ul>
<p>The emprically findings are summarized in the table below.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Dense Model Setting</p></th>
<th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>10k</p></th>
<th class="head text-center"><p>100k</p></th>
<th class="head text-center"><p>1M</p></th>
<th class="head text-center"><p>8.8M</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Sparse model</p></td>
<td class="text-left"><p>BM25</p></td>
<td class="text-center"><p>79.93</p></td>
<td class="text-center"><p>63.88</p></td>
<td class="text-center"><p>40.14</p></td>
<td class="text-center"><p>17.56</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Trained without hard negatives</p></td>
<td class="text-left"><p>128 dim</p></td>
<td class="text-center"><p>87.50</p></td>
<td class="text-center"><p>68.63</p></td>
<td class="text-center"><p>39.76</p></td>
<td class="text-center"><p>15.71</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>256 dim</p></td>
<td class="text-center"><p>88.82</p></td>
<td class="text-center"><p>70.79</p></td>
<td class="text-center"><p>41.74</p></td>
<td class="text-center"><p>17.08</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>768 dim</p></td>
<td class="text-center"><p>88.99</p></td>
<td class="text-center"><p>71.06</p></td>
<td class="text-center"><p>42.24</p></td>
<td class="text-center"><p>17.34</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Trained with hard negatives</p></td>
<td class="text-left"><p>128 dim</p></td>
<td class="text-center"><p>90.32</p></td>
<td class="text-center"><p>77.92</p></td>
<td class="text-center"><p>54.45</p></td>
<td class="text-center"><p>27.34</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>256 dim</p></td>
<td class="text-center"><p>91.10</p></td>
<td class="text-center"><p>78.90</p></td>
<td class="text-center"><p>55.51</p></td>
<td class="text-center"><p>28.16</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>768 dim</p></td>
<td class="text-center"><p>91.48</p></td>
<td class="text-center"><p>79.42</p></td>
<td class="text-center"><p>56.05</p></td>
<td class="text-center"><p>28.55</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="approximate-nearest-neighbor-search">
<span id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-part2-ann-search"></span><h2><span class="section-number">25.9. </span>Approximate Nearest Neighbor Search<a class="headerlink" href="#approximate-nearest-neighbor-search" title="Link to this heading">#</a></h2>
<section id="id60">
<h3><span class="section-number">25.9.1. </span>Overview<a class="headerlink" href="#id60" title="Link to this heading">#</a></h3>
<p>Applying dense retrieval in the first-stage of the ad-hoc retrieval system involves performing nearest neighbor search among web-scale documents in the high-dimensional embedding space. Exact nearest neighbor search is inherently expensive due to the curse of dimensionality and the large number of documents. Consider a <span class="math notranslate nohighlight">\(D\)</span>-dimensional Euclidean space <span class="math notranslate nohighlight">\(\mathbb{R}^{D}\)</span>, the problem is to find the nearest element <span class="math notranslate nohighlight">\(\mathrm{NN}(x)\)</span>, in a finite set <span class="math notranslate nohighlight">\(\mathcal{Y} \subset \mathbb{R}^{D}\)</span> of <span class="math notranslate nohighlight">\(n\)</span> vectors, minimizing the distance to the query vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{D}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{NN}(x)=\arg \min _{y \in \mathcal{Y}} d(x, y).
\]</div>
<p>A brute force exhaustive distance calculation has the complexity of <span class="math notranslate nohighlight">\(\mathcal{O}(n D)\)</span>. Several multi-dimensional indexing methods, such as the popular KD-tree <span id="id61">[<a class="reference internal" href="#id1511" title="Jerome H Friedman, Jon Louis Bentley, and Raphael Ari Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209–226, 1977.">FBF77</a>]</span> or other branch and bound techniques, have been proposed to reduce the search time. However, nowadays the dominating approaches are approximate nearest neighbor search via vector quantization, which is the primary focus of this section.</p>
</section>
<section id="vector-quantization">
<h3><span class="section-number">25.9.2. </span>Vector Quantization<a class="headerlink" href="#vector-quantization" title="Link to this heading">#</a></h3>
<section id="approximate-representation-and-storage">
<h4><span class="section-number">25.9.2.1. </span>Approximate Representation And Storage<a class="headerlink" href="#approximate-representation-and-storage" title="Link to this heading">#</a></h4>
<p>Quantization is a technique widely used to reduce the cardinality of high dimensional representation space, in particular when the input data is real-valued.</p>
<p>Formally, a <strong>quantizer</strong> is a function <span class="math notranslate nohighlight">\(q\)</span> mapping a multi-dimensional vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{D}\)</span> to a pre-defined centroid <span class="math notranslate nohighlight">\(q(x) = c_i\)</span>, where <span class="math notranslate nohighlight">\(c_i \in \mathcal{C} = \{c_1,...,c_{k}\}\)</span>.
The values <span class="math notranslate nohighlight">\(c_i \in \mathbb{R}^D\)</span> are called <strong>centroids</strong>, and the set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is the <em>codebook</em> of size <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>The set <span class="math notranslate nohighlight">\(\mathcal{V}_{i}\)</span> of vectors mapped to a given index <span class="math notranslate nohighlight">\(i\)</span> is referred to as a (Voronoi) cell, and defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_{i} \triangleq\left\{x \in \mathbb{R}^{D}: q(x)=c_{i}\right\}.
\]</div>
<p>The <span class="math notranslate nohighlight">\(k\)</span> cells of a quantizer form a partition of <span class="math notranslate nohighlight">\(\mathbb{R}^{D}\)</span>. By definition, all the vectors lying in the same cell <span class="math notranslate nohighlight">\(\mathcal{V}_{i}\)</span> are reconstructed by the same centroid <span class="math notranslate nohighlight">\(c_{i}\)</span>. That is, they are all <strong>approximated</strong> by centroid <span class="math notranslate nohighlight">\(c_i\)</span>.</p>
<p>How do we measure the approximation quality of such representation? The quality of a quantizer is usually measured by the mean squared error between the input vector <span class="math notranslate nohighlight">\(x\)</span> and its reproduction value <span class="math notranslate nohighlight">\(q(x):\)</span></p>
<div class="math notranslate nohighlight">
\[
\operatorname{MSE}(q)=\mathbb{E}_{X}\left[d(q(x), x)^{2}\right]=\int p(x) d(q(x), x)^{2} d x
\]</div>
<p>where <span class="math notranslate nohighlight">\(d(x, y)=\|x-y\|\)</span> is the Euclidean distance between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and where <span class="math notranslate nohighlight">\(p(x)\)</span> is the probability distribution function corresponding the randomly sampled <span class="math notranslate nohighlight">\(x\)</span>. Approximate calculation of the integral can be achieved by Monte-Carlo sampling.</p>
<p>In order for the quantizer to be optimal, it has to satisfy two properties known as the Lloyd optimality conditions. First, a vector <span class="math notranslate nohighlight">\(x\)</span> must be quantized to its nearest codebook centroid, in terms of the Euclidean distance:</p>
<div class="math notranslate nohighlight">
\[
q(x)=\arg \min _{\mathrm{s}_{i} \in \mathcal{C}} d\left(x, c_{1}\right)
\]</div>
<p>As a result, the cells are delimited by hyperplanes. The second Lloyd condition is that the reconstruction value must be the expectation of the vectors lying in the Voronoi cell:</p>
<div class="math notranslate nohighlight">
\[
c_{i}=\mathbb{E}_{X}[x \mid i]=\int_{V_{i}} p(x) x d x .
\]</div>
<p>The Lloyd quantizer, which corresponds to the <strong><span class="math notranslate nohighlight">\(k\)</span>-means clustering algorithm</strong>, finds a near-optimal codebook by iteratively assigning the vectors of a training set to centroids and re-estimating these centroids from the assigned vectors.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookconstruction">
<img alt="../../_images/codebook_construction.png" src="../../_images/codebook_construction.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.25 </span><span class="caption-text">Codecook construction can be achieved using k-means algorithm to compute <span class="math notranslate nohighlight">\(K\)</span> centroids from database vectors.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookconstruction" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>storage</strong> for <span class="math notranslate nohighlight">\(N\)</span> vectors now reduce to storage of their index values plus the centroids in the codebook. Each index value requires <span class="math notranslate nohighlight">\(\log_{2} k\)</span> bits. On the other hand, storing the original vectors typically take more than <span class="math notranslate nohighlight">\(\log_2(k)\)</span> bits.</p>
<p>Two important benefits of compressing the dataset are</p>
<ul class="simple">
<li><p>Memory access times are generally the limiting factor on processing speed; With compression, the processing speed will be drastically accelerated.</p></li>
<li><p>Reduce sheer memory capacity for big datasets.</p></li>
</ul>
<p>In <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemo"><span class="std std-numref">Fig. 25.26</span></a>, we illustrate the storage saving by representing a <span class="math notranslate nohighlight">\(D\)</span> dimensional vector by a codebook of 256 centroids. We only need 8-bits (<span class="math notranslate nohighlight">\(2^8 = 256\)</span>) to store a centroid id. Each vector is now replace by a 8-bit integers.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemo">
<a class="reference internal image-reference" href="../../_images/codebook_memory_saving_demo.png"><img alt="../../_images/codebook_memory_saving_demo.png" src="../../_images/codebook_memory_saving_demo.png" style="width: 333.9px; height: 136.79999999999998px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.26 </span><span class="caption-text">Illustration of memory saving benefits of vector quantization. A <span class="math notranslate nohighlight">\(D\)</span>-dimensional float vector is stored as its nearest centroid integer id, which only occupies <span class="math notranslate nohighlight">\(\log_2 k\)</span> bit.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="approximating-distances-using-quantized-codes">
<h4><span class="section-number">25.9.2.2. </span>Approximating Distances Using Quantized Codes<a class="headerlink" href="#approximating-distances-using-quantized-codes" title="Link to this heading">#</a></h4>
<p>Given the representation choices for the query vector <span class="math notranslate nohighlight">\(x\)</span> and the database vector <span class="math notranslate nohighlight">\(y\)</span>, we define two options in approximating the distance <span class="math notranslate nohighlight">\(d(x, y)\)</span> [<a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-distance-compute"><span class="std std-numref">Fig. 25.27</span></a>].</p>
<p><strong>Symmetric distance computation (SDC)</strong>: both the vectors <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are represented by their respective centroids <span class="math notranslate nohighlight">\(q(x)\)</span> and <span class="math notranslate nohighlight">\(q(y)\)</span>. The distance <span class="math notranslate nohighlight">\(d(x, y)\)</span> is approximated by the distance <span class="math notranslate nohighlight">\(\hat{d}(x, y) \triangleq d(q(x), q(y))\)</span>.</p>
<p><strong>Asymmetric distance computation (ADC)</strong>: the database vector <span class="math notranslate nohighlight">\(y\)</span> is represented by <span class="math notranslate nohighlight">\(q(y)\)</span>, but the query <span class="math notranslate nohighlight">\(x\)</span> is not encoded. The distance <span class="math notranslate nohighlight">\(d(x, y)\)</span> is approximated by the distance <span class="math notranslate nohighlight">\(\tilde{d}(x, y) \triangleq d(x, q(y))\)</span></p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-distance-compute">
<a class="reference internal image-reference" href="../../_images/distance_compute.png"><img alt="../../_images/distance_compute.png" src="../../_images/distance_compute.png" style="width: 577.8px; height: 260.09999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.27 </span><span class="caption-text">Illustration of the symmetric (left) and asymmetric distance (right) computation. The distance <span class="math notranslate nohighlight">\(d(x, y)\)</span> is estimated with either the distance <span class="math notranslate nohighlight">\(d(q(x), q(y))\)</span> (left) or the distance <span class="math notranslate nohighlight">\(d(x, q(y))\)</span> (right).</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-distance-compute" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Suppose now we have a query vector <span class="math notranslate nohighlight">\(x\)</span> and we want to find its nearest neighbors among all the <span class="math notranslate nohighlight">\(y\)</span> in the database <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p>There are benefits in performing symmetric distance computation. To perform symmetric distance computation, we can pre-compute a <span class="math notranslate nohighlight">\(K\times K\)</span> table to cache the Euclidean distance between all centroids. After computing the encoding <span class="math notranslate nohighlight">\(q(x)\)</span>, we can get <span class="math notranslate nohighlight">\(d(q(x), q(y))\)</span> by table lookup.
On the other hand, to perform asymmetric distance computation between <span class="math notranslate nohighlight">\(x\)</span> and all <span class="math notranslate nohighlight">\(y\in \mathcal{Y}\)</span>, we can directly calculate the Euclidean distance between the query vector <span class="math notranslate nohighlight">\(x\)</span> and centroid <span class="math notranslate nohighlight">\(q(y)\)</span> in the codebook.</p>
</section>
</section>
<section id="product-quantization">
<h3><span class="section-number">25.9.3. </span>Product Quantization<a class="headerlink" href="#product-quantization" title="Link to this heading">#</a></h3>
<section id="from-vector-quantization-to-product-quantization">
<h4><span class="section-number">25.9.3.1. </span>From Vector Quantization To Product Quantization<a class="headerlink" href="#from-vector-quantization-to-product-quantization" title="Link to this heading">#</a></h4>
<p>Let us consider a quantizer producing 64 bits codes, i.e., which can contain <span class="math notranslate nohighlight">\(k=2^{64} \approx 1.8\times 10^{19} \)</span> centroids. It is prohibitive run the k-means algorithm and practically impossible to store the <span class="math notranslate nohighlight">\(D \times k\)</span> floating point values representing the <span class="math notranslate nohighlight">\(k\)</span> centroids.</p>
<p>Product quantization serves as an efficient solution to address these computation and memory consumption issues in vector quantization. The key idea of product quantization is <strong>grouping and splitting</strong>[ <a class="reference internal" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemoproductquantization"><span class="std std-numref">Fig. 25.28</span></a>]. The input vector <span class="math notranslate nohighlight">\(x\)</span> is split into <span class="math notranslate nohighlight">\(m\)</span> distinct subvectors <span class="math notranslate nohighlight">\(u_{j}, 1 \leq\)</span> <span class="math notranslate nohighlight">\(j \leq m\)</span> of dimension <span class="math notranslate nohighlight">\(D^{*}=D / m\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is a multiple of <span class="math notranslate nohighlight">\(m\)</span>. The subvectors are then quantized separately using <span class="math notranslate nohighlight">\(m\)</span> distinct quantizers. A given vector <span class="math notranslate nohighlight">\(x\)</span> is therefore quantized as follows:</p>
<div class="math notranslate nohighlight">
\[\underbrace{x_{1}, \ldots, x_{D^{*}}}_{u_{1}(x)}, \ldots, \underbrace{x_{D-D^{*}+1}, \ldots, x_{D}}_{u_{m}(x)} \rightarrow q_{1}\left(u_{1}(x)\right), \ldots, q_{m}\left(u_{m}(x)\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(q_{j}\)</span> is a quantizer used to quantize the <span class="math notranslate nohighlight">\(j^{\text {th }}\)</span> subvector using the codebook <span class="math notranslate nohighlight">\(\mathcal{C}_{j} = \{c_{j,1},...,c_{j,k^*}\}\)</span>. Here we assume that
all subquantizers have the same finite number <span class="math notranslate nohighlight">\(k^*\)</span> centroids.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemoproductquantization">
<img alt="../../_images/codebook_memory_saving_demo_product_quantization.png" src="../../_images/codebook_memory_saving_demo_product_quantization.png" />
<figcaption>
<p><span class="caption-number">Fig. 25.28 </span><span class="caption-text">Illustration of memory saving benefits of vector product quantization. A <span class="math notranslate nohighlight">\(D\)</span>-dimensional float vector is first split into <span class="math notranslate nohighlight">\(m\)</span> subvectors, and each subvector is stored as its nearest centroid integer id, which only occupies <span class="math notranslate nohighlight">\(\log_2 K^*\)</span> bit.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-fig-codebookmemorysavingdemoproductquantization" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In this case, a reproduction value of the product quantizer is identified by an element of the product index set <span class="math notranslate nohighlight">\(\mathcal{I}=\)</span> <span class="math notranslate nohighlight">\(\mathcal{I}_{1} \times \ldots \times \mathcal{I}_{m}\)</span>. The codebook is therefore defined as the Cartesian product</p>
<div class="math notranslate nohighlight">
\[
\mathcal{C}=\mathcal{C}_{1} \times \ldots \times \mathcal{C}_{m}
\]</div>
<p>and a centroid of this set is the concatenation of centroids of the <span class="math notranslate nohighlight">\(m\)</span> subquantizers. In that case, the total number of centroids is given by</p>
<div class="math notranslate nohighlight">
\[
k=\left(k^{*}\right)^{m}
\]</div>
<p>Note that in the extremal case where <span class="math notranslate nohighlight">\(m=D\)</span>, the components of a vector <span class="math notranslate nohighlight">\(x\)</span> are all quantized separately. Then the product quantizer turns out to be a scalar quantizer, where the quantization function associated with each component may be different.</p>
<p>The strength of a product quantizer is to produce a large set of centroids from several small groups of centroids; each group is associated with its own subquantizer.</p>
<!-- 
\iffalse
Storing the codebook $\mathcal{C}$ explicitly is not efficient. Instead, we store the $m \times k^{*}$ centroids of all the subquantizers, i.e., $m D^{*} k^{*}=k^{*} D$ floating points values. Quantizing an element requires $k^{*} D$ floating point operations. Table I summarizes the resource requirements associated with k-means, HKM and product $k$-means. The product quantizer is clearly the the only one that can be indexed in memory for large values of $k$.

{\begin{tabular}{lcc} 
	& memory usage & assignment complexity \\
	\hline k-means & $k D$ & $k D$ \\
	product k-means & $m k^{*} D^{*}=k^{1 / m} D$ & $m k^{*} D^{*}=k^{1 / m} D$ \\
	\hline
\end{tabular}
}

In order to provide good quantization properties when choosing a constant value of $k^{*}$, each subvector should have, on average, a comparable energy. One way to ensure this property is to multiply the vector by a random orthogonal matrix prior to quantization. However, for most vector types this is not required and not recommended, as consecutive components are often correlated by construction and are better quantized together with the same subquantizer. As the subspaces are orthogonal, the squared distortion associated with the product quantizer is
$$
\operatorname{MSE}(q)=\sum_{j} \operatorname{MSE}\left(q_{j}\right)
$$
where $\operatorname{MSE}\left(q_{j}\right)$ is the distortion associated with quantizer $q_{j}$. Figure 1 shows the MSE as a function of the code length for different $\left(m, K^{*}\right)$ tuples, where the code length is $l=m \log _{2} K^{*}$, if $K^{*}$ is a power of two. The curves are obtained for a set of 128-dimensional SIFT descriptors, see section $\mathrm{V}$ for details. One can observe that for a fixed number of bits, it is better to use a small number of subquantizers with many centroids than having many subquantizers with few bits. At the extreme when $m=1$, the product quantizer becomes a regular k-means codebook.

High values of $K^{*}$ increase the computational cost of the quantizer, as shown by Table I. They also increase the memory usage of storing the centroids $\left(K^{*} \times D\right.$ floating point values), which further reduces the efficiency if the centroid look-up table does no longer fit in cache memory. In the case where $m=1$, we can not afford using more than 16 bits to keep this cost tractable. Using $K*=256, m=8$ is often a reasonable choice.

\fi -->
</section>
<section id="id62">
<h4><span class="section-number">25.9.3.2. </span>Approximating Distances Using Quantized Codes<a class="headerlink" href="#id62" title="Link to this heading">#</a></h4>
<p>Like vector quantization, we also have different options in approximating distance calculation between query vector <span class="math notranslate nohighlight">\(x\)</span> and database vector <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p><strong>Symmetric distance computation (SDC)</strong>:</p>
<div class="math notranslate nohighlight">
\[
\hat{d}(x, y)=d(q(x), q(y))=\sqrt{\sum_{j} d\left(q_{j}(x), q_{j}(y)\right)^{2}}
\]</div>
<p>where the distance <span class="math notranslate nohighlight">\(d\left(c_{j, i}, c_{j, i^{\prime}}\right)^{2}\)</span> is read from a look-up table associated with the <span class="math notranslate nohighlight">\(j^{\text {th }}\)</span> subquantizer. Each look-up table contains all the squared distances between pairs of centroids <span class="math notranslate nohighlight">\(\left(i, i^{\prime}\right)\)</span> of the subquantizer, or <span class="math notranslate nohighlight">\(\left(k^{*}\right)^{2}\)</span> squared distances <span class="math notranslate nohighlight">\(^{1}\)</span>.</p>
<p><strong>Asymmetric distance computation (ADC)</strong>:</p>
<div class="math notranslate nohighlight">
\[
\tilde{d}(x, y)=d(x, q(y))=\sqrt{\sum_{j} d\left(u_{j}(x), q_{j}\left(u_{j}(y)\right)\right)^{2}}
\]</div>
<p>where the squared distances <span class="math notranslate nohighlight">\(d\left(u_{j}(x), c_{j, i}\right)^{2}: j=\)</span> <span class="math notranslate nohighlight">\(1 \ldots m, i=1 \ldots k^{*}\)</span>, are computed prior to the search.
For nearest neighbors search, we do not compute the square roots in practice: the square root function is monotonically increasing and the squared distances produces the same vector ranking.</p>
</section>
</section>
<section id="approximate-non-exhaustive-nearest-neighbor-search">
<h3><span class="section-number">25.9.4. </span>Approximate Non-exhaustive Nearest Neighbor Search<a class="headerlink" href="#approximate-non-exhaustive-nearest-neighbor-search" title="Link to this heading">#</a></h3>
<section id="hierarchical-quantization-and-inverted-file-indexing">
<h4><span class="section-number">25.9.4.1. </span>Hierarchical Quantization And Inverted File Indexing<a class="headerlink" href="#hierarchical-quantization-and-inverted-file-indexing" title="Link to this heading">#</a></h4>
<p>While performing approximate nearest neighbor search with vector quantization or product quantization can already achieve computation acceleration and storage saving in the distance calculation, the search is still exhaustive in which we are computing distance between the query vector and all the vectors in the database. Exhaustive search is not scalable to database containing billions of vectors and scenarios having high query through-puts.</p>
<p>To avoid exhaustive search we can design a hierarchical quantization strategy to reduce the number of candidates that we will run distance calculation and use product quantization to speed up the distance calculation.</p>
<p>The candidate reduction is achieved via a technique called <strong>inverted file index</strong> (IVF). IVF applies vector quantization via  k-means clustering to produce a large number (e.g., 100) of dataset partitions. At the query time, we identify the a number (e.g., 10) of partitions that are the nearest to the query and only compare the query vector to database vector in the these partitions. The residual distance between each vector and its associated partition centroid is approximated by a residual product quantization.</p>
<p>In sparse retrieval, inverted indexing refers to the mapping from a term to a list of documents in the database that contain the term. It resemble the index in the back of a textbook, which maps words or concepts to page numbers</p>
<p>In the context of vector quantization for efficient dense retrieval, we use k-means clustering to partition all vectors in the dataset.<br />
For each partition, an inverted file list refers to the document vectors and its corresponding document ids belonging to this partition.</p>
<p>Given a query, once we determine which partition the query belongs to (using a quantizer), we reduce the search space to the documents in the same partition.</p>
<p>So far, there are two layers of quantization, which are realized through a <strong>coarse quantizer</strong> and a <strong>residual  product quantizer</strong>. More formally, we denote the centroid <span class="math notranslate nohighlight">\(q_{\mathrm{c}}(y)\)</span> associated with a vector <span class="math notranslate nohighlight">\(y\)</span>. Then the product quantizer <span class="math notranslate nohighlight">\(q_{\mathrm{p}}\)</span> is used to encode the residual vector</p>
<div class="math notranslate nohighlight">
\[
r(y)=y-q_{\mathrm{c}}(y)
\]</div>
<p>corresponding to the offset in the Voronoi cell. The energy of the residual vector is small compared to that of the vector itself. The vector is approximated by</p>
<div class="math notranslate nohighlight">
\[
\tilde{y} = q_{\mathrm{c}}(y)+q_{\mathrm{p}}\left(y-q_{\mathrm{c}}(y)\right).
\]</div>
<p>Commonly we represent <span class="math notranslate nohighlight">\(y\)</span> by the tuple <span class="math notranslate nohighlight">\(\left(q_{\mathrm{c}}(y), q_{\mathrm{p}}(r(y))\right)\)</span>. Like binary representation of a number, the coarse quantizer part represents the most significant bits, while the product quantizer part represents the least significant bits.</p>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 25.2 </span> (shared product quantizer for residuals)</p>
<section class="remark-content" id="proof-content">
<p>The product quantizer can be learned on a set of residual vectors. Ideally, we can learn a product quantizer for each partition since the residual vectors likely to be dependent on the coarse quantizer. One can further reduce memory cost significantly by using the same product quantizer across all coarse quantizers, although this probably gives inferior results</p>
</section>
</div></section>
</section>
</section>
<section id="benchmark-datasets">
<h2><span class="section-number">25.10. </span>Benchmark Datasets<a class="headerlink" href="#benchmark-datasets" title="Link to this heading">#</a></h2>
<section id="ms-marco">
<h3><span class="section-number">25.10.1. </span>MS MARCO<a class="headerlink" href="#ms-marco" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://microsoft.github.io/msmarco/Datasets">MS MARCO</a> (Microsoft MAchine Reading Comprehension) <span id="id63">[<a class="reference internal" href="#id1495" title="Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: a human generated machine reading comprehension dataset. In CoCo&#64; NIPS. 2016.">NRS+16</a>]</span> is a large scale dataset widely used to train and evaluate models for the document retrieval and ranking tasks as well as tasks like key phrase extraction for question
answering. MS MARCO dataset is sampled from Bing search engine user logs, with Bing retrieved passages given queries and human annotated relevance labels. There are more than 530,000 questions in the “train” data partition, and the evaluation is usually performed on around 6,800 questions in the “dev” and “eval” data partition. The ground-truth labels for the “eval” partition are not published. The original data set contains more than <span class="math notranslate nohighlight">\(8.8\)</span> million passages.</p>
<p>There are two tasks: Passage ranking and document ranking; and two subtasks in each case: full ranking and re-ranking.</p>
<p>Each task uses a large human-generated set of training labels. The two tasks have different sets of test queries. Both tasks use similar form of training data with usually one positive training document/passage per training query. In the case of passage ranking, there is a direct human label that says the passage can be used to answer the query, whereas for training the document ranking task we transfer the same passage-level labels to document-level labels. Participants can also use external corpora for large scale language model pretraining, or adapt algorithms built for one task (e.g. passage ranking) to the other task (e.g. document ranking). This allows participants to study a variety of transfer learning strategies.</p>
<section id="document-ranking-task">
<h4><span class="section-number">25.10.1.1. </span>Document Ranking Task<a class="headerlink" href="#document-ranking-task" title="Link to this heading">#</a></h4>
<p>The first task focuses on document ranking. We have two subtasks related to this: Full ranking and top-100 re-ranking.</p>
<p>In the full ranking (retrieval) subtask, you are expected to rank documents based on their relevance to the query, where documents can be retrieved from the full document collection provided. You can submit up to 100 documents for this task. It models a scenario where you are building an end-to-end retrieval system.</p>
<p>In the re-ranking subtask, we provide you with an initial ranking of 100 documents from a simple IR system, and you are expected to re-rank the documents in terms of their relevance to the question. This is a very common real-world scenario, since many end-to-end systems are implemented as retrieval followed by top-k re-ranking. The re-ranking subtask allows participants to focus on re-ranking only, without needing to implement an end-to-end system. It also makes those re-ranking runs more comparable, because they all start from the same set of 100 candidates.</p>
<!-- 
\begin{table}
	\footnotesize
	\centering
	\begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
		\hline
		elegxo meaning & what does physical medicine do & feeding rice cereal how many times per day\\
		\hline
		most dependable affordable cars & lithophile definition & what is a flail chest \\
		\hline
		put yourself on child support in texas & what happens in a wrist sprain & what are rhetorical topics \\
		\hline
		what is considered early fall & what causes elevated nitrate levels in aquariums & lyme disease symptoms mood \\
		\hline
		what forms the epineurium & an alpha helix is an example of which protein structure? & aggregate demand curve \\
		\hline
		what county is ackley iowa in & what is adiabatic? & what is a nrr noise reduction rating mean \\
		\hline
		fibroid symptoms after menopause & what are the requirements for neurologist & watts \& browning engineers \\

		\hline
	\end{tabular}
	\caption{Example queries in MS MARCO dataset.}\label{ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:tab:example_queries_MSMARCO}
\end{table}

Documents
\begin{table}[H]
	\notsotiny
	\centering
	\begin{tabular}{p{0.15\textwidth}p{0.80\textwidth}}
		\hline
		Query & Document\\
		\hline
		{\scriptsize what is a dogo dog breed} & D3233725       \url{http://www.akc.org/dog-breeds/dogo-argentino/care/}      Dogo Argentino  Dogo Argentino Miscellaneous The Dogo Argentino is a pack-hunting dog, bred for the pursuit of big-game such as wild boar and puma, and possesses the strength, intelligence and quick responsiveness of a serious athlete. His short, plain and smooth coat is completely white, but a dark patch near the eye is permitted as long as it doesn't cover more than 10\% of the head. Dog Breeds Dogo Argentinocompare this breed with any other breed/s of your choosing Personality: Loyal, trustworthy, and, above all else, courageous Energy Level: Somewhat Active; Dogos require vigorous exercise to stay at their physical and temperamental best Good with Children: Better with Supervision Good with other Dogs: With Supervision Shedding: Infrequent Grooming: Occasional Trainability: Responds Well Height: 24-27 inches (male), 23.5-26 inches (female)Weight: 80-100 pounds Life Expectancy: 9-15 years Barking Level: Barks When Necessarymeet the Dogo Argentino Did you know? The Dogo Argentino is also known as the Argentinian Mastiff. How similar are you? Find out!Breed History1995The Dogo Argentino was first recorded in the Foundation Stock Service. Breed Standard Learn more about the Dogo Argentino breed standard. Click Here Colors \& Markings10Learn Moreat a Glance Energy \& Sizemedium ENERGY \& large size NATIONAL BREED CLUBThe Dogo Argentino Club Of America Find Dogo Argentino Puppiesthe Breed Standard Embed the breed standard on your site. Download the complete breed standard or club flier PDFs.01020304050607General Appearance Molossian normal type, mesomorphic and macrothalic, within the desirable proportions without gigantic dimensions. Its aspect is harmonic and vigorous due to its powerful muscles which stand out under the consistent and elastic skin adhered to the body through a not very lax subcutaneous tissue. It walks quietly but firmly, showing its intelligence and quick responsiveness and revealing by means of its movement its permanent happy natural disposition. Of a kind and loving nature, of a striking whiteness, its physical virtues turn it into a real athlete. Dogo Argentino Care Nutrition \& Feeding Good nutrition for Dogo Argentinos is very important from puppyhood to their senior years. Read More Coat \& Grooming The breed's coat needs only weekly brushing. Read More Exercise
		The Dogo Argentina is not a good choice for the novice owner. Read More Health Dogo Argentinos are generally a healthy breed. Read M
		oreget involved in Clubs \& Events National Breed Clubthe Dogo Argentino Club of America Local Clubs Find the Local Clubs in your area
		. Meetups Explore meetups.com and see all of the local Dogo Argentinos in your area.breeds Similar to the Dogo Argentino Cane Corso P
		lott Rhodesian Ridgeback Great Dane Bullmastiffexplore Other Breeds By Dog Breed By Dog Group \\
		{\scriptsize NA}  & D3048094 \url{https://answers.yahoo.com/question/index?qid=20080718121858AAmfk0V}      I have trouble swallowing due to MS, can I crush valium \& other meds to be easier to swallowll? Health Other - Health I have trouble swallowing due to MS, can I crush valium \& other meds to be easier to swallowll? Follow 5 answers Answers Relevance Rating Newest Oldest Best Answer: If you have a problem swallowing, try crushing Valium (or other tablets) between two spoons, and taking them in a teaspoon of your favorite Jelly (raspberry???). 	The jelly helps the crushed meds slide down &nbsp;Anonymous · 10 years ago0 2 Comment Asker's rating Ask your pharmacist if any or all of your meds can be made into syrup form if you have trouble swallowing. Many forms of medication are designed to be swallowed whole and not interferred with. Do not take advice from those people on here who are only guessing at a correct answer. Seek the advice of professionals. Lady spanner · 10 years ago0 0 Comment I'm pretty sure its not a good idea to crush pills. You should definitely ask your doctor before doing anything like that, it might be dangerous.little Wing · 10 years ago0 0 Comment Please ask your doctor! This is	not a question for random people to answer. Medication is not something to mess around with. Look at Heath Ledger. He will be missed by everyone, especially his daughter. Don't make the same mistake.pink · 10 years ago0 1 Comment Your doctor or any pharmacist should be able to tell you. Could vary for each medication. Bosco · 10 years ago0 0 Comment Maybe you would like to learn more about one of these? Glucose Monitoring Devices Considering an online college? Need migraine treatment? VPN options for your computer \\
		{\scriptsize NA} &  D2342771     \url{http://www.marketwatch.com/story/the-4-best-strategies-for-dealing-with-customer-service-2014-08-14}     The 4 best st
		rategies for dealing with customer service Shutterstock.com / wavebreakmedia If you want your cable company, airline or pretty much any other company to resolve your complaints quickly and completely, you may need to change the way you deal with customer service. According to the latest data from the American Customer Service Index, Americans are increasingly dissatisfied with the companies they deal with. In the first quarter of 2014, overall customer satisfaction scores across all industries fell to 76.2 out of 100, which the researchers who compile the ratings say was “one of the largest [drops] in the 20-year history of the Index.” And some industries are particularly hated: Internet (63 out of 100) and cable and subscription TV (65) companies and airlines (69) rank at the bottom when it comes to customer satisfaction. Part of the dissatisfaction may be because we are interacting with customer service the wrong way. A report released Wednesday by customer-service software firm Zendesk, which looked at customer-service interactions from more than 25,000 companies across 140 countries, as well as insight from customer-service experts, found that some consumers are acting in ways that aren’t yielding them good results. Here are four strategies to employ when dealing with customer service. Don’t be too stern Being “overbearing” or “overly stern” is “a common strategy for some customers seeking better service,” the Zendesk survey revealed. “However, the data indicates that customers who are polite tend to submit higher customer satisfaction scores than those who aren’t.”	Indeed, customers who ask the customer service rep to “please” help them with their request, and who said “thank you” for help they received throughout the call reported that they got better customer service than those who did not use these words, the survey showed. Of course, it could be that people who use these words tend to be more satisfied anyway. But there’s a strong chance that saying these platitudes ingratiates the customer service rep to you -- making them more willing to help you quickly and kindly with your request, experts say. “If you can find any excuse at all to praise them and build them up -- ‘you’re very good at what you do,’ ‘I really appreciate the extra effort,’ — people will almost always bend over backwards to maintain that good opinion and to show you just how good they really are,” says Barry Maher, the author of “Filling the Glass,” which focuses in part on getting better customer service and on personal empowerment. “Treat people well who aren’t used to being treated well…and you may well be astonished by the results you get.” Chip Bell, the founder of business consulting firm Chip Bell Group and author of “The 9 1/2 Principles of Innovative Service” adds that “you don’t have to be from the South to show respect with a liberal amount of ‘sir’ and ‘ma’am — it will open doors otherwise closed.”At the very least, “don’t go in guns a’ blazing,” says Shep Hyken, author of “Amaze Every Customer Every Time.” “The moment you lose your cool is the moment you lose control.” And, if that kindness does not work — rather than starting to get nasty — simply ask to speak to a higher authority, says Maher. Take notes Take a lesson from Rich Davis, the Comcast customer who recorded his call with the company and saw it go viral: It often pays to have good records of the antics a company is up to if you want to get your way down the road. “Have you ever heard a recording when you call for assistance that stated, ‘be advised this call may be recorded for quality assurance,’?” says motivational coach and speaker Chantay Bridges. “Flip the script, let the rep know: you are documenting them and keeping a record, that their service is also being noted.” While few customers keep detailed notes or recordings of their interactions, experts say, this will help them should there be a problem down the road, as they’ll have proof of what went on. Don’t keep your problems to yourself Most customers know that they should contact customer service via multiple channels like phone, email and social media to get their complaint resolved ASAP. But few take the logical next step: contacting competitors via these channels too to explain how their current company is not serving their needs. “If you receive a response from a competitor, watch or ask for a counter from the business you originally were contacting,” says John Huehn, CEO of In the Chat, which provides social media and text
		messaging platforms to enhance customer service. This could provide you with the leverage you need to get your issue resolved. It’s OK to keep it brief When it comes to email complaints, longer isn’t better, so don’t waste your breath (you might, after all, need that energy for dealing with the company later on). While experts often advise consumers to give customer service reps as many details as possible so they can effectively fix the customer’s complaint, customers who wrote long emails didn’t report getting any better customer service than those who wrote short ones, the Zendesk survey revealed. Indeed, the customer’s satisfaction levels at the end of
		the exchange were roughly the same whether they wrote 50 words or 200 words. Jason Maynard, product manager and data science lead at Zendesk, says that this doesn’t mean you should not include detail (indeed, that helps) but that you should make this detail as succinct as possible; if you have a complicated problem or request, you may want to consider calling, as most of us aren’t trained as technical writers. \\
		\hline
	\end{tabular}
	\caption{Example queries (may not exist) and relevant documents for document ranking tasks in MS MARCO dataset.}\label{ch:neural-network-and-deep-learning:ApplicationNLP_IRSearch:tab:example_documents_MSMARCO}
\end{table} -->
</section>
<section id="passage-ranking-task">
<h4><span class="section-number">25.10.1.2. </span>Passage Ranking Task<a class="headerlink" href="#passage-ranking-task" title="Link to this heading">#</a></h4>
<p>Similar to the document ranking task, the passage ranking task also has a full ranking and reranking subtasks.</p>
<p>In context of full ranking (retrieval) subtask, given a question, you are expected to rank passages from the full collection in terms of their likelihood of containing an answer to the question. You can submit up to 1,000 passages for this end-to-end retrieval task.</p>
<p>In context of top-1000 reranking subtask, we provide you with an initial ranking of 1000 passages and you are expected to rerank these passages based on their likelihood of containing an answer to the question. In this subtask, we can compare different reranking methods based on the same initial set of 1000 candidates, with the same rationale as described for the document reranking subtask.</p>
<p>One caveat of the MSMARCO collection is that only contains binary annotations for fewer than two positive examples per query, and no explicit annotations for non-relevant passages. During the reranking task, negative examples are generated from the top candidates of a traditional retrieval system. This approach works reasonably well, but accidentally picking relevant passages as negative examples is possible.</p>
<!-- 
\begin{table}
\scriptsize
\centering
\begin{tabular}{p{0.15\textwidth}p{0.4\textwidth}p{0.4\textwidth}}
	\hline
	Query & Relevant Passage & Irrelevant Passage\\
	\hline
	foods that will help lower blood sugar & Lemons are rich in Vitamin C and their acidity helps to lower other foods' glycemic indexes. Oat and rice bran crackers make healthy snacks. Complement with organic nut butter or cheese. Other foods that stabilize blood sugar are cheese, egg yolks, berries and brewer's yeast.
	& Low hemoglobin, high blood pressure, high levels of bad cholesterol and abnormal blood sugar levels are a few factors that influence blood health. Your diet can go a long way in promoting healthy blood, and most foods that are good for the blood also promote healthy
	weight and general well being.n fact, foods that contain monounsaturated and polyunsaturated fat actually lower your bad cholesterol levels while increasing good cholesterol. Foods that promote healthy blood cholesterol levels include plant oils -- except for palm and coconut oil -- as well as fish, nuts and avocados.
	\\
	cancer of the pancreas symptoms & Symptoms of Pancreatic Cancer. Pancreatic cancer may cause only vague unexplained symptoms. Pain (usually in the abdomen or back), weight loss, jaundice (yellowing of the skin and/or eyes) with or without itching, loss of appetite, nausea, change in stool, pancreatitis and recent-onset diabetes are symptoms that may indicate pancreatic cancer. &Pancreatic cancer develops as abnormal pancreatic cells multiply rapidly in the pancreas. These cells don't die, but continue growing to form tumors. As the stages of pancreatic cancer progress in dogs, tissue in the pancreas begins to die. In the later stages of pancreatic cancer, tumors can spread to other organs, causing tissue death and organ dysfunction throughout the body.\\
	is pizza considered fast food & Fast Food Pizza is Unhealthy. Fast food pizza is unhealthy because of its ingredients. Fast food pizza is made on a white crust that is filled with refined carbs. These refined or processed grains are stripped of most of the healthy nutrients in the name of taste. & I have already proven that the mode can be numerical in the sentences above. For an example of categorical data, say I surveyed some people about what their favorite food was and this was the data: Pizza, pizza, pizza, ice cream, ice cream, strawberries, strawberries, oranges, spaghetti. The mode would have been pizza.\\
	cost to install a sump pump & The average cost to install a sump pump ranges from $550 to $1,100 depending on the size of the pump. How do you build a sump pit? A sump pit is literally a hole just large enough to hold the pump with a plastic lining. & Protect machinery from water damage with an elevator sump pump with an oil sensor from Grainger. If water is allowed to collect in an elevator pit it can facilitate the growth of bacteria, mold and mildew. These pumps that are designed and approved for safe operation of pumping, alarming and monitoring of elevator sump pits, transformer vaults and other applications where oil and water must be detected.\\
	\hline
\end{tabular}
\caption{Example queries, relevant passages, and irrelevant passages for passage ranking tasks in MS MARCO dataset.}
\end{table} -->
</section>
</section>
<section id="terc">
<h3><span class="section-number">25.10.2. </span>TERC<a class="headerlink" href="#terc" title="Link to this heading">#</a></h3>
<section id="trec-deep-learning-track">
<h4><span class="section-number">25.10.2.1. </span>TREC-deep Learning Track<a class="headerlink" href="#trec-deep-learning-track" title="Link to this heading">#</a></h4>
<p>Deep Learning Track at the Text REtrieval Conferences (TRECs) deep learning track<sup>[^7]</sup><span id="id64">[<a class="reference internal" href="#id1450" title="Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820, 2020.">CMY+20</a>]</span> is another large scale dataset used to evaluate retrieval and ranking model through two tasks: Document retrieval and passage retrieval.</p>
<p>Both tasks use a large human-generated set of training labels, from the MS-MARCO dataset. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, and there are a test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, and there are a test set of 43 queries.</p>
</section>
<section id="trec-car">
<h4><span class="section-number">25.10.2.2. </span>TREC-CAR<a class="headerlink" href="#trec-car" title="Link to this heading">#</a></h4>
<p>TREC-CAR (Complex Answer Retrieval) <span id="id65">[<a class="reference internal" href="#id1448" title="Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. Trec complex answer retrieval overview. In TREC. 2017.">DVRC17</a>]</span> is a dataset where the input query is the concatenation of a Wikipedia article title with the title of one of its sections. The ground-truth documents are the paragraphs within that section. The corpus consists of all English Wikipedia paragraphs except the abstracts. The released dataset has five predefined folds, and we use the first four as a training set (approx. 3M queries), and the remaining as a validation set (approx. 700k queries). The test set has approx. 2,250 queries.</p>
</section>
</section>
<section id="natural-question-nq">
<h3><span class="section-number">25.10.3. </span>Natural Question (NQ)<a class="headerlink" href="#natural-question-nq" title="Link to this heading">#</a></h3>
<p>Natural Question (NQ) <span id="id66">[<a class="reference internal" href="#id1462" title="Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and others. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.">KPR+19</a>]</span> introduces a large dataset for open-domain QA. The original dataset contains more than 300,000 questions collected from Google search logs. In <span id="id67">[<a class="reference internal" href="#id1463" title="Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.">KOuguzM+20</a>]</span>, around 62,000 factoid questions are selected, and all the Wikipedia articles are processed as the collection of passages. There are more than 21 million passages in the corpus.</p>
</section>
<section id="entity-questions">
<h3><span class="section-number">25.10.4. </span>Entity Questions<a class="headerlink" href="#entity-questions" title="Link to this heading">#</a></h3>
<p><span id="id68">[<a class="reference internal" href="#id1658" title="Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. arXiv preprint arXiv:2109.08535, 2021.">SZLC21</a>]</span></p>
<p>a set of simple, entityrich questions based on facts from Wikidata
(e.g., “Where was Arve Furset born?”)</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1666">
<caption><span class="caption-number">Table 25.4 </span><span class="caption-text">Retrieval accuracy for dense and sparse retrieval models on Natural Questions and our EntityQuestions benchmark.</span><a class="headerlink" href="#id1666" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>DPR <br> (NQ)</p></th>
<th class="head text-center"><p>DPR <br> (multi)</p></th>
<th class="head text-center"><p>BM25</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Natural Questions</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 0 . 1}\)</span></p></td>
<td class="text-center"><p>79.4</p></td>
<td class="text-center"><p>64.4</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>EntityQuestions (this work)</p></td>
<td class="text-center"><p>49.7</p></td>
<td class="text-center"><p>56.7</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 2 . 0}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>What is the capital of [E]?</p></td>
<td class="text-center"><p>77.3</p></td>
<td class="text-center"><p>78.9</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{9 0 . 6}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Who is [E] married to?</p></td>
<td class="text-center"><p>35.6</p></td>
<td class="text-center"><p>48.1</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 9 . 7}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Where is the headquarter of [E]?</p></td>
<td class="text-center"><p>70.0</p></td>
<td class="text-center"><p>72.0</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 5 . 0}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Where was [E] born?</p></td>
<td class="text-center"><p>25.4</p></td>
<td class="text-center"><p>41.8</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 5 . 3}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Where was [E] educated?</p></td>
<td class="text-center"><p>26.4</p></td>
<td class="text-center"><p>41.8</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 3 . 1}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Who was [E] created by?</p></td>
<td class="text-center"><p>54.1</p></td>
<td class="text-center"><p>57.7</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 2 . 6}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Who is [E]’s child?</p></td>
<td class="text-center"><p>19.2</p></td>
<td class="text-center"><p>33.8</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 5 . 0}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>(17 more types of questions)</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="beir">
<h3><span class="section-number">25.10.5. </span>BEIR<a class="headerlink" href="#beir" title="Link to this heading">#</a></h3>
<p><strong>BEIR (Benchmarking Information Retrieval)</strong> <span id="id69">[<a class="reference internal" href="#id1646" title="Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.">TRRuckle+21</a>]</span> is a <strong>heterogeneous benchmark</strong> designed to evaluate <strong>zero-shot generalization</strong> in retrieval models. It includes <strong>18 datasets across 9 tasks</strong>, covering fact-checking, question answering, news retrieval, biomedical IR, and more. The goal is to assess model performance in <strong>out-of-distribution (OOD) scenarios</strong>, as compared to large, homogeneous datasets like <strong>MS MARCO</strong>.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-benchmark-fig-beir-datasets">
<a class="reference internal image-reference" href="../../_images/BEIR_datasets.png"><img alt="../../_images/BEIR_datasets.png" src="../../_images/BEIR_datasets.png" style="width: 905.85px; height: 285.3px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.29 </span><span class="caption-text">An overview of the diverse tasks and datasets in BEIR benchmark. Image from <span id="id70">[<a class="reference internal" href="#id1646" title="Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.">TRRuckle+21</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-benchmark-fig-beir-datasets" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Dataset</p></th>
<th class="head text-center"><p>BEIR-Name</p></th>
<th class="head text-center"><p>Type</p></th>
<th class="head text-center"><p>Queries</p></th>
<th class="head text-center"><p>Corpus</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>MSMARCO</p></td>
<td class="text-center"><p>msmarco</p></td>
<td class="text-center"><p>train, dev, test</p></td>
<td class="text-center"><p>6,980</p></td>
<td class="text-center"><p>8,840,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>TREC-COVID</p></td>
<td class="text-center"><p>trec-covid</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>50</p></td>
<td class="text-center"><p>171,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>NFCorpus</p></td>
<td class="text-center"><p>nfcorpus</p></td>
<td class="text-center"><p>train, dev, test</p></td>
<td class="text-center"><p>323</p></td>
<td class="text-center"><p>3,600</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>BioASQ</p></td>
<td class="text-center"><p>bioasq</p></td>
<td class="text-center"><p>train, test</p></td>
<td class="text-center"><p>500</p></td>
<td class="text-center"><p>14,910,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>NQ</p></td>
<td class="text-center"><p>nq</p></td>
<td class="text-center"><p>train, test</p></td>
<td class="text-center"><p>3,452</p></td>
<td class="text-center"><p>2,680,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>HotpotQA</p></td>
<td class="text-center"><p>hotpotqa</p></td>
<td class="text-center"><p>train, dev, test</p></td>
<td class="text-center"><p>7,405</p></td>
<td class="text-center"><p>5,230,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>FIQA-2018</p></td>
<td class="text-center"><p>fiqa</p></td>
<td class="text-center"><p>train, dev, test</p></td>
<td class="text-center"><p>648</p></td>
<td class="text-center"><p>57,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Signal-1M(RT)</p></td>
<td class="text-center"><p>signal1m</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>97</p></td>
<td class="text-center"><p>2,860,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>TREC-NEWS</p></td>
<td class="text-center"><p>trec-news</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>57</p></td>
<td class="text-center"><p>595,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Robust04</p></td>
<td class="text-center"><p>robust04</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>249</p></td>
<td class="text-center"><p>528,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>ArguAna</p></td>
<td class="text-center"><p>arguana</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>1,406</p></td>
<td class="text-center"><p>8,670</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Touche-2020</p></td>
<td class="text-center"><p>webis-touche2020</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>49</p></td>
<td class="text-center"><p>382,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>CQADupstack</p></td>
<td class="text-center"><p>cqadupstack</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>13,145</p></td>
<td class="text-center"><p>457,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Quora</p></td>
<td class="text-center"><p>quora</p></td>
<td class="text-center"><p>dev, test</p></td>
<td class="text-center"><p>10,000</p></td>
<td class="text-center"><p>523,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DBPedia</p></td>
<td class="text-center"><p>dbpedia-entity</p></td>
<td class="text-center"><p>dev, test</p></td>
<td class="text-center"><p>400</p></td>
<td class="text-center"><p>4,630,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>SCIDOCS</p></td>
<td class="text-center"><p>scidocs</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>1,000</p></td>
<td class="text-center"><p>25,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>FEVER</p></td>
<td class="text-center"><p>fever</p></td>
<td class="text-center"><p>train, dev, test</p></td>
<td class="text-center"><p>6,666</p></td>
<td class="text-center"><p>5,420,000</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Climate-FEVER</p></td>
<td class="text-center"><p>climate-fever</p></td>
<td class="text-center"><p>test</p></td>
<td class="text-center"><p>1,535</p></td>
<td class="text-center"><p>5,420,000</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>SciFact</p></td>
<td class="text-center"><p>scifact</p></td>
<td class="text-center"><p>train, test</p></td>
<td class="text-center"><p>300</p></td>
<td class="text-center"><p>5,000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Key Findings from evaluting BM25 and different dense models:</p>
<ol class="arabic simple">
<li><p><strong>BM25 remains a strong baseline</strong> – Outperforms many neural models across diverse domains in zero-shot settings.</p></li>
<li><p><strong>Dense retrieval struggles with generalization</strong> – Models like <strong>DPR and ANCE</strong> often fail on OOD datasets and show a large gap combined to BM25.</p></li>
<li><p><strong>Document expansion improves BM25</strong> - Document expansion technique like docT5Query can further improve BM25 across diverse domains.</p></li>
<li><p><strong>Trade-off between accuracy and efficiency</strong> – Re-ranking models (BM25 + CrossEncoder) or late interaction model like ColBERT perform better than BM25, but are computationally expensive compared to a single sparse and dense models.</p></li>
<li><p><strong>Importance of negative sampling and strong teacher distillation</strong> - TAS-B, which employed topic-balanced negative sampling and strong teacher distillation, showing the best performance compared to other deep models.</p></li>
<li><p><strong>Potential bias towards BM25</strong> - Many benchmarks have relevance labels heavily based on lexical matching, which can disadvantage deep models.</p></li>
</ol>
<div class="pst-scrollable-table-container"><table class="table" id="id1667">
<caption><span class="caption-number">Table 25.5 </span><span class="caption-text">Selected evaluation results in BEIR, including in-domain results (MS MARCO) and zero-shot out-of-domain results.</span><a class="headerlink" href="#id1667" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p>Dataset</p></th>
<th class="head text-center"><p>BM25</p></th>
<th class="head text-center"><p>docT5query</p></th>
<th class="head text-center"><p>TAS-B</p></th>
<th class="head text-center"><p>GenQ</p></th>
<th class="head text-center"><p>ColBERT</p></th>
<th class="head text-center"><p>BM25+CE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>MS MARCO (in-domain)</p></td>
<td class="text-center"><p>0.228</p></td>
<td class="text-center"><p>0.338</p></td>
<td class="text-center"><p>0.408</p></td>
<td class="text-center"><p>0.408</p></td>
<td class="text-center"><p>0.401</p></td>
<td class="text-center"><p>0.413</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Quora</p></td>
<td class="text-center"><p>0.789</p></td>
<td class="text-center"><p>0.802</p></td>
<td class="text-center"><p>0.835</p></td>
<td class="text-center"><p>0.830</p></td>
<td class="text-center"><p>0.854</p></td>
<td class="text-center"><p>0.825</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DBPedia</p></td>
<td class="text-center"><p>0.313</p></td>
<td class="text-center"><p>0.331</p></td>
<td class="text-center"><p>0.384</p></td>
<td class="text-center"><p>0.328</p></td>
<td class="text-center"><p>0.392</p></td>
<td class="text-center"><p>0.409</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>SciFact</p></td>
<td class="text-center"><p>0.665</p></td>
<td class="text-center"><p>0.675</p></td>
<td class="text-center"><p>0.643</p></td>
<td class="text-center"><p>0.644</p></td>
<td class="text-center"><p>0.671</p></td>
<td class="text-center"><p>0.688</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Avg. Performance vs. BM25</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>+1.6%</p></td>
<td class="text-center"><p>-2.8%</p></td>
<td class="text-center"><p>-3.6%</p></td>
<td class="text-center"><p>+2.5%</p></td>
<td class="text-center"><p>+11%</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="lotte">
<h3><span class="section-number">25.10.6. </span>LoTTE<a class="headerlink" href="#lotte" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://github.com/stanford-futuredata/ColBERT/blob/main/LoTTE.md">LoTTE benchmark</a> (Long-Tail Topic-stratified Evaluation for IR) was introduced in <span id="id71">[<a class="reference internal" href="#id1657" title="Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488, 2021.">SKSF+21</a>]</span> to complement the out-of-domain tests of BEIR <span id="id72">[<a class="reference internal" href="#id1646" title="Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.">TRRuckle+21</a>]</span>. LoTTE focuses on natural user queries that pertain to long-tail topics, ones that might not be covered by an entity-centric knowledge base like Wikipedia. LoTTE consists of 12 test sets, each with 500-2000 queries and 100k to 200M passages.</p>
</section>
<section id="mteb">
<h3><span class="section-number">25.10.7. </span>MTEB<a class="headerlink" href="#mteb" title="Link to this heading">#</a></h3>
<p>The Massive Text Embedding Benchmark (MTEB) is a large-scale benchmark designed to evaluate the performance of text embedding models across a diverse range of tasks, beyond the text similarity tasks or retrieval tasks. MTEB covers eight embedding tasks spanning 58 datasets in 112 languages, including <strong>classification</strong>, <strong>clustering</strong>, <strong>retrieval</strong>, <strong>reranking</strong>, <strong>pair classification</strong>, and <strong>summarization</strong>.</p>
<figure class="align-default" id="ch-neural-network-and-deep-learning-applicationnlp-irsearch-benchmark-fig-mteb-datasets">
<a class="reference internal image-reference" href="../../_images/mteb_datasets.png"><img alt="../../_images/mteb_datasets.png" src="../../_images/mteb_datasets.png" style="width: 610.65px; height: 309.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25.30 </span><span class="caption-text">An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade. Image from <span id="id73">[<a class="reference internal" href="#id1647" title="Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022.">MTMR22</a>]</span>.</span><a class="headerlink" href="#ch-neural-network-and-deep-learning-applicationnlp-irsearch-benchmark-fig-mteb-datasets" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>No universal best model – No single embedding method consistently outperforms others across all tasks, highlighting the need for task-specific tuning.</p></li>
<li><p>Scaling improves performance – Larger models (e.g., ST5-XXL, SGPT-5.8B) generally achieve better results, but at the cost of computational efficiency.</p></li>
<li><p>Task specialization matters –
ST5 models excel in classification &amp; STS, but perform poorly in retrieval.
GTR models dominate retrieval tasks, while struggling in STS.
MPNet and MiniLM perform well on reranking and clustering.</p></li>
<li><p>Self-supervised models lag behind supervised methods – Unsupervised embeddings like SimCSE-BERT-unsup underperform compared to fine-tuned models.</p></li>
</ul>
</section>
</section>
<section id="note-on-bibliography-and-software">
<h2><span class="section-number">25.11. </span>Note On Bibliography And Software<a class="headerlink" href="#note-on-bibliography-and-software" title="Link to this heading">#</a></h2>
<section id="bibliography">
<h3><span class="section-number">25.11.1. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h3>
<p>For excellent reviews in neural information retrieval, see <span id="id74">[<a class="reference internal" href="#id1440" title="Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. A deep look into neural ranking models for information retrieval. Information Processing &amp; Management, 57(6):102067, 2020.">GFP+20</a>, <a class="reference internal" href="#id1494" title="Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: bert and beyond. Synthesis Lectures on Human Language Technologies, 14(4):1–325, 2021.">LNY21</a>, <a class="reference internal" href="#id1441" title="Bhaskar Mitra, Nick Craswell, and others. An introduction to neural information retrieval. Now Foundations and Trends Boston, MA, 2018.">MC+18</a>]</span></p>
<p>For traditional information retrieval, see <span id="id75">[<a class="reference internal" href="#id1503" title="Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. Information retrieval: Implementing and evaluating search engines. Mit Press, 2016.">BCC16</a>, <a class="reference internal" href="#id1527" title="W Bruce Croft, Donald Metzler, and Trevor Strohman. Search engines: Information retrieval in practice. Volume 520. Addison-Wesley Reading, 2010.">CMS10</a>, <a class="reference internal" href="#id1522" title="Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc, 2009.">RZ09</a>, <a class="reference internal" href="#id1502" title="Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information retrieval. Volume 39. Cambridge University Press Cambridge, 2008.">SchutzeMR08</a>]</span></p>
<div class="docutils container" id="id76">
<div role="list" class="citation-list">
<div class="citation" id="id1568" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">AWB+19</a><span class="fn-bracket">]</span></span>
<p>Qingyao Ai, Xuanhui Wang, Sebastian Bruch, Nadav Golbandi, Michael Bendersky, and Marc Najork. Learning groupwise multivariate scoring functions using deep neural networks. In <em>Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval</em>, 85–92. 2019.</p>
</div>
<div class="citation" id="id1559" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">Bur10</a><span class="fn-bracket">]</span></span>
<p>Christopher JC Burges. From ranknet to lambdarank to lambdamart: an overview. <em>Learning</em>, 11(23-581):81, 2010.</p>
</div>
<div class="citation" id="id1503" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id75">BCC16</a><span class="fn-bracket">]</span></span>
<p>Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. <em>Information retrieval: Implementing and evaluating search engines</em>. Mit Press, 2016.</p>
</div>
<div class="citation" id="id1606" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">CQL+07</a><span class="fn-bracket">]</span></span>
<p>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In <em>Proceedings of the 24th international conference on Machine learning</em>, 129–136. 2007.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">CFGH20</a><span class="fn-bracket">]</span></span>
<p>Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2003.04297">https://arxiv.org/abs/2003.04297</a>, <a class="reference external" href="https://arxiv.org/abs/2003.04297">arXiv:2003.04297</a>.</p>
</div>
<div class="citation" id="id1450" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id64">CMY+20</a><span class="fn-bracket">]</span></span>
<p>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. Overview of the trec 2019 deep learning track. <em>arXiv preprint arXiv:2003.07820</em>, 2020.</p>
</div>
<div class="citation" id="id1527" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id75">CMS10</a><span class="fn-bracket">]</span></span>
<p>W Bruce Croft, Donald Metzler, and Trevor Strohman. <em>Search engines: Information retrieval in practice</em>. Volume 520. Addison-Wesley Reading, 2010.</p>
</div>
<div class="citation" id="id77" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DZM+22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id48">1</a>,<a role="doc-backlink" href="#id49">2</a>)</span>
<p>Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: few-shot dense retrieval from 8 examples. <em>arXiv preprint arXiv:2209.11755</em>, 2022.</p>
</div>
<div class="citation" id="id1439" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">DZS+17</a><span class="fn-bracket">]</span></span>
<p>Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural ranking models with weak supervision. In <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 65–74. 2017.</p>
</div>
<div class="citation" id="id1232" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">DCLT18</a><span class="fn-bracket">]</span></span>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</div>
<div class="citation" id="id1448" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">DVRC17</a><span class="fn-bracket">]</span></span>
<p>Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. Trec complex answer retrieval overview. In <em>TREC</em>. 2017.</p>
</div>
<div class="citation" id="id1511" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id61">FBF77</a><span class="fn-bracket">]</span></span>
<p>Jerome H Friedman, Jon Louis Bentley, and Raphael Ari Finkel. An algorithm for finding best matches in logarithmic expected time. <em>ACM Transactions on Mathematical Software (TOMS)</em>, 3(3):209–226, 1977.</p>
</div>
<div class="citation" id="id1440" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id74">GFP+20</a><span class="fn-bracket">]</span></span>
<p>Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. A deep look into neural ranking models for information retrieval. <em>Information Processing &amp; Management</em>, 57(6):102067, 2020.</p>
</div>
<div class="citation" id="id1507" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">GSL+20</a><span class="fn-bracket">]</span></span>
<p>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In <em>International Conference on Machine Learning</em>, 3887–3896. PMLR, 2020.</p>
</div>
<div class="citation" id="id1514" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">HG19</a><span class="fn-bracket">]</span></span>
<p>Dany Haddad and Joydeep Ghosh. Learning more from less: towards strengthening weak supervision for ad-hoc retrieval. In <em>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 857–860. 2019.</p>
</div>
<div class="citation" id="id1304" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HVD15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id51">1</a>,<a role="doc-backlink" href="#id52">2</a>)</span>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. <em>arXiv preprint arXiv:1503.02531</em>, 2015.</p>
</div>
<div class="citation" id="id1458" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id36">HofstatterLY+21</a><span class="fn-bracket">]</span></span>
<p>Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In <em>Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 113–122. 2021.</p>
</div>
<div class="citation" id="id114" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HHG+13<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In <em>Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>, 2333–2338. 2013.</p>
</div>
<div class="citation" id="id1452" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">HSLW19</a><span class="fn-bracket">]</span></span>
<p>Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. <em>arXiv preprint arXiv:1905.01969</em>, 2019.</p>
</div>
<div class="citation" id="id1661" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ICH+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id46">1</a>,<a role="doc-backlink" href="#id57">2</a>)</span>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. <em>arXiv preprint arXiv:2112.09118</em>, 2021.</p>
</div>
<div class="citation" id="id1506" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">JDJegou19</a><span class="fn-bracket">]</span></span>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. <em>IEEE Transactions on Big Data</em>, 7(3):535–547, 2019.</p>
</div>
<div class="citation" id="id1463" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KOuguzM+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id14">2</a>,<a role="doc-backlink" href="#id34">3</a>,<a role="doc-backlink" href="#id37">4</a>,<a role="doc-backlink" href="#id67">5</a>)</span>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2004.04906</em>, 2020.</p>
</div>
<div class="citation" id="id1484" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">KZ20</a><span class="fn-bracket">]</span></span>
<p>Omar Khattab and Matei Zaharia. Colbert: efficient and effective passage search via contextualized late interaction over bert. In <em>Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</em>, 39–48. 2020.</p>
</div>
<div class="citation" id="id1462" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id66">KPR+19</a><span class="fn-bracket">]</span></span>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and others. Natural questions: a benchmark for question answering research. <em>Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.</p>
</div>
<div class="citation" id="id1492" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">LLXL21</a><span class="fn-bracket">]</span></span>
<p>Yizhi Li, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. More robust dense retrieval with contrastive dual learning. In <em>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</em>, 287–296. 2021.</p>
</div>
<div class="citation" id="id1494" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id74">LNY21</a><span class="fn-bracket">]</span></span>
<p>Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: bert and beyond. <em>Synthesis Lectures on Human Language Technologies</em>, 14(4):1–325, 2021.</p>
</div>
<div class="citation" id="id1478" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LYL21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id54">1</a>,<a role="doc-backlink" href="#id55">2</a>)</span>
<p>Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In <em>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</em>, 163–173. 2021.</p>
</div>
<div class="citation" id="id1460" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">LWLQ21</a><span class="fn-bracket">]</span></span>
<p>Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. <em>arXiv preprint arXiv:2106.04554</em>, 2021.</p>
</div>
<div class="citation" id="id1457" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id53">LJZ20</a><span class="fn-bracket">]</span></span>
<p>Wenhao Lu, Jian Jiao, and Ruofei Zhang. Twinbert: distilling knowledge to twin-structured bert models for efficient retrieval. <em>arXiv preprint arXiv:2002.06275</em>, 2020.</p>
</div>
<div class="citation" id="id1453" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">LETC21</a><span class="fn-bracket">]</span></span>
<p>Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for text retrieval. <em>Transactions of the Association for Computational Linguistics</em>, 9:329–345, 2021.</p>
</div>
<div class="citation" id="id1505" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">MY18</a><span class="fn-bracket">]</span></span>
<p>Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 42(4):824–836, 2018.</p>
</div>
<div class="citation" id="id1191" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">MSC+13</a><span class="fn-bracket">]</span></span>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In <em>Advances in neural information processing systems</em>, 3111–3119. 2013.</p>
</div>
<div class="citation" id="id1441" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id74">MC+18</a><span class="fn-bracket">]</span></span>
<p>Bhaskar Mitra, Nick Craswell, and others. <em>An introduction to neural information retrieval</em>. Now Foundations and Trends Boston, MA, 2018.</p>
</div>
<div class="citation" id="id1647" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id73">MTMR22</a><span class="fn-bracket">]</span></span>
<p>Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: massive text embedding benchmark. <em>arXiv preprint arXiv:2210.07316</em>, 2022.</p>
</div>
<div class="citation" id="id1495" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">NRS+16</a><span class="fn-bracket">]</span></span>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: a human generated machine reading comprehension dataset. In <em>CoCo&#64; NIPS</em>. 2016.</p>
</div>
<div class="citation" id="id1476" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">NZG+20</a><span class="fn-bracket">]</span></span>
<p>Ping Nie, Yuyu Zhang, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. Dc-bert: decoupling question and document for efficient contextual encoding. In <em>Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>, 1829–1832. 2020.</p>
</div>
<div class="citation" id="id1515" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">NSN18</a><span class="fn-bracket">]</span></span>
<p>Yifan Nie, Alessandro Sordoni, and Jian-Yun Nie. Multi-level abstraction convolutional model with weak supervision for information retrieval. In <em>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</em>, 985–988. 2018.</p>
</div>
<div class="citation" id="id1451" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NC19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id15">2</a>,<a role="doc-backlink" href="#id16">3</a>,<a role="doc-backlink" href="#id23">4</a>,<a role="doc-backlink" href="#id37">5</a>)</span>
<p>Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. <em>arXiv preprint arXiv:1901.04085</em>, 2019.</p>
</div>
<div class="citation" id="id1474" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NYCL19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id15">2</a>,<a role="doc-backlink" href="#id16">3</a>,<a role="doc-backlink" href="#id17">4</a>,<a role="doc-backlink" href="#id18">5</a>,<a role="doc-backlink" href="#id23">6</a>)</span>
<p>Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. <em>arXiv preprint arXiv:1910.14424</em>, 2019.</p>
</div>
<div class="citation" id="id1480" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>QDL+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id42">1</a>,<a role="doc-backlink" href="#id43">2</a>,<a role="doc-backlink" href="#id44">3</a>,<a role="doc-backlink" href="#id47">4</a>)</span>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2010.08191</em>, 2020.</p>
</div>
<div class="citation" id="id1513" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">RSL+21</a><span class="fn-bracket">]</span></span>
<p>Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision. <em>arXiv preprint arXiv:2112.07708</em>, 2021.</p>
</div>
<div class="citation" id="id1655" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id59">RG20</a><span class="fn-bracket">]</span></span>
<p>Nils Reimers and Iryna Gurevych. The curse of dense low-dimensional information retrieval for large index sizes. <em>arXiv preprint arXiv:2012.14210</em>, 2020.</p>
</div>
<div class="citation" id="id1522" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id75">RZ09</a><span class="fn-bracket">]</span></span>
<p>Stephen Robertson and Hugo Zaragoza. <em>The probabilistic relevance framework: BM25 and beyond</em>. Now Publishers Inc, 2009.</p>
</div>
<div class="citation" id="id1657" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id71">SKSF+21</a><span class="fn-bracket">]</span></span>
<p>Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: effective and efficient retrieval via lightweight late interaction. <em>arXiv preprint arXiv:2112.01488</em>, 2021.</p>
</div>
<div class="citation" id="id1502" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id75">SchutzeMR08</a><span class="fn-bracket">]</span></span>
<p>Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. <em>Introduction to information retrieval</em>. Volume 39. Cambridge University Press Cambridge, 2008.</p>
</div>
<div class="citation" id="id1658" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id68">SZLC21</a><span class="fn-bracket">]</span></span>
<p>Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. <em>arXiv preprint arXiv:2109.08535</em>, 2021.</p>
</div>
<div class="citation" id="id1485" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">SHG+14</a><span class="fn-bracket">]</span></span>
<p>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In <em>Proceedings of the 23rd ACM international conference on conference on information and knowledge management</em>, 101–110. 2014.</p>
</div>
<div class="citation" id="id1488" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">Soh16</a><span class="fn-bracket">]</span></span>
<p>Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. <em>Advances in neural information processing systems</em>, 2016.</p>
</div>
<div class="citation" id="id1454" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TSJ+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id20">1</a>,<a role="doc-backlink" href="#id24">2</a>)</span>
<p>Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. Improving document representations by generating pseudo query embeddings for dense retrieval. <em>arXiv preprint arXiv:2105.03599</em>, 2021.</p>
</div>
<div class="citation" id="id1479" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id51">TLL+19</a><span class="fn-bracket">]</span></span>
<p>Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. <em>arXiv preprint arXiv:1903.12136</em>, 2019.</p>
</div>
<div class="citation" id="id1646" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TRRuckle+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id69">1</a>,<a role="doc-backlink" href="#id70">2</a>,<a role="doc-backlink" href="#id72">3</a>)</span>
<p>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models. <em>arXiv preprint arXiv:2104.08663</em>, 2021.</p>
</div>
<div class="citation" id="id1456" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id53">VTGS20</a><span class="fn-bracket">]</span></span>
<p>Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. Distilling knowledge for fast retrieval-based chat-bots. In <em>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2081–2084. 2020.</p>
</div>
<div class="citation" id="id1493" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WI20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id31">1</a>,<a role="doc-backlink" href="#id38">2</a>)</span>
<p>Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In <em>International Conference on Machine Learning</em>, 9929–9939. PMLR, 2020.</p>
</div>
<div class="citation" id="id1660" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id58">WLC+19</a><span class="fn-bracket">]</span></span>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: extracting high quality monolingual datasets from web crawl data. <em>arXiv preprint arXiv:1911.00359</em>, 2019.</p>
</div>
<div class="citation" id="id1481" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XXL+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id39">1</a>,<a role="doc-backlink" href="#id40">2</a>,<a role="doc-backlink" href="#id41">3</a>)</span>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. <em>arXiv preprint arXiv:2007.00808</em>, 2020.</p>
</div>
<div class="citation" id="id1662" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">ZML+20</a><span class="fn-bracket">]</span></span>
<p>Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. Repbert: contextualized text embeddings for first-stage retrieval. <em>arXiv preprint arXiv:2006.15498</em>, 2020.</p>
</div>
<div class="citation" id="id1498" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id56">ZQH+21</a><span class="fn-bracket">]</span></span>
<p>Honglei Zhuang, Zhen Qin, Shuguang Han, Xuanhui Wang, Michael Bendersky, and Marc Najork. Ensemble distillation for bert-based ranking models. In <em>Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</em>, 131–136. 2021.</p>
</div>
</div>
</div>
</section>
<section id="software">
<h3><span class="section-number">25.11.2. </span>Software<a class="headerlink" href="#software" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/facebookresearch/faiss/wiki/">Faiss</a> is a recently developed computational library for efficient similarity search and clustering of dense vectors.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_application_IR"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="information_retrieval_fundamentals_part1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">24. </span>Information Retrieval and Sparse Retrieval</p>
      </div>
    </a>
    <a class="right-next"
       href="application_LLM_in_IR.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">26. </span>Application of LLM in IR (WIP)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-dense-models">25.1. Semantic Dense Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">25.1.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-architecture-paradigms">25.1.2. Two Architecture Paradigms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classic-representation-based-learning">25.1.3. Classic Representation-based Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dssm">25.1.3.1. DSSM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-dssm">25.1.3.2. CNN-DSSM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfomer-retrievers-and-rerankers">25.2. Transfomer Retrievers and Rerankers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">25.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bi-encoder-retriever">25.2.2. Bi-Encoder Retriever</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-encoder-for-point-wise-ranking">25.2.3. Cross-Encoder For Point-wise Ranking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#duo-bert-for-pairwise-ranking">25.2.4. Duo-BERT For Pairwise Ranking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multistage-retrieval-and-ranking-pipeline">25.2.5. Multistage Retrieval and Ranking Pipeline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dc-bert">25.2.6. DC-BERT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-attribute-and-multi-task-modeling">25.2.7. Multi-Attribute and Multi-task Modeling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-vector-retrievers">25.3. Multi-Vector Retrievers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">25.3.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colbert">25.3.2. ColBERT</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">25.3.2.1. Overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding">25.3.2.2. Encoding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#late-interaction">25.3.2.3. Late Interaction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-re-ranking">25.3.2.4. Retrieval &amp; Re-ranking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">25.3.2.5. Evaluation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-clusters-as-pseudo-query-embeddings">25.3.3. Semantic Clusters As Pseudo Query Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-level-representation-and-retrieval-colbert">25.3.4. Token-level Representation and Retrieval (ColBert)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#colbert-v2">25.3.5. Colbert v2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ranker-training">25.4. Ranker Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">25.4.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">25.4.2. Training Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training-objective-functions">25.4.3. Model Training Objective Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-regression-objective">25.4.3.1. Pointwise Regression Objective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-ranking-objective">25.4.3.2. Pointwise Ranking Objective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-ranking-via-triplet-loss">25.4.3.3. Pairwise Ranking via Triplet Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#n-pair-loss">25.4.3.4. N-pair Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#n-pair-dual-loss">25.4.3.5. N-pair Dual Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#doc-doc-n-pair-loss">25.4.3.6. Doc-Doc N-pair Loss</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-sampling-strategies">25.5. Training Data Sampling Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principles">25.5.1. Principles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-sampling-methods-i-heuristic-methods">25.5.2. Negative Sampling Methods I: Heuristic Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-negatives-and-in-batch-negatives">25.5.2.1. Random Negatives and In-batch Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#popularity-based-negative-sampling">25.5.2.2. Popularity-based Negative Sampling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-aware-negative-sampling">25.5.2.3. Topic-aware Negative Sampling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-sampling-methods-ii-model-based-methods">25.5.3. Negative Sampling Methods II: Model-based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#static-hard-negative-examples">25.5.3.1. Static Hard Negative Examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-hard-negative-mining">25.5.3.2. Dynamic Hard Negative Mining</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-batch-large-scale-negatives">25.5.3.3. Cross-Batch Large-Scale Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum-negatives">25.5.3.4. Momentum Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-positives">25.5.3.5. Hard Positives</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label-denoising">25.5.4. Label Denoising</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-negatives">25.5.4.1. False Negatives</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-positives">25.5.4.2. False Positives</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-augmentation">25.5.5. Data Augmentation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation">25.6. Knowledge Distillation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">25.6.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation-training-framework">25.6.2. Knowledge Distillation Training Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-distillation-strategies">25.6.3. Example Distillation Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bi-encoder-teacher-distillation">25.6.3.1. Bi-encoder Teacher Distillation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-encoder-embedding-similarity-distillation">25.6.3.2. Cross-Encoder Embedding Similarity Distillation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-teacher-distillation">25.6.3.3. Ensemble Teacher Distillation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-for-retrieval">25.7. Pretraining for Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contriever">25.7.1. Contriever</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-sparse-and-dense-retrieval">25.8. Discussion: Sparse and Dense Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-size-and-embedding-dimensionality-impact">25.8.1. Index Size and Embedding Dimensionality Impact</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-nearest-neighbor-search">25.9. Approximate Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">25.9.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-quantization">25.9.2. Vector Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-representation-and-storage">25.9.2.1. Approximate Representation And Storage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-distances-using-quantized-codes">25.9.2.2. Approximating Distances Using Quantized Codes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#product-quantization">25.9.3. Product Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#from-vector-quantization-to-product-quantization">25.9.3.1. From Vector Quantization To Product Quantization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">25.9.3.2. Approximating Distances Using Quantized Codes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-non-exhaustive-nearest-neighbor-search">25.9.4. Approximate Non-exhaustive Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-quantization-and-inverted-file-indexing">25.9.4.1. Hierarchical Quantization And Inverted File Indexing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-datasets">25.10. Benchmark Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ms-marco">25.10.1. MS MARCO</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-ranking-task">25.10.1.1. Document Ranking Task</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#passage-ranking-task">25.10.1.2. Passage Ranking Task</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terc">25.10.2. TERC</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trec-deep-learning-track">25.10.2.1. TREC-deep Learning Track</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trec-car">25.10.2.2. TREC-CAR</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-question-nq">25.10.3. Natural Question (NQ)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entity-questions">25.10.4. Entity Questions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beir">25.10.5. BEIR</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lotte">25.10.6. LoTTE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mteb">25.10.7. MTEB</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-on-bibliography-and-software">25.11. Note On Bibliography And Software</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">25.11.1. Bibliography</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">25.11.2. Software</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>