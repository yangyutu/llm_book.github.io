
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>17. Llama Series &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_LLM_case_study/llama_series';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="18. DeepSeek Series" href="deepseek_series.html" />
    <link rel="prev" title="16. *Reinforcement Learning Essentials" href="../chapter_training/reinforcement_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architecture Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reasoning.html">14. LLM Reasoning (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">17. Llama Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="deepseek_series.html">18. DeepSeek Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">23. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">24. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">25. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">26. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">27. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">28. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">29. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Llama Series</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-1">17.1. Llama 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2">17.2. Llama 2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">17.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning-and-alignment">17.2.2. Finetuning and Alignment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">17.2.2.1. Supervised Fine-Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-collection">17.2.2.2. Preference Data Collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">17.2.2.3. Reward Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-alignment">17.2.2.4. Iterative Alignment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">17.3. Llama 3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">17.3.1. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training">17.3.2. Post-Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">17.4. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llama-series">
<h1><span class="section-number">17. </span>Llama Series<a class="headerlink" href="#llama-series" title="Link to this heading">#</a></h1>
<section id="llama-1">
<h2><span class="section-number">17.1. </span>Llama 1<a class="headerlink" href="#llama-1" title="Link to this heading">#</a></h2>
<p>Llama-1 is first open-source model coming from Meta <span id="id1">[<a class="reference internal" href="#id530" title="Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and et al. LLaMA: open and efficient foundation language models. ArXiv:2302.13971, 2023a.">TLI+3a</a>]</span>. Compared to GPT, the following changes have been made to enhance training stability and model performance:</p>
<ul class="simple">
<li><p><strong>Pre-RMSNorm</strong> is used as a layer normalization method</p></li>
<li><p><strong>SwiGLU</strong> is used as the activation function</p></li>
<li><p><strong>RoPE</strong> is used as position encoding to enhance long sequence modeling ability</p></li>
</ul>
<p>Llama-1 training was using next-token-prediction self-supervised learning on <strong>1.4T</strong> token, as a comparison GPT-3 was roughly trained on <strong>500B</strong> tokens. These pre-training data are mixed from multiple sources and are all public data. The data volume and sampling ratio of each source are shown in the table below</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Dataset</p></th>
<th class="head text-center"><p>Sampling prop.</p></th>
<th class="head text-center"><p>Epochs</p></th>
<th class="head text-right"><p>Disk size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>CommonCrawl</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(67.0 \%\)</span></p></td>
<td class="text-center"><p>1.10</p></td>
<td class="text-right"><p>3.3 TB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>C4</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(15.0 \%\)</span></p></td>
<td class="text-center"><p>1.06</p></td>
<td class="text-right"><p>783 GB</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Github</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(4.5 \%\)</span></p></td>
<td class="text-center"><p>0.64</p></td>
<td class="text-right"><p>328 GB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Wikipedia</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(4.5 \%\)</span></p></td>
<td class="text-center"><p>2.45</p></td>
<td class="text-right"><p>83 GB</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Books</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(4.5 \%\)</span></p></td>
<td class="text-center"><p>2.23</p></td>
<td class="text-right"><p>85 GB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ArXiv</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(2.5 \%\)</span></p></td>
<td class="text-center"><p>1.06</p></td>
<td class="text-right"><p>92 GB</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>StackExchange</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(2.0 \%\)</span></p></td>
<td class="text-center"><p>1.03</p></td>
<td class="text-right"><p>78 GB</p></td>
</tr>
</tbody>
</table>
</div>
<p>Llama-1 is considered as a big milestone as it shows that</p>
<ul class="simple">
<li><p>It is possible to train state-of-the-art models using publicly available datasets exclusively,</p></li>
<li><p>To optimize for inference time efficiency, one can consider training a small model with large amount of data (e.g., training time optimal effiency would suggest training on 10B model on 200B tokens based on <span id="id2">[<a class="reference internal" href="#id444" title="Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, and et al. Training compute-optimal large language models. ArXiv:2203.15556, 2022.">HBM+22</a>]</span>)</p></li>
</ul>
<p>Evaluting on benchmarks including standard common sense reasoning, closed QA, reading comprehension, mathematical reasoning, code generation, and language understanding,  LLaMA-1 13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-1 65B is competitive with the best models, Chinchilla-70B and PaLM-540B.</p>
</section>
<section id="llama-2">
<h2><span class="section-number">17.2. </span>Llama 2<a class="headerlink" href="#llama-2" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">17.2.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>Contribution from Llama-2 <span id="id3">[<a class="reference internal" href="#id531" title="Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. LLaMA 2: open foundation and fine-tuned chat models. ArXiv:2307.09288, 2023b.">TMS+3b</a>]</span> are two-folds:</p>
<ul class="simple">
<li><p>Improvement on pretraining, with a slight change of model architecture:</p>
<ul>
<li><p>Llama-2 pre-training used 2T data tokens from publicly available sources, with 40% more data compared to Llama-1.</p></li>
<li><p>Adopt GQA for larger sizes (i.e., 34B, 70B) to reduce the overall parameter quantity.</p></li>
<li><p>Double the context length from 2k to 4k</p></li>
</ul>
</li>
<li><p>Concentrated improvement on post-pretraining, including SFT, iterative reward modeling, and RLHF to enhance the model’s ability in diagolue and instruction-following safely.</p></li>
</ul>
<p>The improvement of pretraining for Llama-2 vs Llama-1 is summarized in the following table. Essentially, additional data quantity and improved data quality bring additional gain across all benchmarks.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model</p></th>
<th class="head text-center"><p>Size</p></th>
<th class="head text-center"><p>Code</p></th>
<th class="head text-center"><p>Commonsense Reasoning</p></th>
<th class="head text-center"><p>World Knowledge</p></th>
<th class="head text-center"><p>Reading Comprehension</p></th>
<th class="head text-center"><p>Math</p></th>
<th class="head text-center"><p>MMLU</p></th>
<th class="head text-center"><p>BBH</p></th>
<th class="head text-center"><p>AGI Eval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>LLAMA 1</p></td>
<td class="text-center"><p>13B</p></td>
<td class="text-center"><p>18.9</p></td>
<td class="text-center"><p>66.1</p></td>
<td class="text-center"><p>52.6</p></td>
<td class="text-center"><p>62.3</p></td>
<td class="text-center"><p>10.9</p></td>
<td class="text-center"><p>46.9</p></td>
<td class="text-center"><p>37.0</p></td>
<td class="text-center"><p>33.9</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p>33B</p></td>
<td class="text-center"><p>26.0</p></td>
<td class="text-center"><p>70.0</p></td>
<td class="text-center"><p>58.4</p></td>
<td class="text-center"><p>67.6</p></td>
<td class="text-center"><p>21.4</p></td>
<td class="text-center"><p>57.8</p></td>
<td class="text-center"><p>39.8</p></td>
<td class="text-center"><p>41.7</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p>65B</p></td>
<td class="text-center"><p>30.7</p></td>
<td class="text-center"><p>70.7</p></td>
<td class="text-center"><p>60.5</p></td>
<td class="text-center"><p>68.6</p></td>
<td class="text-center"><p>30.8</p></td>
<td class="text-center"><p>63.4</p></td>
<td class="text-center"><p>43.5</p></td>
<td class="text-center"><p>47.6</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>LLAMA 2</p></td>
<td class="text-center"><p>7B</p></td>
<td class="text-center"><p>16.8</p></td>
<td class="text-center"><p>63.9</p></td>
<td class="text-center"><p>48.9</p></td>
<td class="text-center"><p>61.3</p></td>
<td class="text-center"><p>14.6</p></td>
<td class="text-center"><p>45.3</p></td>
<td class="text-center"><p>32.6</p></td>
<td class="text-center"><p>29.3</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p>13B</p></td>
<td class="text-center"><p>24.5</p></td>
<td class="text-center"><p>66.9</p></td>
<td class="text-center"><p>55.4</p></td>
<td class="text-center"><p>65.8</p></td>
<td class="text-center"><p>28.7</p></td>
<td class="text-center"><p>54.8</p></td>
<td class="text-center"><p>39.4</p></td>
<td class="text-center"><p>39.1</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p>34B</p></td>
<td class="text-center"><p>27.8</p></td>
<td class="text-center"><p>69.9</p></td>
<td class="text-center"><p>58.7</p></td>
<td class="text-center"><p>68.0</p></td>
<td class="text-center"><p>24.2</p></td>
<td class="text-center"><p>62.6</p></td>
<td class="text-center"><p>44.1</p></td>
<td class="text-center"><p>43.4</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p>70B</p></td>
<td class="text-center"><p>37.5</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 1 . 9}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{6 3 . 6}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{6 9 . 4}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{3 5 . 2}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{6 8 . 9}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{5 1 . 2}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{5 4 . 2}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>In the following, we review key process for pretraining and post-training alignment of LLama2 model to human preference [<a class="reference internal" href="#chapter-training-fig-alignmenet-llama2-alignment"><span class="std std-numref">Fig. 17.1</span></a>]. Specifially, LLama2 alignment consists of the following iterative steps:</p>
<ul class="simple">
<li><p>Collecting human preference data</p></li>
<li><p>Training reward model to predict human preference, iteratively</p></li>
<li><p>Using reward model to guide model improvement via rejection sampleing SFT and RLHF</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-alignmenet-llama2-alignment">
<a class="reference internal image-reference" href="../../_images/llama2_alignment.png"><img alt="../../_images/llama2_alignment.png" src="../../_images/llama2_alignment.png" style="width: 875.6999999999999px; height: 407.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.1 </span><span class="caption-text">Overview of iterative alignment process in LLama2 after pretraining and supervised fine-tuning. Note that reward model is also iteratively updated the ensure the reward modeling remain within distribution. Image from <span id="id4">[<a class="reference internal" href="#id530" title="Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and et al. LLaMA: open and efficient foundation language models. ArXiv:2302.13971, 2023a.">TLI+3a</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignmenet-llama2-alignment" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="finetuning-and-alignment">
<h3><span class="section-number">17.2.2. </span>Finetuning and Alignment<a class="headerlink" href="#finetuning-and-alignment" title="Link to this heading">#</a></h3>
<section id="supervised-fine-tuning">
<h4><span class="section-number">17.2.2.1. </span>Supervised Fine-Tuning<a class="headerlink" href="#supervised-fine-tuning" title="Link to this heading">#</a></h4>
<p>The pretrained model was first supervised fine-tuned before further RLHF. The fine-tuned data is focused on improving the model’s helpfulness and safety by providing positive examples. The <strong>quality and diversty is more important than quanity</strong> - data in the order of tens of thousands were sufficient for high quality results.</p>
<figure class="align-default" id="chapter-llm-case-study-fig-llama2-sft-data-example">
<a class="reference internal image-reference" href="../../_images/SFT_data_example.png"><img alt="../../_images/SFT_data_example.png" src="../../_images/SFT_data_example.png" style="width: 652.0px; height: 307.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.2 </span><span class="caption-text">Example of a helpfulness (top) and safety (bottom) training data for SFT. Image from <span id="id5">[<a class="reference internal" href="#id530" title="Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and et al. LLaMA: open and efficient foundation language models. ArXiv:2302.13971, 2023a.">TLI+3a</a>]</span>.</span><a class="headerlink" href="#chapter-llm-case-study-fig-llama2-sft-data-example" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="preference-data-collection">
<h4><span class="section-number">17.2.2.2. </span>Preference Data Collection<a class="headerlink" href="#preference-data-collection" title="Link to this heading">#</a></h4>
<p>To collect comprehensive human feedback data, the Meta team considered both <strong>open-source</strong> and <strong>in-house</strong> data.</p>
<p>For open-source data, they used datasets from Anthropic, OpenAI, etc., containing approximately 1.50M human preference data points. These data primarily focused on two aspects: safety and usefulness, where safety refers to whether the model produces unsafe outputs, and usefulness refers to the extent to which the model’s outputs can address human requests.</p>
<p>For in-house data, they hired human annotator to produce both safety and usefulness labels on about ~1.4M data points. Annotators first wrote an input prompt, then selected outputs from two models based on corresponding criteria to serve as positive and negative examples.</p>
<p>Note that these preference data is used in reward model training. In the iterative alignment process of LLaMA-2, the distribution of model-generated content would change, leading to degradation of the reward model. To prevent this phenomenon, <strong>new annotated data needed to be collected during the training process to retrain the reward model.</strong></p>
</section>
<section id="reward-modeling">
<h4><span class="section-number">17.2.2.3. </span>Reward Modeling<a class="headerlink" href="#reward-modeling" title="Link to this heading">#</a></h4>
<p>After collecting human feedback data, reward models are trained based on the collected data, which are then used to provide on-the-fly feedback signal in subsequent training processes. To obtain more detailed reward signals, data related to safety tasks and usefulness tasks are used separately to train two reward models.</p>
<p>To better help reward models distinguish the differences between positive and negative examples, a margin <span class="math notranslate nohighlight">\(m(y^+, y^-)\)</span> is added between human preference between positive and negative examples. The optimized training objective for the reward model is shown in the following equation:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x, y^+) - r_\theta(x, y^-) - m(y^+, y^-)))
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y^+\)</span>, and <span class="math notranslate nohighlight">\(y^-\)</span> represent the model input, positive example, and negative example respectively. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses</p>
</section>
<section id="iterative-alignment">
<h4><span class="section-number">17.2.2.4. </span>Iterative Alignment<a class="headerlink" href="#iterative-alignment" title="Link to this heading">#</a></h4>
<p>LLama-2 took a iterative approch to align the LLM to human preference. As the alignment progresses, we are able to train better reward models and collect more prompts. We therefore can train successive versions for RLHF models, referred to here as RLHF-V1, …, RLHF-V5.
Two alignment algorithms are explored:</p>
<ul class="simple">
<li><p><strong>SFT with Rejection Sampling</strong>. <span class="math notranslate nohighlight">\(K\)</span> outputs from the model (might include previous version model) and select the best candidate with the reward model. The highest rewarded output is collected to perform SFT.</p></li>
<li><p><strong>Proximal Policy Optimization (PPO)</strong> as in the standard RLHF literature.</p></li>
</ul>
<p>Until RLHF (V4), only Rejection Sampling SFT is only used, and after that, two algorithms were combined sequentially - applying PPO on top of the resulted Rejection Sampling checkpoint before sampling again.</p>
</section>
</section>
</section>
<section id="llama-3">
<h2><span class="section-number">17.3. </span>Llama 3<a class="headerlink" href="#llama-3" title="Link to this heading">#</a></h2>
<p>Llama-3 uses the same model architecture of Llama-2. The key improvement are summarized below.</p>
<ul class="simple">
<li><p><strong>Training data</strong>: Llama-3 was trained on 15T tokens (~8 times of the ~2T tokens used in Llama2) to enhance language understanding, coding, and reasoning. Training data were carefully selected, with good quality by leveraging Llama2 model.</p></li>
<li><p><strong>Coding and reasoning</strong>: The code training data has been expanded 4 times, significantly improving the model’s performance in coding and logical reasoning abilities.</p></li>
<li><p><strong>Large context window</strong>: Llama 3 supports condex window of 128k, whereas Llama 2 only supports context window of 4k.</p></li>
<li><p><strong>Multi-lingual support</strong>: The training data of LLaMA-3 includes over 5% non-English tokens from more than 30 languages, which significantly improves its multilingual processing capabilities</p></li>
<li><p><strong>Multimodality support</strong>: By adding multi-modality adapters, one can leverage Llama 3 to take inputs from images, videos, and audio in addition to text.</p></li>
<li><p><strong>Scale and performance</strong>: Llama 3 has three model sizes: 8B, 70B, 405B; Llama 3 achieved competitive performances among models of similar sizes [<a class="reference internal" href="#chapter-llm-case-study-fig-llama3-llama3-performance"><span class="std std-numref">Fig. 17.3</span></a>].</p></li>
</ul>
<figure class="align-default" id="chapter-llm-case-study-fig-llama3-llama3-performance">
<a class="reference internal image-reference" href="../../_images/llama3_performance.png"><img alt="../../_images/llama3_performance.png" src="../../_images/llama3_performance.png" style="width: 602.6999999999999px; height: 297.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.3 </span><span class="caption-text">Comparison of the FT Llama3 model with other models across a diverse benchmarks.  Image from <span id="id6">[<a class="reference internal" href="#id1622" title="Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.">DJP+24</a>]</span>.</span><a class="headerlink" href="#chapter-llm-case-study-fig-llama3-llama3-performance" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The training Llama-3 has two phases:</p>
<ul class="simple">
<li><p><strong>Pre-training phase</strong>, which conducts next-token-prediction task on large-scale data. Particularly a series of scaling laws was developed to guide the selection of data composition.</p></li>
<li><p><strong>Post-training phase</strong>, which invovles using SFT, rejection sampling, RLHF, and DPO for multi-rounds to improve the alignment outcome.</p></li>
</ul>
<section id="pretraining">
<h3><span class="section-number">17.3.1. </span>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">#</a></h3>
<p>Critical factors affecting pretraining results include:</p>
<ul class="simple">
<li><p><strong>Data quanity, quality, and distribution</strong>. Meta has developed a series of data filtering pipelines to ensure data quality, including heuristic filters, NSFW filters, semantic duplicate data removal technology, and text classifiers for predicting data quality. The effectiveness of these tools benefits from the performance of previous versions of Llama, especially in identifying high-quality data.</p></li>
<li><p><strong>Optimal model size and data mixture</strong> with given the pre-training compute budget - undertraining a larger model or overtraining a small model will yield inferior results[<a class="reference internal" href="#chapter-llm-case-study-fig-llama3-llama3-scaling-law"><span class="std std-numref">Fig. 17.4</span></a>]. <strong>Scaling laws</strong> were first developped to establish the relationship between downstream performance and compute budget and then used to identify optimal model size and data mixture.</p></li>
</ul>
<figure class="align-default" id="chapter-llm-case-study-fig-llama3-llama3-scaling-law">
<a class="reference internal image-reference" href="../../_images/llama3_scaling_law.png"><img alt="../../_images/llama3_scaling_law.png" src="../../_images/llama3_scaling_law.png" style="width: 385.8px; height: 284.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.4 </span><span class="caption-text">Under different compute budget contraints (different curves), different model sizes (dots on curves) can achieve different performance. Red points at parabola minimums are correponding optimal model size with given budgets. Image from <span id="id7">[<a class="reference internal" href="#id1622" title="Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.">DJP+24</a>]</span>.</span><a class="headerlink" href="#chapter-llm-case-study-fig-llama3-llama3-scaling-law" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>long-context extension</strong> was implemented via a multi-stage training fashion: Llama 3 was first pretrained using a context window of 8K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128K tokens. This long-context pre-training stage was performed using approximately 800B training tokens.</p>
</section>
<section id="post-training">
<h3><span class="section-number">17.3.2. </span>Post-Training<a class="headerlink" href="#post-training" title="Link to this heading">#</a></h3>
<p>The post-training workflow in summarized in <a class="reference internal" href="#chapter-llm-case-study-fig-llama3-posttraining-overview"><span class="std std-numref">Fig. 17.5</span></a>, with the following key steps:</p>
<ul class="simple">
<li><p>Collection of task prompts, completions, and human annotations.</p></li>
<li><p>Reward modeling to train a reward model based on pairwise ranking loss.</p></li>
<li><p>Rejection sampling to obtain high-quality positive examples.</p></li>
<li><p>Iterative improvement via SFT and DPO</p></li>
</ul>
<figure class="align-default" id="chapter-llm-case-study-fig-llama3-posttraining-overview">
<a class="reference internal image-reference" href="../../_images/posttraining_overview.png"><img alt="../../_images/posttraining_overview.png" src="../../_images/posttraining_overview.png" style="width: 870.0999999999999px; height: 326.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.5 </span><span class="caption-text">Illustration of the overall post-training approach for Llama 3. The post-training strategy involves rejection sampling, supervised finetuning, and direct preference optimization. Image from <span id="id8">[<a class="reference internal" href="#id1622" title="Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.">DJP+24</a>]</span>.</span><a class="headerlink" href="#chapter-llm-case-study-fig-llama3-posttraining-overview" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Training data for post-training consists of SFT data and preference data.</p>
<p>SFT data consists of following sources:</p>
<ul class="simple">
<li><p>High quality responses from <strong>rejection-sampling</strong> - An LLM model checkpoint is used to generate K (between 10 and 30) responses for each prompt, and then scores these answers using the Reward Model to select the highest-scoring and best-performing answer.</p></li>
<li><p><strong>Synthetic data</strong> was used to handle issues in code generation, including difficulty in following instructions, code syntax errors, incorrect code generation, and difficulty in fixing bugs.</p></li>
</ul>
<p>Preference data follows a similar annotation procedure in Llama-2. A new step include encouraging annotators to further improve the prefered responses. A small portion of data have following preference rank:</p>
<div class="math notranslate nohighlight">
\[\operatorname{edited} \succ \operatorname{chosen} \succ rejected\]</div>
<p>SFT uses a standard cross entropy loss only on the target tokens, while <strong>masking out loss on prompt tokens</strong>.</p>
<p>DPO from LLaMA3 has the following improvements:</p>
<ul class="simple">
<li><p>In DPO loss, formating related tokens (including the header and termination tokens) are masked out to stabilize DPO training. These tokens barely contribute to the score of the responses, yet they affects the preference loss computation.</p></li>
<li><p><strong>Regularizated DPO</strong>: an additional negative log likelihood loss of the chosen response was added to further stabilize DPO training by maintaining the generated expected format and preventing the decrease of log probability of the chosen response. See more details in <a class="reference internal" href="../chapter_training/alignment.html#chapter-training-sec-llm-alignment-dpo-variant-dpop-regularized-dpo"><span class="std std-ref">DPO-Positive and Regularized DPO</span></a>.</p></li>
</ul>
<!-- ## CodeLlama

{cite:p}`roziere2023code` -->
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">17.4. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id9">
<div role="list" class="citation-list">
<div class="citation" id="id1622" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DJP+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id7">2</a>,<a role="doc-backlink" href="#id8">3</a>)</span>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and others. The llama 3 herd of models. <em>arXiv preprint arXiv:2407.21783</em>, 2024.</p>
</div>
<div class="citation" id="id444" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">HBM+22</a><span class="fn-bracket">]</span></span>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, and et al. Training compute-optimal large language models. <em>ArXiv:2203.15556</em>, 2022.</p>
</div>
<div class="citation" id="id530" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TLI+3a<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>)</span>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and et al. LLaMA: open and efficient foundation language models. <em>ArXiv:2302.13971</em>, 2023a.</p>
</div>
<div class="citation" id="id531" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">TMS+3b</a><span class="fn-bracket">]</span></span>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. LLaMA 2: open foundation and fine-tuned chat models. <em>ArXiv:2307.09288</em>, 2023b.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_LLM_case_study"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_training/reinforcement_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16. </span>*Reinforcement Learning Essentials</p>
      </div>
    </a>
    <a class="right-next"
       href="deepseek_series.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">18. </span>DeepSeek Series</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-1">17.1. Llama 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2">17.2. Llama 2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">17.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning-and-alignment">17.2.2. Finetuning and Alignment</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning">17.2.2.1. Supervised Fine-Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-collection">17.2.2.2. Preference Data Collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">17.2.2.3. Reward Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-alignment">17.2.2.4. Iterative Alignment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">17.3. Llama 3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">17.3.1. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training">17.3.2. Post-Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">17.4. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>