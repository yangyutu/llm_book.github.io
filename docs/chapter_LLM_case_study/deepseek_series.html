
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>17. DeepSeek Series &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_LLM_case_study/deepseek_series';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="18. Llama Series" href="llama_series.html" />
    <link rel="prev" title="16. *Reinforcement Learning Essentials" href="../chapter_training/reinforcement_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reasoning.html">14. LLM Reasoning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">17. DeepSeek Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="llama_series.html">18. Llama Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">23. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">24. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">25. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">26. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">27. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">28. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">29. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/conversational_IR.html">30. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">31. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">32. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DeepSeek Series</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-v3">17.1. DeepSeek V3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">17.1.1. Pretraining</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-coder">17.2. DeepSeek Coder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-curation">17.2.1. Data Curation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-strategy">17.2.2. PreTraining Strategy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#long-context-extension">17.2.3. Long Context Extension</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intruction-tuning">17.2.4. Intruction Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">17.2.5. Reinforcement Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">17.2.6. Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-math">17.3. DeepSeek Math</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">17.3.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pipeline">17.3.2. Data Pipeline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">17.3.3. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-sft-and-rl">17.3.4. Post-Training: SFT and RL</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-reasoning-models">17.4. DeepSeek Reasoning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1-zero">17.4.1. DeepSeek-R1-Zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1">17.4.2. DeepSeek-R1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1-distillation">17.4.3. DeepSeek-R1-Distillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">17.5. Bibliography</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">17.5.1. Software</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deepseek-series">
<h1><span class="section-number">17. </span>DeepSeek Series<a class="headerlink" href="#deepseek-series" title="Link to this heading">#</a></h1>
<section id="deepseek-v3">
<h2><span class="section-number">17.1. </span>DeepSeek V3<a class="headerlink" href="#deepseek-v3" title="Link to this heading">#</a></h2>
<p>Pretraining corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer.</p>
<section id="pretraining">
<h3><span class="section-number">17.1.1. </span>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="deepseek-coder">
<h2><span class="section-number">17.2. </span>DeepSeek Coder<a class="headerlink" href="#deepseek-coder" title="Link to this heading">#</a></h2>
<p><span id="id1">[<a class="reference internal" href="#id1619" title="Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, and others. Deepseek-coder: when the large language model meets programming–the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.">GZY+24</a>]</span>
<span id="id2">[<a class="reference internal" href="#id1622" title="Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, and others. Deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024.">ZGS+24</a>]</span></p>
<section id="data-curation">
<h3><span class="section-number">17.2.1. </span>Data Curation<a class="headerlink" href="#data-curation" title="Link to this heading">#</a></h3>
<p>The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English coderelated natural language corpus, and 3% code-unrelated Chinese natural language corpus.</p>
</section>
<section id="pretraining-strategy">
<h3><span class="section-number">17.2.2. </span>PreTraining Strategy<a class="headerlink" href="#pretraining-strategy" title="Link to this heading">#</a></h3>
<p>Two training strategies:</p>
<ul class="simple">
<li><p>Next Token Prediction. Like the typical language model next-token-prediction task. In this process, various files are concatenated to form a fixed-length entry.</p></li>
<li><p>Fill-in-the-Middle. In the code pre-training
scenario, it is often necessary to generate corresponding inserted content based on the given
context and subsequent text. Due to specific dependencies in a programming language, relying
solely on next token prediction is insufficient to learn this fill-in-the-middle capability.</p></li>
</ul>
<p>Fill-in-the Middle was proposed in <span id="id3">[<a class="reference internal" href="#id1620" title="Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.">BJT+22</a>, <a class="reference internal" href="#id1621" title="Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, and others. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.">LAZ+23</a>]</span></p>
<p>This approach involves randomly dividing the text into three parts, then shuffling the order of these parts and connecting them with special characters. This method aims to incorporate a fill-in-the-blank pretraining task during the training process.</p>
<p>Within the FIM methodology, two distinct modes are employed: PSM (Prefix-Suffix-Middle) and SPM
(Suffix-Prefix-Middle). In the PSM mode, the training corpus is organized in the sequence
of prefix, suffix, middle, aligning the text in a way that the middle segment is flanked by the
prefix and suffix. Conversely, the SPM mode arranges the segments as suffix, prefix, middle,
presenting a different structural challenge. These modes are instrumental in enhancing the
model’s capability to handle various structural arrangements in code, providing a robust training
framework for advanced code prediction tasks.</p>
</section>
<section id="long-context-extension">
<h3><span class="section-number">17.2.3. </span>Long Context Extension<a class="headerlink" href="#long-context-extension" title="Link to this heading">#</a></h3>
<p>we extend the context length of DeepSeek-Coder-V2 to 128K using
Yarn (Peng et al., 2023). The hyper-parameters of YARN are the same as DeepSeek-V2: the scale
𝑠 to 40, 𝛼 to 1, 𝛽 to 32. We further continue training the model using two stages to enhance
its capability for handling long contexts.</p>
<p>In the first stage, we utilize a sequence length of 32K and a batch size of 1152 for 1000 steps</p>
<p>In the second stage, we train the model for an additional 1000 steps, employing a sequence length of 128K and a batch size of 288 sequences.</p>
</section>
<section id="intruction-tuning">
<h3><span class="section-number">17.2.4. </span>Intruction Tuning<a class="headerlink" href="#intruction-tuning" title="Link to this heading">#</a></h3>
<p>We develop DeepSeek-Coder-Instruct by enhancing the DeepSeek-Coder-Base through instructionbased fine-tuning using high-quality data.</p>
</section>
<section id="reinforcement-learning">
<h3><span class="section-number">17.2.5. </span>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading">#</a></h3>
<p>Reinforcement learning was used to further enhance the reasoning and instruction-following ability.</p>
<p>This stages involve several key elements:</p>
<ul class="simple">
<li><p>Training task prompts collection - this involves collecting prompts related to code and math from various sources, and each code prompt comes with corresponding test cases. After filtering the prompts, there are approximately 40 k data in total.</p></li>
<li><p>Reward modeling:</p>
<ul>
<li><p>Rewards for math problem is constructed based on ground-truth labels.</p></li>
<li><p>Rewards for coding is more complex. Code compiler itself can already provide <span class="math notranslate nohighlight">\(0-1\)</span> feedback (whether the code pass all test cases or not) but some coding task prompts have a limited number of test cases for full coverage. Instead, the team trained a reward model on the data provided by the compiler, and use the reward model to provide signal during RL training, which is more robust.</p></li>
</ul>
</li>
<li><p>The reinforcement learning algorithm is GRPO (see <a class="reference internal" href="../chapter_training/alignment.html#chapter-training-sec-llm-alignment-grpo"><span class="std std-ref">Group Relative Policy Optimization (GRPO)</span></a>).</p></li>
</ul>
</section>
<section id="evaluation">
<h3><span class="section-number">17.2.6. </span>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h3>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-coder-performance">
<a class="reference internal image-reference" href="../../_images/deepseek_coder_performance.png"><img alt="../../_images/deepseek_coder_performance.png" src="../../_images/deepseek_coder_performance.png" style="width: 774.8000000000001px; height: 375.05px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.1 </span><span class="caption-text">The performance of DeepSeek-Coder. Image from <span id="id4">[<a class="reference internal" href="#id1619" title="Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, and others. Deepseek-coder: when the large language model meets programming–the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.">GZY+24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-coder-performance" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="deepseek-math">
<h2><span class="section-number">17.3. </span>DeepSeek Math<a class="headerlink" href="#deepseek-math" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">17.3.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common
Crawl, together with natural language and code data.</p>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-math-performance">
<a class="reference internal image-reference" href="../../_images/deepseek_math_performance.png"><img alt="../../_images/deepseek_math_performance.png" src="../../_images/deepseek_math_performance.png" style="width: 427.5px; height: 253.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.2 </span><span class="caption-text">Top1 accuracy of open-source models on the competition-level MATH benchmark Image from <span id="id5">[<a class="reference internal" href="#id1614" title="Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.">SWZ+24</a>]</span></span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-math-performance" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="data-pipeline">
<h3><span class="section-number">17.3.2. </span>Data Pipeline<a class="headerlink" href="#data-pipeline" title="Link to this heading">#</a></h3>
<p>DeepSeekMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This
dataset is extracted from the Common Crawl (CC) using a fastText-based classifier (Joulin et al.,
2016). In the initial iteration, the classifier is trained using instances from OpenWebMath (Paster
et al., 2023) as positive examples, while incorporating a diverse selection of other web pages to
serve as negative examples. Subsequently, we employ the classifier to mine additional positive
instances from the CC, which are further refined through human annotation.</p>
<p>By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath
Corpus, a high-quality dataset of 120B tokens from web pages filtered for mathematical content, which is almost 7 times the size of the math web pages used by Minerva
(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath
(Paster et al., 2023).</p>
<p>The DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and
is the largest in size.</p>
<ul class="simple">
<li><p>High-quality</p></li>
<li><p>Multilingual</p></li>
<li><p>Large-scale</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-math-data-pipeline">
<a class="reference internal image-reference" href="../../_images/deepseek_math_data_pipeline.png"><img alt="../../_images/deepseek_math_data_pipeline.png" src="../../_images/deepseek_math_data_pipeline.png" style="width: 666.6px; height: 257.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.3 </span><span class="caption-text">An iterative pipeline that collects mathematical web pages from Common Crawl. Image from <span id="id6">[<a class="reference internal" href="#id1614" title="Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.">SWZ+24</a>]</span></span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-math-data-pipeline" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Data validation</p>
<p>We apply math training to a general pre-trained language model with 1.3B parameters,
We separately train a model on each mathematical corpus for 150B tokens.</p>
<p>evaluated using few-shot chain-of-thought prompting.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Math Corpus</p></th>
<th class="head text-center"><p>Size</p></th>
<th class="head text-center"><p>GSM8K</p></th>
<th class="head text-center"><p>MATH</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>No Math Training</p></td>
<td class="text-center"><p>N/A</p></td>
<td class="text-center"><p>2.9%</p></td>
<td class="text-center"><p>3.0%</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>MathPile</p></td>
<td class="text-center"><p>8.9B</p></td>
<td class="text-center"><p>2.7%</p></td>
<td class="text-center"><p>3.3%</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>OpenWebMath</p></td>
<td class="text-center"><p>13.6B</p></td>
<td class="text-center"><p>11.5%</p></td>
<td class="text-center"><p>8.9%</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Proof-Pile-2</p></td>
<td class="text-center"><p>51.9B</p></td>
<td class="text-center"><p>14.3%</p></td>
<td class="text-center"><p>11.2%</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DeepSeekMath Corpus</p></td>
<td class="text-center"><p>120.2B</p></td>
<td class="text-center"><p>23.8%</p></td>
<td class="text-center"><p>13.6%</p></td>
</tr>
</tbody>
</table>
</div>
<p>Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance
with Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not
the only key factor in mathematical reasoning capability. A smaller model pre-trained on
high-quality data could achieve strong performance as well.</p>
</section>
<section id="id7">
<h3><span class="section-number">17.3.3. </span>Pretraining<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Our model is initialized with DeepSeek-Coder-Base-v1.5 7B</p>
<p>and trained for 500B tokens. The distribution of the data is as follows: 56%
is from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github
code, and the remaining 10% is natural language data from Common Crawl in both English and
Chinese.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Percentage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>DeepSeekMath Corpus</p></td>
<td class="text-left"><p>56%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>AlgebraicStack</p></td>
<td class="text-left"><p>4%</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>arXiv</p></td>
<td class="text-left"><p>10%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Github code</p></td>
<td class="text-left"><p>20%</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>natural language data</p></td>
<td class="text-left"><p>10%</p></td>
</tr>
</tbody>
</table>
</div>
<p>assessment</p>
<ul class="simple">
<li><p>produce self-contained mathematical solutions without relying on external tools</p></li>
<li><p>solve mathematical problems using tools</p></li>
<li><p>conduct formal theorem proving</p></li>
</ul>
<p>Models are evaluated with chain-of-thought prompting</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model</p></th>
<th class="head text-center"><p>Size</p></th>
<th class="head text-center"><p>English Benchmarks</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p>Chinese Benchmarks</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>GSM8K</p></td>
<td class="text-center"><p>MATH</p></td>
<td class="text-center"><p>OCW</p></td>
<td class="text-center"><p>SAT</p></td>
<td class="text-center"><p>MMLU STEM</p></td>
<td class="text-center"><p>CMATH</p></td>
<td class="text-center"><p>Gaokao MathCloze</p></td>
<td class="text-center"><p>Gaokao <br> MathQA</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Mistral</p></td>
<td class="text-center"><p>7B</p></td>
<td class="text-center"><p>40.3%</p></td>
<td class="text-center"><p>14.3%</p></td>
<td class="text-center"><p>9.2%</p></td>
<td class="text-center"><p>71.9%</p></td>
<td class="text-center"><p>51.1%</p></td>
<td class="text-center"><p>44.9%</p></td>
<td class="text-center"><p>5.1%</p></td>
<td class="text-center"><p>23.4%</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Llemma</p></td>
<td class="text-center"><p>7B</p></td>
<td class="text-center"><p>37.4%</p></td>
<td class="text-center"><p>18.1%</p></td>
<td class="text-center"><p>6.3%</p></td>
<td class="text-center"><p>59.4%</p></td>
<td class="text-center"><p>43.1%</p></td>
<td class="text-center"><p>43.4%</p></td>
<td class="text-center"><p>11.9%</p></td>
<td class="text-center"><p>23.6%</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Llemma</p></td>
<td class="text-center"><p>34B</p></td>
<td class="text-center"><p>54.0%</p></td>
<td class="text-center"><p>25.3%</p></td>
<td class="text-center"><p>10.3%</p></td>
<td class="text-center"><p>71.9%</p></td>
<td class="text-center"><p>52.9%</p></td>
<td class="text-center"><p>56.1%</p></td>
<td class="text-center"><p>11.9%</p></td>
<td class="text-center"><p>26.2%</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DeepSeekMath-Base</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>64.2%</p></td>
<td class="text-center"><p>36.2%</p></td>
<td class="text-center"><p>15.4%</p></td>
<td class="text-center"><p>84.4%</p></td>
<td class="text-center"><p>56.5%</p></td>
<td class="text-center"><p>71.7%</p></td>
<td class="text-center"><p>20.3%</p></td>
<td class="text-center"><p>35.3%</p></td>
</tr>
</tbody>
</table>
</div>
<p>Findings from pretraining</p>
<p>Code training benefits program-aided mathematical reasoning in a two-stage training setting where first stage consists of code training and second stage consists of math training.</p>
</section>
<section id="post-training-sft-and-rl">
<h3><span class="section-number">17.3.4. </span>Post-Training: SFT and RL<a class="headerlink" href="#post-training-sft-and-rl" title="Link to this heading">#</a></h3>
<p>After pre-training, we apply mathematical instruction tuning to supervised intruction tuning DeepSeekMath-Base with da containing <strong>chain-of-thought</strong>, <strong>program-of-thought</strong>, and <strong>tool-integrated reasoning</strong>. The total number of training examples is 776K. Specifically,</p>
<ul class="simple">
<li><p>English mathematical datasets covers diverse fields of mathematics, e.g., algebra, probability, number theory, calculus, and geometry. For example, GSM8K and MATH problems with toolintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the training set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or PoT. Our English collection</p></li>
<li><p>Chinese mathematical datasets include  Chinese K-12 mathematical problems spanning 76 sub-topics such as linear equations, with solutions annotated in both CoT and toolintegrated reasoning format.</p></li>
</ul>
<p>We conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-ofthought-format questions related to GSM8K and MATH from the SFT data, which consists
of around 144K questions.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model</p></th>
<th class="head text-center"><p>Size</p></th>
<th class="head text-center"><p>English Benchmarks</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p>Chinese Benchmarks</p></th>
<th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>GSM8K</p></td>
<td class="text-center"><p>MATH</p></td>
<td class="text-center"><p>MGSM-zh</p></td>
<td class="text-center"><p>CMATH</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Gemini Ultra</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>94.4%</p></td>
<td class="text-center"><p>53.2%</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>GPT-4</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>92.0%</p></td>
<td class="text-center"><p>52.9%</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>86.0%</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Qwen</p></td>
<td class="text-center"><p>72B</p></td>
<td class="text-center"><p>78.9%</p></td>
<td class="text-center"><p>35.2%</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DeepSeekMath-Instruct</p></td>
<td class="text-center"><p>7B</p></td>
<td class="text-center"><p>82.9%</p></td>
<td class="text-center"><p>46.8%</p></td>
<td class="text-center"><p>73.2%</p></td>
<td class="text-center"><p>84.6%</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>DeepSeekMath-RL</p></td>
<td class="text-center"><p>7B</p></td>
<td class="text-center"><p>88.2%</p></td>
<td class="text-center"><p>51.7%</p></td>
<td class="text-center"><p>79.6%</p></td>
<td class="text-center"><p>88.8%</p></td>
</tr>
</tbody>
</table>
</div>
<p>DeepSeekMath-RL 7B beats all opensource models from 7B to 70B, as well as the majority of closed-source models. Although
DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data
of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.</p>
</section>
</section>
<section id="deepseek-reasoning-models">
<h2><span class="section-number">17.4. </span>DeepSeek Reasoning Models<a class="headerlink" href="#deepseek-reasoning-models" title="Link to this heading">#</a></h2>
<section id="deepseek-r1-zero">
<h3><span class="section-number">17.4.1. </span>DeepSeek-R1-Zero<a class="headerlink" href="#deepseek-r1-zero" title="Link to this heading">#</a></h3>
<p>DeepSeek Team explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. The resulting model is named as <strong>DeepSeek-R1-Zero</strong>.</p>
<p>Specifically, DeepSeek-R1-Zero starts with DeepSeek-V3-Base as the base model and employ a single-stage GRPO reinforcement learning (see <a class="reference internal" href="../chapter_training/alignment.html#chapter-training-sec-llm-alignment-grpo"><span class="std std-ref">Group Relative Policy Optimization (GRPO)</span></a>) to improve model’s reason ability [<a class="reference internal" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-r1-zero-workflow"><span class="std std-numref">Fig. 17.4</span></a>]. The reward signal consists of two types of rewards:</p>
<ul class="simple">
<li><p>Accuracy rewards: The accuracy reward model evaluates whether the response is correct. (for math and coding problems, the result is deterministic)</p></li>
<li><p>Format rewards: Encourage model to put thinking process between <code class="docutils literal notranslate"><span class="pre">&lt;think&gt;</span></code> <code class="docutils literal notranslate"><span class="pre">&lt;/think&gt;</span></code></p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-r1-zero-workflow">
<a class="reference internal image-reference" href="../../_images/deepseek_r1_zero_workflow.png"><img alt="../../_images/deepseek_r1_zero_workflow.png" src="../../_images/deepseek_r1_zero_workflow.png" style="width: 599.55px; height: 247.79999999999998px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.4 </span><span class="caption-text">The training workflow for DeepSeek-R1-Zero, which is based on pure reinforcement learning. Reward signals are based on Accuracy reward and format reward.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-r1-zero-workflow" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 17.1 </span> (Advantages Pure RL)</p>
<section class="remark-content" id="proof-content">
<p>Pure-RL is slower upfront (trial and error takes time) — but iteliminates the costly, time-intensive labeling bottleneck. Further, the human labeling CoT data is not necessarily the optimal thought process to solve any problems. Using pure RL could have following profound impact in the long run:</p>
<ul class="simple">
<li><p>Without the data labeling bottleneck, it’ll be faster, scalable, and way more efficient for building reasoning models.</p></li>
<li><p>Let’s the model’s self-evolution process to explore better ways to solve problems, instead of relying on human priors. This is necessary for developing superintelligence.</p></li>
</ul>
</section>
</div><p>As the training proceeds, the model started to develop sophisticated reasoning behaviors, such as reflection, where the model
revisits and reevaluates its previous steps and then the model explores of alternative approaches to problem-solving.</p>
<p>Such self-evolution of reasoning behavior is not a result of explicit programm but instead incentivized from the model’s interaction with the reinforcement learning environment (i.e., the reward signal).</p>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 17.2 </span> (Reward modeling for reasoning task)</p>
<section class="remark-content" id="proof-content">
<p>Reward modeling for reasoning task is much more straightforward than typical human preference learning tasks, which requires non-trivial efforts to build reward model to approximate complicated human preference.</p>
<p>The reasoning tasks usually have groundtruth - correct answer or not; as a comparison, human preference is usually hard to quantify.</p>
</section>
</div><p>As shown in <a class="reference internal" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-rzero-thinking-time-evolution"><span class="std std-numref">Fig. 17.5</span></a>, DeepSeek-R1-Zero learns to allocate more thinking time in solving problems</p>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-rzero-thinking-time-evolution">
<a class="reference internal image-reference" href="../../_images/deepseek_rzero_thinking_time_evolution.png"><img alt="../../_images/deepseek_rzero_thinking_time_evolution.png" src="../../_images/deepseek_rzero_thinking_time_evolution.png" style="width: 612.75px; height: 391.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.5 </span><span class="caption-text">The average response length of DeepSeek-R1-Zero on the training set during the RL
process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. Image from <span id="id8">[<a class="reference internal" href="#id1618" title="Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and others. Deepseek-r1: incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.">GYZ+25</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-rzero-thinking-time-evolution" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>AIME 2024</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p>MATH-500</p></th>
<th class="head text-center"><p>GPQA <br> Diamond</p></th>
<th class="head text-center"><p>LiveCode <br> Bench <br> pass&#64;1</p></th>
<th class="head text-center"><p>CodeForces</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>cons&#64;64</p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>pating</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>OpenAI-o1-mini</p></td>
<td class="text-center"><p>63.6</p></td>
<td class="text-center"><p>80.0</p></td>
<td class="text-center"><p>90.0</p></td>
<td class="text-center"><p>60.0</p></td>
<td class="text-center"><p>53.8</p></td>
<td class="text-center"><p>1820</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>OpenAI-o1-0912</p></td>
<td class="text-center"><p>74.4</p></td>
<td class="text-center"><p>83.3</p></td>
<td class="text-center"><p>94.8</p></td>
<td class="text-center"><p>77.3</p></td>
<td class="text-center"><p>63.4</p></td>
<td class="text-center"><p>1843</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>DeepSeek-R1-Zero</p></td>
<td class="text-center"><p>71.0</p></td>
<td class="text-center"><p>86.7</p></td>
<td class="text-center"><p>95.9</p></td>
<td class="text-center"><p>73.3</p></td>
<td class="text-center"><p>50.0</p></td>
<td class="text-center"><p>1444</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>DeepSeek-R1</p></td>
<td class="text-center"><p>71.0</p></td>
<td class="text-center"><p>79.8</p></td>
<td class="text-center"><p>97.3</p></td>
<td class="text-center"><p>71.5</p></td>
<td class="text-center"><p>65.9</p></td>
<td class="text-center"><p>2029</p></td>
</tr>
</tbody>
</table>
</div>
<p>There are also some observed drawbacks from training model - the output has issues like <strong>poor readability</strong> and <strong>language mixing</strong>.
However, this is expected:</p>
<ul class="simple">
<li><p>The reward signal does not enforce readability and language mixing.</p></li>
<li><p>Enforcing readability and language mixing could interfere with the development of reasoning ability during training process.</p></li>
</ul>
</section>
<section id="deepseek-r1">
<h3><span class="section-number">17.4.2. </span>DeepSeek-R1<a class="headerlink" href="#deepseek-r1" title="Link to this heading">#</a></h3>
<p>On top of encourging results from  DeepSeek-R1-Zero, DeepSeek-R1 has the objectives of</p>
<ul class="simple">
<li><p>Further enhancing reasoning performance and accelerate convergence</p></li>
<li><p>Improving user-friendliness by generating human readable, clear anc coherent chain of throught</p></li>
<li><p>Improving the model’s general ability beyond reasoning tasks.</p></li>
</ul>
<p>DeepSeek-R1 main technical approaches improve over Zero by using high-quality <strong>cold start</strong> data and a <strong>multi-stage training</strong> pipeline.</p>
<ul class="simple">
<li><p>The cold start data consists of a small amount (thousands) of long and high-quality (structred, human friendly) CoT data to fine-tune the model as the initial RL actor.</p></li>
<li><p>The multi-stage training pipeline is shown in <a class="reference internal" href="#chapter-training-fig-reasoning-deepseek-r1-workflow"><span class="std std-numref">Fig. 17.6</span></a>, which includes</p>
<ul>
<li><p>A supervised fine-tuning stage on cold-start data to get to DeepSeek-R0.25</p></li>
<li><p>A subsequent reasoning-oriented RL to get to DeepSeek-R0.5</p></li>
<li><p>Additional SFT and RL process on a combined data consisting of</p>
<ul>
<li><p>New CoT SFT data collected from DeepSeek-R0.5 via rejection sampling</p></li>
<li><p>Existing supervised training data from DeepSeek V3 to enhancing non-reasoning tasks</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In the final RL for all scenarios, reward signals are coming from</p>
<ul class="simple">
<li><p>Reasoning tasks rewards adopt the rule-based reward in DeepSeek-R1-Zero.</p></li>
<li><p>Non-reasoning tasks rewards focus on aligning with human preferences, like helpfulness, harmless, and safety.</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-reasoning-deepseek-r1-workflow">
<a class="reference internal image-reference" href="../../_images/deepseek_r1_workflow.png"><img alt="../../_images/deepseek_r1_workflow.png" src="../../_images/deepseek_r1_workflow.png" style="width: 718.2px; height: 490.05px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.6 </span><span class="caption-text">Summary of DeepSeek-R1 multi-stage training, starting from DeepSeek-V3.</span><a class="headerlink" href="#chapter-training-fig-reasoning-deepseek-r1-workflow" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The motivation and technical details on each training stage is summarized as follows</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Resulted Model</p></th>
<th class="head text-left"><p>Motivation &amp; Technical Details</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>SFT on cold start CoT data</p></td>
<td class="text-left"><p>DeepSeek-R0.25</p></td>
<td class="text-left"><p>Prevent early instability in RL training; Incorporate human reasoning priors; Improve output readability</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Pure RL</p></td>
<td class="text-left"><p>DeepSeek-R0.5</p></td>
<td class="text-left"><p>Enhance reasoning ability; Add additional language consistency reward</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Large scale SFT + RL</p></td>
<td class="text-left"><p>DeepSeek-R1</p></td>
<td class="text-left"><p>Enhance reasoning and general capability; Use ~600k reasoning + ~200k general training samples; Use RL to enhance helpfulness and harmless</p></td>
</tr>
</tbody>
</table>
</div>
<p>As shown in <a class="reference internal" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-r1-benchmark"><span class="std std-numref">Fig. 17.7</span></a>, DeepSeek-R1 achieves comparable performance to OpenAI-o1-1217 on reasoning tasks.</p>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-deepseek-r1-benchmark">
<a class="reference internal image-reference" href="../../_images/deepseek_r1_benchmark.png"><img alt="../../_images/deepseek_r1_benchmark.png" src="../../_images/deepseek_r1_benchmark.png" style="width: 815.25px; height: 488.25px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17.7 </span><span class="caption-text">Benchmark performanc of DeepSeek-R1 in reasoning oriented tasks. Image from <span id="id9">[<a class="reference internal" href="#id1618" title="Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and others. Deepseek-r1: incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.">GYZ+25</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-deepseek-r1-benchmark" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="deepseek-r1-distillation">
<h3><span class="section-number">17.4.3. </span>DeepSeek-R1-Distillation<a class="headerlink" href="#deepseek-r1-distillation" title="Link to this heading">#</a></h3>
<p>DeepSeek team explored a straightforward distillation approach:</p>
<ul class="simple">
<li><p>Using the reasoning data generated by DeepSeek-R1</p></li>
<li><p>Only SFT is used and does not include an RL stage,</p></li>
</ul>
<p>As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeekR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform nonreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32BPreview on all evaluation metrics,</p>
<p>DeepSeek team also conducted examperiment to answer the question if one can directly apply large-scale RL training to a student model to achieve performance comparable with distillation method?</p>
<p>The results and implications from the following table:</p>
<ul class="simple">
<li><p>Distilling more powerful models into smaller ones gives much better results than its counterparts trained by large-scale RL, despite the latter requires enormous computational power.</p></li>
<li><p>Large-scale RL still need efficiency improvement on computation and samples to achieve on-par results with distillation.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p>AIME 2024</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p>MATH-500</p></th>
<th class="head text-center"><p>GPQA Diamond</p></th>
<th class="head text-center"><p>LiveCodeBench</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>cons&#64;64</p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>pass&#64;1</p></td>
<td class="text-center"><p>pass&#64;1</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>QwQ-32B-Preview</p></td>
<td class="text-center"><p>50.0</p></td>
<td class="text-center"><p>60.0</p></td>
<td class="text-center"><p>90.6</p></td>
<td class="text-center"><p>54.5</p></td>
<td class="text-center"><p>41.9</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>DeepSeek-R1-Zero-Qwen-32B</p></td>
<td class="text-center"><p>47.0</p></td>
<td class="text-center"><p>60.0</p></td>
<td class="text-center"><p>91.6</p></td>
<td class="text-center"><p>55.0</p></td>
<td class="text-center"><p>40.2</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>DeepSeek-R1-Distill-Qwen-32B</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 2 . 6}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 3 . 3}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{9 4 . 3}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{6 2 . 1}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{5 7 . 2}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">17.5. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<section id="software">
<h3><span class="section-number">17.5.1. </span>Software<a class="headerlink" href="#software" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/unslothai/unsloth">Unsloth</a></p>
<div class="docutils container" id="id10">
<div role="list" class="citation-list">
<div class="citation" id="id1620" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">BJT+22</a><span class="fn-bracket">]</span></span>
<p>Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. <em>arXiv preprint arXiv:2207.14255</em>, 2022.</p>
</div>
<div class="citation" id="id1618" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GYZ+25<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and others. Deepseek-r1: incentivizing reasoning capability in llms via reinforcement learning. <em>arXiv preprint arXiv:2501.12948</em>, 2025.</p>
</div>
<div class="citation" id="id1619" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GZY+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, and others. Deepseek-coder: when the large language model meets programming–the rise of code intelligence. <em>arXiv preprint arXiv:2401.14196</em>, 2024.</p>
</div>
<div class="citation" id="id1621" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">LAZ+23</a><span class="fn-bracket">]</span></span>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, and others. Starcoder: may the source be with you! <em>arXiv preprint arXiv:2305.06161</em>, 2023.</p>
</div>
<div class="citation" id="id1614" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SWZ+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and others. Deepseekmath: pushing the limits of mathematical reasoning in open language models. <em>arXiv preprint arXiv:2402.03300</em>, 2024.</p>
</div>
<div class="citation" id="id1622" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">ZGS+24</a><span class="fn-bracket">]</span></span>
<p>Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, and others. Deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence. <em>arXiv preprint arXiv:2406.11931</em>, 2024.</p>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_LLM_case_study"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_training/reinforcement_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16. </span>*Reinforcement Learning Essentials</p>
      </div>
    </a>
    <a class="right-next"
       href="llama_series.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">18. </span>Llama Series</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-v3">17.1. DeepSeek V3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">17.1.1. Pretraining</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-coder">17.2. DeepSeek Coder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-curation">17.2.1. Data Curation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-strategy">17.2.2. PreTraining Strategy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#long-context-extension">17.2.3. Long Context Extension</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intruction-tuning">17.2.4. Intruction Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">17.2.5. Reinforcement Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">17.2.6. Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-math">17.3. DeepSeek Math</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">17.3.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pipeline">17.3.2. Data Pipeline</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">17.3.3. Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-sft-and-rl">17.3.4. Post-Training: SFT and RL</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-reasoning-models">17.4. DeepSeek Reasoning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1-zero">17.4.1. DeepSeek-R1-Zero</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1">17.4.2. DeepSeek-R1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-r1-distillation">17.4.3. DeepSeek-R1-Distillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">17.5. Bibliography</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">17.5.1. Software</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>