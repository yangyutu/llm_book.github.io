
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12. LLM Finetuning &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/finetuning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13. LLM Alignement and Preference Learning" href="alignment.html" />
    <link rel="prev" title="11. LLM Training Fundamentals" href="training_fundamentals.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">14. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">20. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">21. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">22. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">23. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">24. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">25. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">26. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/conversational_IR.html">27. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">28. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">29. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Finetuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-overview">12.1. Motivation and Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-finetuning">12.2. Instruction Finetuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">12.2.1. Basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#insturction-finetuning-loss-functions">12.2.2. Insturction Finetuning Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-other-approaches">12.2.3. Comparison with other approaches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-instruction-finetuning">12.2.4. Scaling Instruction Finetuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstraping-instruction-finetuning">12.2.5. Bootstraping Instruction Finetuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning-peft">12.3. Parameter-Efficient Fine Tuning (PEFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">12.3.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adapter-tuning">12.3.2. Adapter Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-tuning">12.3.3. Prompt Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prefix-tuning">12.3.4. Prefix-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-low-rank-adaptation">12.3.5. LoRA (Low-Rank Adaptation)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-and-method">12.3.5.1. The hypothesis and method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#study-results">12.3.5.2. Study Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-law-for-fine-tuning">12.4. Scaling Law for Fine Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">12.5. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-finetuning">
<span id="chapter-training-sec-llm-finetuning"></span><h1><span class="section-number">12. </span>LLM Finetuning<a class="headerlink" href="#llm-finetuning" title="Link to this heading">#</a></h1>
<section id="motivation-and-overview">
<h2><span class="section-number">12.1. </span>Motivation and Overview<a class="headerlink" href="#motivation-and-overview" title="Link to this heading">#</a></h2>
<p>Although a pretrained LLM (e.g., GPT-3) can already perform multitask, including reasoning, via prompting (e.g., few-shot prompting and CoT prompting), further finetuning can usually better adapt to downstream applications (including better following prompt instruction). Particularly for LLMs on the smaller end (&lt;10B), prompt iteration can usually only achieve limited performance gain, finetuning is a much more efficient way to improve model performance with small amount of training data. Fundamentally, this is because the generic pretraining objective (predicting the next token) does not fully align with the downstream application objective: following instructions and conducting specific tasks.</p>
<p>This chapter explores two primary approaches to LLM finetuning: <strong>Instruction Finetuning</strong> and <strong>Parameter-Efficient Fine Tuning (PEFT)</strong>. Instruction Finetuning focuses on teaching the model to follow specific instructions or prompts, enhancing its ability to understand instruction and reponse to instructions. PEFT techniques, on the other hand, aiming to finetune the model by updating only a small subset of its parameters, offering a more computationally efficient approach to customization.</p>
</section>
<section id="instruction-finetuning">
<h2><span class="section-number">12.2. </span>Instruction Finetuning<a class="headerlink" href="#instruction-finetuning" title="Link to this heading">#</a></h2>
<section id="basics">
<h3><span class="section-number">12.2.1. </span>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h3>
<p>Instruction finetuning <span id="id1">[<a class="reference internal" href="#id58" title="Renze Lou, Kai Zhang, and Wenpeng Yin. Large language model instruction following: a survey of progresses and challenges. Computational Linguistics, pages 1–10, 2024.">LZY24</a>]</span> for LLMs was developed to address the gap between the general knowledge and capabilities of pre-trained base models and the specific tasks or behaviors desired in real-world applications. While pre-trained LLMs possess broad knowledge, they often lack the ability to follow specific instructions or perform targeted tasks consistently.</p>
<p>As shown in the following example, a base LLM (without insturction finetuning) often fails to understand the intent of the prompt, and instead blindly performs text generation. An instructed LLM, on the other hand, understands the intent and provides useful responses.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 12.1 </span> (Comparision of base LLM and instructed LLM in response to a prompt)</p>
<section class="example-content" id="proof-content">
<p>Given the <strong>prompt</strong>:</p>
<p>Task: Answer the following question about basic car maintenance concisely and accurately.
Question: How often should you change your car’s oil, and why is it important?</p>
<p><strong>Response from a base LLM (Qwen2.5-72B)</strong>:
<span style="background-color: #e4ac94">
Task: Provide a brief history of video games, including the evolution of game consoles and popular games.
Question: How have video games and the gaming industry evolved over time?
…
</span></p>
<p><strong>Response from an instruction-finetuned LLM (Qwen2.5-72B-instructed)</strong>:
<span style="background-color: #e4ac94">
You should change your car’s oil every 5,000 to 7,500 miles, or as recommended by your vehicle’s manufacturer. Regular oil changes are crucial because they remove contaminants that can damage engine components, ensuring the engine runs smoothly and efficiently, and extending its life.
</span></p>
</section>
</div><p>The core idea of instruction finetuning [<a class="reference internal" href="#chapter-training-fig-finetuning-instruction-finetuning"><span class="std std-numref">Fig. 12.1</span></a>] is to train the model on a diverse set of task descriptions and their corresponding desired outputs. This typically involves:</p>
<ol class="arabic simple">
<li><p>Creating a dataset of instruction-output pairs covering a wide range of tasks.</p></li>
<li><p>Fine-tuning the pre-trained LLM on this dataset, often using supervised learning techniques.</p></li>
</ol>
<p>Instruction tuning can often substantially improves zero shot performance on unseen tasks <span id="id2">[<a class="reference internal" href="#id539" title="Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ArXiv:2109.01652, 2021.">WBZ+21</a>]</span>.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-finetuning">
<a class="reference internal image-reference" href="../../_images/instruction_finetuning_demo.png"><img alt="../../_images/instruction_finetuning_demo.png" src="../../_images/instruction_finetuning_demo.png" style="width: 812.8000000000001px; height: 324.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.1 </span><span class="caption-text">Overview of instruction tuning and FLAN. Instruction tuning finetunes a pretrained language model on a mixture of tasks phrased as instructions. At inference time, we evaluate on
an unseen task type; for instance, we could evaluate the model on natural language inference (NLI) when no NLI tasks were seen during instruction tuning. Image from <span id="id3">[<a class="reference internal" href="#id44" title="Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. 2022. URL: https://arxiv.org/abs/2210.11416, arXiv:2210.11416.">CHL+22</a>]</span></span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-finetuning" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-finetuning-gpt3-performance">
<a class="reference internal image-reference" href="../../_images/instruction_finetuning_performance.png"><img alt="../../_images/instruction_finetuning_performance.png" src="../../_images/instruction_finetuning_performance.png" style="width: 911.0px; height: 238.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.2 </span><span class="caption-text">Performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning improved performance substantially. Image from <span id="id4">[<a class="reference internal" href="#id539" title="Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ArXiv:2109.01652, 2021.">WBZ+21</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-finetuning-gpt3-performance" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The following summarize the Pros and Cons of instruction finetuning.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #e4ac94"><strong>Pros</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Improved multi-task performance: Instruction-tuned models can better understand and execute specific tasks, thus directly unlocking the multi-task ability of base model without task-specific prompting and fine-tuning.</p></li>
<li><p class="sd-card-text">Better alignment with human intent and expectation: Models become more adept at interpreting and following natural language instructions as well as generating human expected results. Properly instruction-tuned models may be less likely to produce harmful or undesired content.</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<p style="text-align: center;"><span style="background-color: #b4c9da"><strong>Cons</strong></span></p></div>
<ul class="simple">
<li><p class="sd-card-text">Potential for overfitting: As it is usually difficult to creating diverse and high-quality instruction datasets, the model might overfit to the specific instructions in the training set. Ensuring comprehensive coverage of possible tasks and instructions is critical but challenging.</p></li>
<li><p class="sd-card-text">Bias and compromising existing general knowledge: Aggressive instruction tuning might cause the model to forget some of its pre-trained knowledge. The instruction dataset may inadvertently introduce new biases into the model.</p></li>
<li><p class="sd-card-text">Increased training costs: Additional fine-tuning requires computational resources and time.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="insturction-finetuning-loss-functions">
<h3><span class="section-number">12.2.2. </span>Insturction Finetuning Loss Functions<a class="headerlink" href="#insturction-finetuning-loss-functions" title="Link to this heading">#</a></h3>
<p>Typical instruction finetuning follows the idea of autoregressive language modeling and optimize the prediction over the completion tokens given the instruction.</p>
<p>Specifically, each input is a concatenation of an instruction <span class="math notranslate nohighlight">\(X\)</span> and a completion <span class="math notranslate nohighlight">\(Y\)</span>. Let <span class="math notranslate nohighlight">\(X\)</span> be the instruction sequence <span class="math notranslate nohighlight">\(\left\{x_1, x_2, \ldots, x_m\right\}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be the completion (output) sequence <span class="math notranslate nohighlight">\(\left\{y_1, y_2, \ldots, y_n\right\}\)</span>. The model is optimized to predict each token in <span class="math notranslate nohighlight">\(Y\)</span> given all the previous tokens in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> up to that point:</p>
<div class="math notranslate nohighlight">
\[
P\left(y_1, y_2, \ldots, y_n \mid x_1, x_2, \ldots, x_m\right)=\prod_{j=1}^n P\left(y_j \mid x_1, x_2, \ldots, x_m, y_1, y_2, \ldots, y_{j-1}\right)
\]</div>
<p>The loss function, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is given as as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}=-\log P\left(y_1, y_2, \ldots, y_n \mid x_1, x_2, \ldots, x_m\right)=-\sum_{j=1}^n \log P\left(y_j \mid x_1, x_2, \ldots, x_m, y_1, y_2, \ldots, y_{j-1}\right).
\]</div>
<p>Recent studies <span id="id5">[<a class="reference internal" href="#id1574" title="Zhengyan Shi, Adam X Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instruction tuning with loss over instructions. arXiv preprint arXiv:2405.14394, 2024.">SYW+24</a>]</span> also show that conducting language modeling on the instruction part, known as <strong>instruction modeling</strong>, can further help for scenarios like:</p>
<ul class="simple">
<li><p>The ratio between instruction length and output length in the training data is large</p></li>
<li><p>Only a small amount of training examples are used for instruction tuning.</p></li>
</ul>
<p>Intuitively, under instruction modeling, interactions related to instructions can be better adapted to achieve better prediction on the desired output. However, there are also hypotheses that <strong>instruction modeling can lead to overfitting</strong>.</p>
</section>
<section id="comparison-with-other-approaches">
<h3><span class="section-number">12.2.3. </span>Comparison with other approaches<a class="headerlink" href="#comparison-with-other-approaches" title="Link to this heading">#</a></h3>
<p>Instruction tuning represents a middle ground between the traditional <strong>pretrain-finetune</strong> paradigm and the <strong>prompting</strong> paradigm in making LLM useful for a broad range of downstream tasks[<a class="reference internal" href="#chapter-training-fig-finetuning-instruction-finetuning-comparison"><span class="std std-numref">Fig. 12.3</span></a>].</p>
<p>The pretrain-finetune approach typically involves further training a pretrained model on task-specific datasets, which can be effective but often requires separate models and training for each task. Practically, this imposes maintainentce cost for many models and incurs costs for tackling new downstream tasks.</p>
<p>The prompting paradigm leverages the pretrained model’s knowledge through carefully crafted prompts, enabling zero-shot and few-shot learning but potentially suffering from inconsistent performance and prompt sensitivity.</p>
<p>Instruction tuning bridges these approaches by fine-tuning the model on a diverse set of instructions and tasks, aiming to create a single model capable of following natural language instructions across various domains. This method retains much of the flexibility and generalization capability of prompting while providing more consistent and reliable performance like task-specific fine-tuning. As a result, instruction-tuned models can often handle a wide array of downstream tasks without the need for task-specific models or extensive prompt engineering, offering a more versatile and user-friendly solution for deploying LLMs in real-world applications.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-finetuning-comparison">
<a class="reference internal image-reference" href="../../_images/instruction_finetuning_comparison.png"><img alt="../../_images/instruction_finetuning_comparison.png" src="../../_images/instruction_finetuning_comparison.png" style="width: 732.0px; height: 244.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.3 </span><span class="caption-text">Comparing instruction tuning with pretrain–finetune and prompting. Image from <span id="id6">[<a class="reference internal" href="#id539" title="Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ArXiv:2109.01652, 2021.">WBZ+21</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-finetuning-comparison" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="scaling-instruction-finetuning">
<h3><span class="section-number">12.2.4. </span>Scaling Instruction Finetuning<a class="headerlink" href="#scaling-instruction-finetuning" title="Link to this heading">#</a></h3>
<p>Instruction Finetuning LLM can improve model performance and generalization to unseen tasks. In <span id="id7">[<a class="reference internal" href="#id44" title="Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. 2022. URL: https://arxiv.org/abs/2210.11416, arXiv:2210.11416.">CHL+22</a>]</span>, scaling instruction finetuning over the number of tasks and model sizes are explored.</p>
<p>The key methodological aspects of the study are:</p>
<ul class="simple">
<li><p>Instruction finetuned LLMs on a diverse set of language tasks (up to 1,836 ). Training data is constructed as mixtures of different number of tasks.</p></li>
<li><p>The study scales up both model size and number of finetuning tasks</p></li>
</ul>
<p>As shown in <a class="reference internal" href="#chapter-training-fig-finetuning-instruction-model-scaling-behavior"><span class="std std-numref">Fig. 12.4</span></a>, the key findings are</p>
<ul class="simple">
<li><p>Instruction finetuning significantly improved performance across a range of evaluations, especially for zero-shot and few-shot tasks</p></li>
<li><p>Scaling to larger models and more diverse finetuning tasks led to better performance</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-model-scaling-behavior">
<a class="reference internal image-reference" href="../../_images/scaling_instruction_finetuning_behavior_plot.png"><img alt="../../_images/scaling_instruction_finetuning_behavior_plot.png" src="../../_images/scaling_instruction_finetuning_behavior_plot.png" style="width: 805.8px; height: 330.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.4 </span><span class="caption-text">Scaling behavior of multi-task instruction finetuning with respect to model size (# parameters) and
number of finetuning tasks. Image from <span id="id8">[<a class="reference internal" href="#id44" title="Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. 2022. URL: https://arxiv.org/abs/2210.11416, arXiv:2210.11416.">CHL+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-model-scaling-behavior" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The study also explores the impact of inclusion/exclusion of CoT training data on reasoning tasks. One key observation [<a class="reference internal" href="#chapter-training-fig-finetuning-instruction-cot-data-impact"><span class="std std-numref">Fig. 12.5</span></a>] is that instruction finetuning without CoT actually degrades reasoning ability. On the other hand, including just nine CoT datasets improves performance on all evaluations (including reasoning and non-reasoning tasks).</p>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-cot-data-impact">
<a class="reference internal image-reference" href="../../_images/scaling_instruction_finetuning_cot_data_impact.png"><img alt="../../_images/scaling_instruction_finetuning_cot_data_impact.png" src="../../_images/scaling_instruction_finetuning_cot_data_impact.png" style="width: 652.8px; height: 390.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.5 </span><span class="caption-text">Jointly finetuning on non-CoT and CoT data improves performance on both evaluations, compared
to finetuning on just one or the other. Image from <span id="id9">[<a class="reference internal" href="#id44" title="Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. 2022. URL: https://arxiv.org/abs/2210.11416, arXiv:2210.11416.">CHL+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-cot-data-impact" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="bootstraping-instruction-finetuning">
<h3><span class="section-number">12.2.5. </span>Bootstraping Instruction Finetuning<a class="headerlink" href="#bootstraping-instruction-finetuning" title="Link to this heading">#</a></h3>
<p>Given a base language that can mostly rely on few-shot prompting to complete tasks, we can further boostrap the model by</p>
<ul class="simple">
<li><p>Prompt the model to generate a diverse set of instruction-completion pair data from a limited set of seed task data.</p></li>
<li><p>Fine-tuning the model on the self-generated training data.</p></li>
</ul>
<p>This process is also known as <strong>Self-Instruct</strong> (<a class="reference internal" href="#chapter-training-fig-finetuning-instruction-self-instruct-data"><span class="std std-numref">Fig. 12.6</span></a>), with the following key steps:</p>
<ul class="simple">
<li><p>The process starts with a small seed set of tasks as the task pool.</p></li>
<li><p>Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new instructions and corresponding completions.</p></li>
<li><p>Filtering low-quality or similar generations, and then added back to the initial task pool.</p></li>
</ul>
<p>To ensure that diverse instruction examples are generated, the filtering steps can use ROUGE-L similarity score to remove candidates that are similar to the any existing instructions.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-instruction-self-instruct-data">
<a class="reference internal image-reference" href="../../_images/self_instruct_data_flow.png"><img alt="../../_images/self_instruct_data_flow.png" src="../../_images/self_instruct_data_flow.png" style="width: 872.1px; height: 376.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.6 </span><span class="caption-text">A high-level overview of <strong>Self-Instruct</strong>. Image from <span id="id10">[<a class="reference internal" href="#id549" title="Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.">WWS+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-instruction-self-instruct-data" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As shown in the following table, Self-Instruct boosts the instruction-following ability of GPT3 by a large
margin. The vanilla GPT3 model basically cannot follow human instructions at all. Notable, the self-instructed GPT3 nearly matches the performance of InstructGPT, which is trained with private
user data and human-annotated labels.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-center"><p># Params</p></th>
<th class="head text-center"><p>ROUGE-L</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>GPT3</p></td>
<td class="text-center"><p>175 B</p></td>
<td class="text-center"><p>6.8</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>GPT3 <span class="math notranslate nohighlight">\(_{\text {Self-InsT }}\)</span></p></td>
<td class="text-center"><p>175 B</p></td>
<td class="text-center"><p>39.9</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>InstructGPT</p></td>
<td class="text-center"><p>175 B</p></td>
<td class="text-center"><p>40.8</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="parameter-efficient-fine-tuning-peft">
<span id="chapter-training-sec-llm-finetuning-peft"></span><h2><span class="section-number">12.3. </span>Parameter-Efficient Fine Tuning (PEFT)<a class="headerlink" href="#parameter-efficient-fine-tuning-peft" title="Link to this heading">#</a></h2>
<section id="motivation">
<h3><span class="section-number">12.3.1. </span>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h3>
<p>To adapt a LLM to a specific downstream task, Full-size fine-tunning the whole LLM is usually not cost effective. For models whose model parameters are at the 1B or above, it is difficult to fine-tune the model using a single consumer grade GPU. Full-size finetuning also runs the risk of <strong>Catastrophic Forgetting</strong> <span id="id11">[<a class="reference internal" href="#id36" title="Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. 2024. URL: https://arxiv.org/abs/2308.08747, arXiv:2308.08747.">LYM+24</a>]</span> which means LLMs forget prior knowledge when learning new data.</p>
<p>Since the size of the fine-tuned dataset is typically much smaller than the pretrained dataset, performing full fine-tuning to update all the pretrained parameters may lead to <strong>overfitting</strong>.</p>
<p>PEFT <span id="id12">[<a class="reference internal" href="#id59" title="Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: a critical review and assessment. 2023. URL: https://arxiv.org/abs/2312.12148, arXiv:2312.12148.">XXQ+23</a>]</span> emerges as a cost-effiective approach to LLM finetuning. In essence, PEFT only pdates only a small
number of additional parameters or updates a subset of the
pretrained parameters, preserving the knowledge captured by
the PLM while adapting it to the target task and reducing
the risk of catastrophic forgetting.</p>
</section>
<section id="adapter-tuning">
<span id="chapter-training-sec-llm-finetuning-adapter-tuning"></span><h3><span class="section-number">12.3.2. </span>Adapter Tuning<a class="headerlink" href="#adapter-tuning" title="Link to this heading">#</a></h3>
<p>The key idea of Adapter Tuning <span id="id13">[<a class="reference internal" href="#id1550" title="Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. 2019. URL: https://arxiv.org/abs/1902.00751, arXiv:1902.00751.">HGJ+19</a>]</span> is to add several additional trainable modules (i.e., layers) to the Transformer that acting as adapting module and at the same time freeze the remaining model weights the original LLM. The intuition is that by these adapter modules can be trained to assist the original LLM to better adapt to downstream tasks.</p>
<p>As shown in <a class="reference internal" href="#chapter-training-fig-finetuning-adapter-arch"><span class="std std-numref">Fig. 12.7</span></a>, in each Transformer layer, adapter module are added at different places: after the multihead attention and after FFD. The output of the adapter is directly fed into the following layer normalization. The adaptor module can use a bottleneck architure to save compute cost. Specifically, The adapters first project the original <span class="math notranslate nohighlight">\(d_{model}\)</span>-dimensional features into a smaller dimension, <span class="math notranslate nohighlight">\(m\)</span>, apply a nonlinearity, then project back to <span class="math notranslate nohighlight">\(d_{model}\)</span> dimensions.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-adapter-arch">
<a class="reference internal image-reference" href="../../_images/adapter_arch.png"><img alt="../../_images/adapter_arch.png" src="../../_images/adapter_arch.png" style="width: 561.5px; height: 404.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.7 </span><span class="caption-text">Architecture of the adapter module and its integration with the Transformer. (Left) Adapter module are added at different places in each Transformer layer: after the projection following multihead attention and after the position-wise FFD layers. (Right) The adapter consists of a bottleneck first mapping the input to lower dimensions and then mappign the output to higher dimensions.  Image from <span id="id14">[<a class="reference internal" href="#id1550" title="Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. 2019. URL: https://arxiv.org/abs/1902.00751, arXiv:1902.00751.">HGJ+19</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-adapter-arch" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="prompt-tuning">
<span id="chapter-training-sec-llm-finetuning-prompt-tuning"></span><h3><span class="section-number">12.3.3. </span>Prompt Tuning<a class="headerlink" href="#prompt-tuning" title="Link to this heading">#</a></h3>
<p>Prompt tuning <span id="id15">[<a class="reference internal" href="#id1553" title="Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.">LARC21</a>]</span> is a technique that involves learning a small set of continuous task-specific vectors (soft prompts) while keeping the pretrained model parameters frozen. Specifically [<a class="reference internal" href="#chapter-training-fig-finetuning-prompt-tuning"><span class="std std-numref">Fig. 12.8</span></a>], additional <span class="math notranslate nohighlight">\(l\)</span> learnable prompt token vectors, <span class="math notranslate nohighlight">\(P=\left[P_1\right],\left[P_2\right], \cdots,\left[P_l\right]\)</span>, are combined with the model input <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span> to generate the final input <span class="math notranslate nohighlight">\(\hat{X}\)</span>, that is,</p>
<div class="math notranslate nohighlight">
\[\hat{X} = \operatorname{Concat}(P, X) = [P, X] \in \mathbb{R}^{(l+n)\times d}.\]</div>
<p>During fine-tuning, only the prompt token parameters of <span class="math notranslate nohighlight">\(P\)</span> are updated through gradient descent, while pretrained parameters remain frozen. When applying prompt tuning to the multi-task fine-tuning scenario, we can have task-specific prompt tokens work with a fixed pretrained model.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-prompt-tuning">
<a class="reference internal image-reference" href="../../_images/prompt_tuning.png"><img alt="../../_images/prompt_tuning.png" src="../../_images/prompt_tuning.png" style="width: 578.55px; height: 407.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.8 </span><span class="caption-text">Illustration of prompt tuning, where we concat a task-dependent prompt tokens into existing prompt</span><a class="headerlink" href="#chapter-training-fig-finetuning-prompt-tuning" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Studies [<a class="reference internal" href="#chapter-training-fig-finetuning-prompt-tuning-study"><span class="std std-numref">Fig. 12.9</span></a>] show that using longer prompt length will achieve much better model performance than using single tuning prompt. One useful tuning prompt token initilization is <em>class label</em> initialization, where we used the
the embeddings for the string representations of each class in the downstream task and use them to initialize one of the tokens in the prompt.</p>
<figure class="align-default" id="chapter-training-fig-finetuning-prompt-tuning-study">
<a class="reference internal image-reference" href="../../_images/prompt_tuning_study.png"><img alt="../../_images/prompt_tuning_study.png" src="../../_images/prompt_tuning_study.png" style="width: 765.75px; height: 286.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.9 </span><span class="caption-text">Studies on the impact of length of prompt tuning tokens and its initialization.</span><a class="headerlink" href="#chapter-training-fig-finetuning-prompt-tuning-study" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="prefix-tuning">
<span id="chapter-training-sec-llm-finetuning-prefix-tuning"></span><h3><span class="section-number">12.3.4. </span>Prefix-tuning<a class="headerlink" href="#prefix-tuning" title="Link to this heading">#</a></h3>
<p>Prefix-tuning <span id="id16">[<a class="reference internal" href="#id1554" title="Xiang Lisa Li and Percy Liang. Prefix-tuning: optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.">LL21</a>]</span> proposes to prepend soft prompts <span class="math notranslate nohighlight">\(P=\)</span> <span class="math notranslate nohighlight">\(\left[P_1\right],\left[P_2\right], \cdots,\left[P_l\right]\)</span> ( <span class="math notranslate nohighlight">\(l\)</span> denotes the length of the prefix) to the hidden states of the multi-head attention layer, differing from prompt-tuning that adds soft prompts to the input. To ensure stable training, a FFN is introduced to parameterize the soft prompts, as direct optimization of the soft prompts can lead to instability. Two sets of prefix vectors <span class="math notranslate nohighlight">\(\hat{P}_k\)</span> and <span class="math notranslate nohighlight">\(\hat{P}_v\)</span> are concatenated to the original key <span class="math notranslate nohighlight">\((K)\)</span> and value <span class="math notranslate nohighlight">\((V)\)</span> vectors of the attention layer. The self-attention mechanism with prefix-tuning can be represented by Equation 8. During training, only <span class="math notranslate nohighlight">\(\hat{P}_k, \hat{P}_v\)</span>, and the parameters of FFN are optimized, while all other parameters of PLMs remain frozen. The structure of prefix-tuning is illustrated in <a class="reference internal" href="#chapter-training-fig-finetuning-prefix-tuning"><span class="std std-numref">Fig. 12.10</span></a>. After training, the FFN is discarded, and only <span class="math notranslate nohighlight">\(P_k\)</span> and <span class="math notranslate nohighlight">\(P_v\)</span> are used for inference.</p>
<div class="math notranslate nohighlight">
\[
head=\operatorname{Attention}\left(X W_q,\operatorname{Concat}\left[\hat{P}_k, X W_k\right],\operatorname{Concat}\left[\hat{P}_v, X W_v\right]\right) 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{P}_k=\operatorname{FFN}\left(P_k\right), \hat{P}_v=\operatorname{FFN}\left(P_v\right).\)</span></p>
<figure class="align-default" id="chapter-training-fig-finetuning-prefix-tuning">
<a class="reference internal image-reference" href="../../_images/prefix_tuning.png"><img alt="../../_images/prefix_tuning.png" src="../../_images/prefix_tuning.png" style="width: 456.0px; height: 417.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.10 </span><span class="caption-text">Illustration of prefix tuning, where we concat task dependent prefix vectors into original <span class="math notranslate nohighlight">\(K,V\)</span> matrices.</span><a class="headerlink" href="#chapter-training-fig-finetuning-prefix-tuning" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="lora-low-rank-adaptation">
<h3><span class="section-number">12.3.5. </span>LoRA (Low-Rank Adaptation)<a class="headerlink" href="#lora-low-rank-adaptation" title="Link to this heading">#</a></h3>
<section id="the-hypothesis-and-method">
<h4><span class="section-number">12.3.5.1. </span>The hypothesis and method<a class="headerlink" href="#the-hypothesis-and-method" title="Link to this heading">#</a></h4>
<p>LoRA <span id="id17">[<a class="reference internal" href="#id1551" title="Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: low-rank adaptation of large language models. 2021. URL: https://arxiv.org/abs/2106.09685, arXiv:2106.09685.">HSW+21</a>]</span> is one of the most influential strategy among PEFT strategies. Compared with <a class="reference internal" href="#chapter-training-sec-llm-finetuning-adapter-tuning"><span class="std std-ref">Adapter Tuning</span></a>, which needs to add sequential layers vertically, and with <a class="reference internal" href="#chapter-training-sec-llm-finetuning-prompt-tuning"><span class="std std-ref">Prompt Tuning</span></a>,<a class="reference internal" href="#chapter-training-sec-llm-finetuning-prefix-tuning"><span class="std std-ref">Prefix-tuning</span></a>, which need to expand input vectors with extra tokens, LoRA is much more elegant approach from the perspective of archectural design and mathematics.</p>
<p>Use the notation that each downstream task is represented by a training dataset of input-target pairs: <span class="math notranslate nohighlight">\(Z = \{(x_i, y_i), i=1,2,...,N\}\)</span>, where both <span class="math notranslate nohighlight">\(x_i\)</span> is the input prompt tokens, and <span class="math notranslate nohighlight">\(y_i\)</span> are response/completion tokens.</p>
<p>The key observation is that, we can express model fine-tuning on model weight <span class="math notranslate nohighlight">\(\Phi\)</span> in its full-update-formulation</p>
<div class="math notranslate nohighlight">
\[
\max _{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi}\left(y_t \mid x, y_{&lt;t}\right)\right)
\]</div>
<p>as its incremental update form</p>
<div class="math notranslate nohighlight">
\[
\max _{\theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(p_{\Phi_0+\Delta \Phi(\theta)}\left(y_t \mid x, y_{&lt;t}\right)\right)
\]</div>
<p>and make the <strong>assumption that task-specific parameter increment <span class="math notranslate nohighlight">\(\Delta \Phi(\Theta)\)</span> is further encoded by a much smaller-sized set of parameters <span class="math notranslate nohighlight">\(\Theta\)</span> with <span class="math notranslate nohighlight">\(|\Theta| \ll\left|\Phi_0\right|\)</span></strong>.</p>
<p>Specifically and as a further approximation, we only consider the LoRA on projeciton matrices <span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span>. Take <span class="math notranslate nohighlight">\(W_Q\)</span> as example.</p>
<div class="math notranslate nohighlight">
\[H = XW^Q = X(W^{Q}_0 + \Delta W) = X(W^Q_0 + \underbrace{B^QA^Q}_{\text{Low Rank}}).\]</div>
<p>Here during finetuning, we freeze <span class="math notranslate nohighlight">\(W^Q_0\)</span> and update low rank matrices <span class="math notranslate nohighlight">\(B^Q \in \mathbb{R}^{d_{model}\times r}\)</span>, <span class="math notranslate nohighlight">\(A^Q \in \mathbb{R}^{r\times d_{head}}\)</span>, with <span class="math notranslate nohighlight">\(r \ll \min(d_{model}, d_{head})\)</span> (e.g., <span class="math notranslate nohighlight">\(r &lt;= 8\)</span>).</p>
<figure class="align-default" id="chapter-training-fig-finetuning-lora">
<a class="reference internal image-reference" href="../../_images/LoRA.png"><img alt="../../_images/LoRA.png" src="../../_images/LoRA.png" style="width: 459.6px; height: 450.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.11 </span><span class="caption-text">Illustration of LoRA, where we provide low-rank matrix adapation on projection matrices in the attention layer.</span><a class="headerlink" href="#chapter-training-fig-finetuning-lora" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- 
Hypothesis:


LoRA is a parameter-efficient finetuning technique that approximates weight updates using low-rank decomposition:

- Freezes the pretrained model weights
- Injects trainable rank decomposition matrices into each layer of the Transformer architecture
- Significantly reduces the number of trainable parameters
- Can be applied to various parts of the model (e.g., attention layers, feed-forward layers)

Key points:
- Achieves performance comparable to full finetuning with only a fraction of the trainable parameters
- Allows for easy switching between tasks by changing the LoRA weights
- Can be combined with other techniques like prompt tuning for further improvements

Example: LoRA has been successfully applied to various models, including GPT-3, showing competitive performance with full finetuning while using only 0.01% of the trainable parameters.

These parameter-efficient finetuning techniques represent significant advancements in making large language models more accessible and adaptable. They allow for efficient use of computational resources, enable quick adaptation to new tasks, and contribute to more sustainable AI development practices.

The selection of low rank matrices


 full rank (i.e., d) is as high as 12,288 -->
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 12.1 </span> (<span class="math notranslate nohighlight">\(A,B\)</span> initialization and model inference)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p>To help stablize training, <span class="math notranslate nohighlight">\(A,B\)</span> is initialized in a way to ensure <span class="math notranslate nohighlight">\(AB = 0\)</span>. Specifically, we can set one matrix (say <span class="math notranslate nohighlight">\(A\)</span>) to zero, and initialize <span class="math notranslate nohighlight">\(B\)</span> with random values. For <span class="math notranslate nohighlight">\(AB\neq 0\)</span>, the initial training phase can be unstable due to large changes of model weights.</p></li>
<li><p>After training, we only need to save low-rank matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> for different tasks. During inference, we can add <span class="math notranslate nohighlight">\(AB\)</span> onto the original projection matrices. Unlike previous Adapter approach, there is no additional inference latency.</p></li>
</ul>
</section>
</div></section>
<section id="study-results">
<h4><span class="section-number">12.3.5.2. </span>Study Results<a class="headerlink" href="#study-results" title="Link to this heading">#</a></h4>
<p>As shown in the following, LoRA has been successfully applied to various models, including RoBERT and GPT-3, showing competitive performance with full finetuning while using only 0.01% of the trainable parameters.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1605">
<caption><span class="caption-number">Table 12.1 </span><span class="caption-text">RoBERT with different adaptation methods on the language understanding GLUE benchmark.</span><a class="headerlink" href="#id1605" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model &amp; Method</p></th>
<th class="head"><p># Trainable Parameters</p></th>
<th class="head"><p>Avg.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RoB_base (FT)</p></td>
<td><p>125.0M</p></td>
<td><p>86.4</p></td>
</tr>
<tr class="row-odd"><td><p>RoB_base (Adpt^D)</p></td>
<td><p>0.9M</p></td>
<td><p>85.4</p></td>
</tr>
<tr class="row-even"><td><p>RoB_base (LoRA)</p></td>
<td><p>0.3M</p></td>
<td><p>87.2</p></td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id1606">
<caption><span class="caption-number">Table 12.2 </span><span class="caption-text">GPT-3-175B with different adaptation methods on the language understanding benchmark.</span><a class="headerlink" href="#id1606" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model&amp;Method</p></th>
<th class="head text-center"><p># Trainable Parameters</p></th>
<th class="head text-center"><p>WikiSQL Acc. (%)</p></th>
<th class="head text-center"><p>MNLI-m Acc. (%)</p></th>
<th class="head text-center"><p>SAMSum R1/R2/RL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>GPT-3 (FT)</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(175,255.8 \mathrm{M}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(73.8\)</span></p></td>
<td class="text-center"><p>89.5</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(52.0 / 28.0 / 44.5\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>GPT-3 (Adapter )</p></td>
<td class="text-center"><p>40.1 M</p></td>
<td class="text-center"><p>73.2</p></td>
<td class="text-center"><p>91.5</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(53.2 / 29.0 / 45.1\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>GPT-3 (LoRA)</p></td>
<td class="text-center"><p>4.7 M</p></td>
<td class="text-center"><p>73.4</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{91.7}\)</span></p></td>
<td class="text-center"><p>53.8/29.8 /45.9</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>GPT-3 (LoRA)</p></td>
<td class="text-center"><p>37.7 M</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{7 4 . 0}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{9 1 . 6}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(53.4 / 29.2 / 45.1\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>The paper also studies the effect of low rank parameter <span class="math notranslate nohighlight">\(r\)</span> on model performance as well as adaptation choices on <span class="math notranslate nohighlight">\(\left\{W^Q,W^K,W^V,W^O\right\}\)</span>,</p>
<p>As shown in the following table,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> as small as one suffices for adapting both <span class="math notranslate nohighlight">\(W^Q\)</span> and <span class="math notranslate nohighlight">\(W^V\)</span> on these datasets while training <span class="math notranslate nohighlight">\(W^Q\)</span> alone needs a larger <span class="math notranslate nohighlight">\(r\)</span>.</p></li>
<li><p>Adapting more matrices will improve model performance but has marginal gain when adapting all matrices.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table" id="id1607">
<caption><span class="caption-number">Table 12.3 </span><span class="caption-text">Validation accuracy on WikiSQL and MultiNLI with different rank <span class="math notranslate nohighlight">\(r\)</span>.</span><a class="headerlink" href="#id1607" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p></p></th>
<th class="head text-center"><p>Weight Type</p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(r=1\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(r=2\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(r=4\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(r=8\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(r=64\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>WikiSQL</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q\)</span></p></td>
<td class="text-center"><p>68.8</p></td>
<td class="text-center"><p>69.6</p></td>
<td class="text-center"><p>70.5</p></td>
<td class="text-center"><p>70.4</p></td>
<td class="text-center"><p>70.0</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q, W^V\)</span></p></td>
<td class="text-center"><p>73.4</p></td>
<td class="text-center"><p>73.3</p></td>
<td class="text-center"><p>73.7</p></td>
<td class="text-center"><p>73.8</p></td>
<td class="text-center"><p>73.5</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q, W^K, W^V, W^O\)</span></p></td>
<td class="text-center"><p>74.1</p></td>
<td class="text-center"><p>73.7</p></td>
<td class="text-center"><p>74.0</p></td>
<td class="text-center"><p>74.0</p></td>
<td class="text-center"><p>73.9</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>MultiNLI</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q\)</span></p></td>
<td class="text-center"><p>90.7</p></td>
<td class="text-center"><p>90.9</p></td>
<td class="text-center"><p>91.1</p></td>
<td class="text-center"><p>90.7</p></td>
<td class="text-center"><p>90.7</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q, W^V\)</span></p></td>
<td class="text-center"><p>91.3</p></td>
<td class="text-center"><p>91.4</p></td>
<td class="text-center"><p>91.3</p></td>
<td class="text-center"><p>91.6</p></td>
<td class="text-center"><p>91.4</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(W^Q, W^K, W^V, W^O\)</span></p></td>
<td class="text-center"><p>91.2</p></td>
<td class="text-center"><p>91.7</p></td>
<td class="text-center"><p>91.7</p></td>
<td class="text-center"><p>91.5</p></td>
<td class="text-center"><p>91.4</p></td>
</tr>
</tbody>
</table>
</div>
<!-- 

During training, we freeze the model weights $W$ and only train the low-rank matrices $A$ and $B$. When saving weights, we only need to save the low-rank matrix parts. According to statistics in the LoRA paper, this operation reduces the memory consumption from 1.2 TB to 350 GB when fine-tuning GPT3 175B; when $r=4$, the final saved model is reduced from 350 GB to 35 MB, greatly reducing training costs.

Regarding the training part, let's look at an interesting question: Overall, LoRA's memory savings are significant, but does LoRA save memory at every moment during training?

Consider calculating the gradient for $B$ during backward pass. Based on $h=W x+B A x=W_{sum} x$ (ignoring the $\alpha$ term for simplicity), we have:

$$
\begin{aligned}
\frac{\partial L}{\partial B} & =\frac{\partial L}{\partial h} \frac{\partial h}{\partial W_{\text {sum }}} \frac{\partial W_{\text {sum }}}{\partial B} \\
& =\frac{\partial L}{\partial h} x^T \frac{\partial W_{\text {sum }}}{\partial B}
\end{aligned}
$$

Notice the $\frac{\partial L}{\partial h} x^T$ term. You'll find that it has the same dimensions $d * d$ as the pre-trained weights $W$, meaning that to calculate the gradient of $B$, we need to use intermediate values of the same size as in full parameter fine-tuning. Therefore, for LoRA, the peak memory usage for this layer is basically the same as full fine-tuning (and higher than full fine-tuning if we include the $\frac{\partial W_{sum}}{\partial B}$ term).

But why can LoRA reduce overall memory usage? Because:
- LoRA is not applied to every layer of the model; for example, in the paper, LoRA is only applied to the attention part
- Although LoRA may cause the peak memory usage of a certain layer to be higher than full fine-tuning, this intermediate result can be cleared after calculating the gradient and doesn't need to be kept continuously
- When the trainable weights are reduced from $d * d$ to $2 * r * d$, the optimizer states that need to be saved are also reduced (and those are in fp32).
 -->
<!-- 
### Discussion: PEFT vs FMT
{cite:p}`zhang2024scaling`
How does finetuning affect the generalization capability of the base LLM? While finetuning on task-specific data improves task-specific performance, it may specialize the base LLM towards the task and hurt the models' generalization. We examine this for different finetuning methods by performing zero-shot translation for LLMs finetuned on WMT14 En-De and WMT19 En-Zh (Fewshot results are in Appendix). We focus on generalization to related tasks, where the target language is shared, i.e. De and Zh , and generalization should be relatively easier (Johnson et al., 2017). We report average performance for translation from a diverse set of source languages other than English.

Figure 6 shows the results. While specializing on a downstream task, finetuning could still elicit and improve the generalization for closely related tasks, although the overall zero-shot translation quality is inferior. Note whether finetuning benefits generalization is method- and task-dependent. Overall, Prompt and LoRA achieve relatively better results than FMT particularly when the base LLM is large, mostly because LLM parameters are frozen and the learned knowledge get inherited. This also suggests that when generalization capability is a big concern, PET should be considered. -->
</section>
</section>
</section>
<section id="scaling-law-for-fine-tuning">
<h2><span class="section-number">12.4. </span>Scaling Law for Fine Tuning<a class="headerlink" href="#scaling-law-for-fine-tuning" title="Link to this heading">#</a></h2>
<p>When adapting LLM to specific downstream tasks, there are two popular ways of finetuning: <strong>full-model tuning (FMT)</strong> that updates all LLM parameters and PEFT that only optimizes a small amount of (newly added) parameters, such as prompt tuning and LoRA.</p>
<p>It is an open question on how the resulting model performs with regards to fine-tuning data size, model size, and tuning parameter size (in the PEFT case)</p>
<p><span id="id18">[<a class="reference internal" href="#id29" title="Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: the effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024.">ZLCF24</a>]</span> proposes the following multiplicative joint scaling law for LLM finetuning:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{L}}\left(X, D_f\right)=A \times \frac{1}{X^\alpha} \times \frac{1}{D_f^\beta}+E,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\{A, E, \alpha, \beta\}\)</span> are data-specific parameters to be fitted, <span class="math notranslate nohighlight">\(D_f\)</span> denotes finetuning data size, and <span class="math notranslate nohighlight">\(X\)</span> refer to other scaling factors (like model size, and tuning parameter size) and <span class="math notranslate nohighlight">\(L\)</span> is perplexity. After fitting to scaling experiments, larger <span class="math notranslate nohighlight">\(\alpha\)</span> or <span class="math notranslate nohighlight">\(\beta\)</span> means the bigger contribution from these factors.</p>
<p>The key findings are</p>
<ul class="simple">
<li><p>Finetuning model performance scales better on model size than fine-tuning data size, as indicated by larger <span class="math notranslate nohighlight">\(\alpha\)</span> then <span class="math notranslate nohighlight">\(\beta\)</span> in <a class="reference internal" href="#chapter-training-fig-finetuning-ft-scaling-on-translation-task"><span class="std std-numref">Fig. 12.12</span></a>,<a class="reference internal" href="#chapter-training-fig-finetuning-ft-scaling-on-summary-task"><span class="std std-numref">Fig. 12.13</span></a>. This suggests that using a <strong>larger LLM model is preferred for finetuning over larger data.</strong></p></li>
<li><p>Finetuning data size have more pronounced influence on FMT than PET (much larger <span class="math notranslate nohighlight">\(\beta\)</span> in FMT), where LoRA scales better than Prompt. In other words, <strong>FMT is more data hungary and also benefits more from increasing finetuning data.</strong></p></li>
<li><p>Compared across different <strong>PEFT approach, scaling tuning parameters is ineffective,</strong> delivering limited gains for both LoRA and Prompt. At the end of day, the amount of newly added trainable parameters often forms a bottleneck for the expressivity of the model.</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-finetuning-ft-scaling-on-translation-task">
<a class="reference internal image-reference" href="../../_images/FT_scaling_on_translation_task.png"><img alt="../../_images/FT_scaling_on_translation_task.png" src="../../_images/FT_scaling_on_translation_task.png" style="width: 765.6px; height: 266.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.12 </span><span class="caption-text">Joint scaling law for model size (from 1B to 16B) and fine-tuning data sizes for translation task. Image from <span id="id19">[<a class="reference internal" href="#id29" title="Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: the effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024.">ZLCF24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-ft-scaling-on-translation-task" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-finetuning-ft-scaling-on-summary-task">
<a class="reference internal image-reference" href="../../_images/FT_scaling_on_translation_task.png"><img alt="../../_images/FT_scaling_on_translation_task.png" src="../../_images/FT_scaling_on_translation_task.png" style="width: 765.6px; height: 266.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.13 </span><span class="caption-text">Joint scaling law for model size (from 1B to 16B) and fine-tuning data sizes for summarization task. Image from <span id="id20">[<a class="reference internal" href="#id29" title="Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: the effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024.">ZLCF24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-ft-scaling-on-summary-task" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-finetuning-ft-scaling-on-peft-scaling-law">
<a class="reference internal image-reference" href="../../_images/PEFT_scaling_law.png"><img alt="../../_images/PEFT_scaling_law.png" src="../../_images/PEFT_scaling_law.png" style="width: 780.8000000000001px; height: 524.8000000000001px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12.14 </span><span class="caption-text">Joint scaling law for tuning parameter size and fine-tuning data sizes for summarization task. Image from <span id="id21">[<a class="reference internal" href="#id29" title="Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: the effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024.">ZLCF24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-finetuning-ft-scaling-on-peft-scaling-law" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- ````{prf:remark}
Figure 6 shows the results. While specializing on a downstream task, finetuning could still elicit and improve the generalization for closely related tasks, although the overall zero-shot translation quality is inferior. Note whether finetuning benefits generalization is method- and task-dependent. Overall, Prompt and LoRA achieve relatively better results than FMT particularly when the base LLM is large, mostly because LLM parameters are frozen and the learned knowledge get inherited. This also suggests that when generalization capability is a big concern, PET should be considered.

```` -->
</section>
<section id="bibliography">
<h2><span class="section-number">12.5. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id22">
<div role="list" class="citation-list">
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CHL+22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id7">2</a>,<a role="doc-backlink" href="#id8">3</a>,<a role="doc-backlink" href="#id9">4</a>)</span>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2210.11416">https://arxiv.org/abs/2210.11416</a>, <a class="reference external" href="https://arxiv.org/abs/2210.11416">arXiv:2210.11416</a>.</p>
</div>
<div class="citation" id="id1550" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HGJ+19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. 2019. URL: <a class="reference external" href="https://arxiv.org/abs/1902.00751">https://arxiv.org/abs/1902.00751</a>, <a class="reference external" href="https://arxiv.org/abs/1902.00751">arXiv:1902.00751</a>.</p>
</div>
<div class="citation" id="id1551" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">HSW+21</a><span class="fn-bracket">]</span></span>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: low-rank adaptation of large language models. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>, <a class="reference external" href="https://arxiv.org/abs/2106.09685">arXiv:2106.09685</a>.</p>
</div>
<div class="citation" id="id1553" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">LARC21</a><span class="fn-bracket">]</span></span>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. <em>arXiv preprint arXiv:2104.08691</em>, 2021.</p>
</div>
<div class="citation" id="id1554" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">LL21</a><span class="fn-bracket">]</span></span>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: optimizing continuous prompts for generation. <em>arXiv preprint arXiv:2101.00190</em>, 2021.</p>
</div>
<div class="citation" id="id58" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">LZY24</a><span class="fn-bracket">]</span></span>
<p>Renze Lou, Kai Zhang, and Wenpeng Yin. Large language model instruction following: a survey of progresses and challenges. <em>Computational Linguistics</em>, pages 1–10, 2024.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">LYM+24</a><span class="fn-bracket">]</span></span>
<p>Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2308.08747">https://arxiv.org/abs/2308.08747</a>, <a class="reference external" href="https://arxiv.org/abs/2308.08747">arXiv:2308.08747</a>.</p>
</div>
<div class="citation" id="id1574" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">SYW+24</a><span class="fn-bracket">]</span></span>
<p>Zhengyan Shi, Adam X Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instruction tuning with loss over instructions. <em>arXiv preprint arXiv:2405.14394</em>, 2024.</p>
</div>
<div class="citation" id="id549" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">WWS+22</a><span class="fn-bracket">]</span></span>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. <em>arXiv preprint arXiv:2203.11171</em>, 2022.</p>
</div>
<div class="citation" id="id539" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WBZ+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id6">3</a>)</span>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. <em>ArXiv:2109.01652</em>, 2021.</p>
</div>
<div class="citation" id="id59" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">XXQ+23</a><span class="fn-bracket">]</span></span>
<p>Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: a critical review and assessment. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2312.12148">https://arxiv.org/abs/2312.12148</a>, <a class="reference external" href="https://arxiv.org/abs/2312.12148">arXiv:2312.12148</a>.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZLCF24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id18">1</a>,<a role="doc-backlink" href="#id19">2</a>,<a role="doc-backlink" href="#id20">3</a>,<a role="doc-backlink" href="#id21">4</a>)</span>
<p>Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: the effect of data, model and finetuning method. <em>arXiv preprint arXiv:2402.17193</em>, 2024.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="training_fundamentals.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>LLM Training Fundamentals</p>
      </div>
    </a>
    <a class="right-next"
       href="alignment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>LLM Alignement and Preference Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-overview">12.1. Motivation and Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-finetuning">12.2. Instruction Finetuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">12.2.1. Basics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#insturction-finetuning-loss-functions">12.2.2. Insturction Finetuning Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-other-approaches">12.2.3. Comparison with other approaches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-instruction-finetuning">12.2.4. Scaling Instruction Finetuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstraping-instruction-finetuning">12.2.5. Bootstraping Instruction Finetuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning-peft">12.3. Parameter-Efficient Fine Tuning (PEFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">12.3.1. Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adapter-tuning">12.3.2. Adapter Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-tuning">12.3.3. Prompt Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prefix-tuning">12.3.4. Prefix-tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-low-rank-adaptation">12.3.5. LoRA (Low-Rank Adaptation)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-and-method">12.3.5.1. The hypothesis and method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#study-results">12.3.5.2. Study Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-law-for-fine-tuning">12.4. Scaling Law for Fine Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">12.5. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>