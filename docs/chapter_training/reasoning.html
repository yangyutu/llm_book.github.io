
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14. LLM Reasoning &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/reasoning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="15. LLM Training Acceleration (WIP)" href="accelerated_training.html" />
    <link rel="prev" title="13. LLM Alignement and Preference Learning" href="alignment.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. LLM Reasoning</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerated_training.html">15. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">16. *Reinforcement Learning Essentials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/deepseek_series.html">17. DeepSeek Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_case_study/llama_series.html">18. Llama Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">19. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">20. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">21. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">22. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_fundamentals.html">23. Text Embedding Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_text_embedding/text_embedding_LLM.html">24. LLM Text Embedding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/multimodality_fundamentals.html">25. Multimodality fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">26. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part1.html">27. Information Retrieval and Sparse Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals_part2.html">28. Information Retrieval and Dense Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">29. Application of LLM in IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/conversational_IR.html">30. Conversational IR (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">31. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">32. Advanced RAG (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Reasoning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">14.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-benchmarks">14.2. Reasoning Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-benchmarks">14.2.1. Math benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coding-benchmark">14.2.2. Coding benchmark</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-knowledge-and-logical-thinking-benchmark">14.2.3. Language, knowledge, and logical thinking benchmark</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-time-scaling-strategies">14.3. Inference-Time Scaling Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-and-search">14.3.1. Sampling and Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proposal-distribution-refinement">14.3.2. Proposal Distribution Refinement</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">14.4. Bibliography</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">14.4.1. Software</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-reasoning">
<h1><span class="section-number">14. </span>LLM Reasoning<a class="headerlink" href="#llm-reasoning" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">14.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p><strong>Reasoning</strong> is the process in which</p>
<ul class="simple">
<li><p>a complex problem is decomposed into a sequence of smaller logical steps</p></li>
<li><p>the correct execution of these logical steps leading to final soution of the problem.
In the context of LLM reasoning, it refers to the process in which LLM is generating intermediate chain of thought steps that lead to the final answer.</p></li>
</ul>
<p>When we ask a question to LLM, the nature of question largely determines if a reasoning is required. For example [<a class="reference internal" href="#chapter-training-fig-reasoning-simple-reasoning-vs-complex-reasoning"><span class="std std-numref">Fig. 14.1</span></a>],</p>
<ul class="simple">
<li><p><em>What is the captial of China</em> is a <strong>factual question</strong> does not require reasoning.</p></li>
<li><p><em>A man walks at a speed of 3 mph and how har has he walked for 2 hours?</em> is a <strong>simple reasoning problem</strong>.</p></li>
<li><p><em>Prove there are infinitely many prime numbers</em> is <strong>complex reasoning problem</strong>. Puzzles, riddles, coding, and math tasks are typically falling into the category of complex reasoning problems.</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-reasoning-simple-reasoning-vs-complex-reasoning">
<a class="reference internal image-reference" href="../../_images/simple_reasoning_vs_complex_reasoning.png"><img alt="../../_images/simple_reasoning_vs_complex_reasoning.png" src="../../_images/simple_reasoning_vs_complex_reasoning.png" style="width: 1737.45px; height: 725.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.1 </span><span class="caption-text">Comparison between simple reasoning task (left) and complex reasoning task (right).</span><a class="headerlink" href="#chapter-training-fig-reasoning-simple-reasoning-vs-complex-reasoning" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The following is a summary on simple reasoning and complex reasoning tasks from different aspects.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Aspects</p></th>
<th class="head text-left"><p>Simple reasoning</p></th>
<th class="head text-left"><p>Complex Reasoning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Problem Complexity</p></td>
<td class="text-left"><p>Low</p></td>
<td class="text-left"><p>High</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Reasoning Steps</p></td>
<td class="text-left"><p>Few, linear</p></td>
<td class="text-left"><p>Many, linear and nonlinear, requiring planning and abstraction</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Domain Knowledge</p></td>
<td class="text-left"><p>Basic or general</p></td>
<td class="text-left"><p>Often requires specialized knowledge</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Example Problems</p></td>
<td class="text-left"><p>Arithmetic, factual QA, simple logic</p></td>
<td class="text-left"><p>Multi-step math, coding, and logical; plannning, optimizaiton</p></td>
</tr>
</tbody>
</table>
</div>
<p>Most well-trained LLMs are capable of basic reasoning skill that can be triggered by CoT prompting; but they often fall short on complex reasoning benchmark. This chapter is about understanding, measuring, and developping complex reasoning skill for LLMs.</p>
<p>The importance of complex reasoning for LLM are:</p>
<ul class="simple">
<li><p><strong>Better Real-World Use</strong> – Strong reasoning helps LLMs solve complex problems in fields like law, medicine, and math, making them more useful.</p></li>
<li><p><strong>Fixing Weaknesses</strong> – LLMs struggle with logical thinking and complex tasking in out-of-domain situations. Improving reasoning makes can potentially improve their generalization to new domains.</p></li>
<li><p><strong>Step Toward AGI</strong> – Reasoning is key to human intelligence. Enhancing it in LLMs moves us closer to Artificial General Intelligence (AGI).</p></li>
</ul>
</section>
<section id="reasoning-benchmarks">
<h2><span class="section-number">14.2. </span>Reasoning Benchmarks<a class="headerlink" href="#reasoning-benchmarks" title="Link to this heading">#</a></h2>
<p>Large Language Models (LLMs) are evaluated using a variety of benchmarks designed to assess their reasoning capabilities across multiple domains.hese benchmarks provide standardized tasks that test models on aspects such as logical reasoning, mathematical problem-solving, and domain-specific knowledge.ere are some of the most prominent reasoning benchmarks:</p>
<section id="math-benchmarks">
<h3><span class="section-number">14.2.1. </span>Math benchmarks<a class="headerlink" href="#math-benchmarks" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>GSM8K (Grade School Math)</strong> - This benchmark consists of 8,500 linguistically diverse elementary school math word problems that require two to eight basic arithmetic operations to solve <span id="id1">[<a class="reference internal" href="#id1531" title="Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and others. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.">CKB+21</a>]</span>.</p></li>
<li><p><strong>MathEval</strong> - <a class="reference external" href="https://github.com/math-eval/MathEval">MathEval</a> is a comprehensive benchmark (~20k problems) that contains 20 other benchmarks, such as GSM8K, MATH, and the math subsection of MMLU. It aims to evaluate the model’s math skill from elementary school math to high school competitions.</p></li>
<li><p><strong>FrontierMath</strong> - This benchmark contains questions from areas of modern math that are difficult for professional mathematicians to solve <span id="id2">[<a class="reference internal" href="#id1608" title="Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, and others. Frontiermath: a benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024.">GEB+24</a>]</span>.</p></li>
</ul>
</section>
<section id="coding-benchmark">
<h3><span class="section-number">14.2.2. </span>Coding benchmark<a class="headerlink" href="#coding-benchmark" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>HumanEval</strong> - This benchmark consists of programming problems where the solution is always a Python function, often just a few lines long <span id="id3">[<a class="reference internal" href="training_fundamentals.html#id47" title="Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and others. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.">CTJ+21</a>]</span>. This assesses a model’s basic ability to generate correct and functional code based on problem descriptions.</p></li>
<li><p><strong>SWE-Bench</strong> - The SWE-Bench comprises 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories <span id="id4">[<a class="reference internal" href="#id1609" title="Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.">JYW+23</a>]</span>.</p></li>
</ul>
</section>
<section id="language-knowledge-and-logical-thinking-benchmark">
<h3><span class="section-number">14.2.3. </span>Language, knowledge, and logical thinking benchmark<a class="headerlink" href="#language-knowledge-and-logical-thinking-benchmark" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>MMLU (Measuring Massive Multitask Language Understanding)</strong> - MMLU consists of approximately 16,000 multiple-choice questions spanning 57 academic subjects, including mathematics, philosophy, law, and medicine <span id="id5">[<a class="reference internal" href="#id1610" title="Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.">HBB+20</a>]</span>. It is widely used to assess a model’s breadth of knowledge and reasoning across diverse fields.</p></li>
<li><p><strong>GPQA (Google-Proof Q&amp;A)</strong> - 448 multiple-choice questions written by domain experts in biology, physics, and chemistry, and requires PhD-level experts to solve.<span id="id6">[<a class="reference internal" href="#id1611" title="David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: a graduate-level google-proof q&amp;a benchmark. arXiv preprint arXiv:2311.12022, 2023.">RHS+23</a>]</span>. It is designed to evaluate a model’s proficiency in solving PhD-level complex problems across different domains.</p></li>
<li><p><strong>AGIEval</strong> - This includes questions from 20 official, public, and high-standard admission and qualification exams, such as the SAT, Gaokao, law school admission tests, math competitions, lawyer qualification tests, and national civil service exams <span id="id7">[<a class="reference internal" href="#id1612" title="Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: a human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.">ZCG+23</a>]</span>. It evaluates a model’s performance on standardized tests that require advanced reasoning skills.</p></li>
<li><p><strong>ARC-AGI (Abstraction and Reasoning Corpus for Artificial General Intelligence)</strong> - <a class="reference external" href="https://arcprize.org/">RC-AGI</a> is designed to assess a model’s abstract reasoning capabilities.t involves tasks that require pattern recognition and logical inference.</p></li>
</ul>
</section>
</section>
<section id="inference-time-scaling-strategies">
<h2><span class="section-number">14.3. </span>Inference-Time Scaling Strategies<a class="headerlink" href="#inference-time-scaling-strategies" title="Link to this heading">#</a></h2>
<p>Inference-time scaling refers to using more compute resources during inference, instead of training stage, to improve LLM’s ability in solving complex tasks.</p>
<p>Different ways for scaling test-time computing:</p>
<ul class="simple">
<li><p>Best-of-N sampling - sampling <span class="math notranslate nohighlight">\(N\)</span> outputs in parallel from a base LLM and take the best one (by a learned verifier or a reward model) as the final output.</p></li>
<li><p>Self-critique prompting - asking the model to self-critique its response and revise its response iteratively.</p></li>
<li><p>Guided search - using a process-based verifier to guide the LLM</p></li>
</ul>
<p>Key findings from <span id="id8">[<a class="reference internal" href="#id1613" title="Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.">SLXK24</a>]</span></p>
<ul class="simple">
<li><p>For easy and intermediate questions, it is more effective to pretrain smaller models with less compute and then apply test-time compute to improve model outputs.</p></li>
<li><p>With the most challenging questions, there is very little benfits from scaling-up test time compute for a smaller model; instead, it is more effective to make progress by scaling up pretraining compute .</p></li>
</ul>
<section id="sampling-and-search">
<h3><span class="section-number">14.3.1. </span>Sampling and Search<a class="headerlink" href="#sampling-and-search" title="Link to this heading">#</a></h3>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-search-method">
<a class="reference internal image-reference" href="../../_images/inference_time_search_method.png"><img alt="../../_images/inference_time_search_method.png" src="../../_images/inference_time_search_method.png" style="width: 924.95px; height: 455.65000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.2 </span><span class="caption-text">Comparing different PRM search methods. Left: Best-of-N samples N full answers and then selects the best answer according to the PRM final score. Center: Beam search samples N candidates at each step, and selects the top M according to the PRM to continue the search from. Right: lookahead-search extends each step in beam-search to utilize a k-step lookahead while assessing which steps to retain and continue the search from. Thus lookahead-search needs more compute. Image from <span id="id9">[<a class="reference internal" href="#id1613" title="Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.">SLXK24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-search-method" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Best-of-N weighted</strong>. We sample N answers independently from the base LLM and then select the best answer according to the PRM’s final answer judgement.</p>
<p><strong>Beam search</strong>. Beam search optimizes the PRM by searching over its per-step predictions. Our implementation is similar to BFS-V <span class="math notranslate nohighlight">\([10,48]\)</span>. Concretely, we consider a fixed number of beams <span class="math notranslate nohighlight">\(N\)</span> and a beam width <span class="math notranslate nohighlight">\(M\)</span>. We then run the following steps:</p>
<ol class="arabic simple">
<li><p>sample <span class="math notranslate nohighlight">\(N\)</span> initial predictions for the first step in the solution</p></li>
<li><p>score the generated steps according to the PRM’s predicted step-wise reward-to-go estimate (which also corresponds to the total reward from the prefix since the reward is sparse in this setting)</p></li>
<li><p>filter for only the top <span class="math notranslate nohighlight">\(\frac{N}{M}\)</span> highest scoring steps</p></li>
<li><p>now from each candidate, sample <span class="math notranslate nohighlight">\(M\)</span> proposals from the next step, resulting in a total of <span class="math notranslate nohighlight">\(N / M \times M\)</span> candidate prefixes again. Then repeat steps 2-4 again.</p></li>
</ol>
<p><strong>Lookahead search.</strong> Lookahead search modifies how beam search evaluates individual steps. It uses lookahead rollouts to improve the accuracy of the PRM’s value estimation in each step of the search process. Specifically,</p>
<ul class="simple">
<li><p>at each step in the beam search, rather than using the PRM score at the current step to select the top candidates, lookahead search performs a simulation,</p></li>
<li><p>Simulation involves rolling out up to <span class="math notranslate nohighlight">\(k\)</span> steps further while stopping early if the end of solution is reached. (To minimize variance in the simulation rollout, we perform rollouts using temperature 0.)</p></li>
<li><p>The PRM’s prediction at the end of this rollout is then used to score the current step in the beam search.</p></li>
</ul>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 14.1 </span> (Lookahead search vs beam search vs MCTS)</p>
<section class="remark-content" id="proof-content">
<p>That is, in other words, we can view beam search as a special case of lookahead search with <span class="math notranslate nohighlight">\(k=0\)</span>. Given an accurate PRM, increasing <span class="math notranslate nohighlight">\(k\)</span> should improve the accuracy of the per-step value estimates at the cost of additional compute.</p>
<p>Also note that this version of lookahead search is a special case of MCTS (Monte carlo Tree Search), wherein the stochastic elements of MCTS, designed to facilitate exploration, are removed since the PRM is already trained and is frozen. These stochastic elements are largely useful for learning the value function (which we’ve already learned with our PRM), but less useful at test-time when we want to exploit rather than explore. Therefore, lookahead search is largely representative of how MCTS-style methods would be applied at test-time.</p>
</section>
</div><p>Key findings:</p>
<ul class="simple">
<li><p>On the easy questions, the verifier will make mostly correct assessments of correctness. Therefore, by applying beam search guided by PRM, we might overfit to  any spurious features learned by the verifier, causing performance degredation.</p></li>
<li><p>On the more difficult questions, the base model is much less likely to sample the correct answer in the first place, so beam search can serve to help guide the model towards producing the correct answer more often.</p></li>
<li><p>With a given inference time budget, beam-search is more effective on harder questions
and at lower compute budgets, whereas best-of-N is more effective on easier questions and at higher budgets (i.e., large N).</p></li>
</ul>
</section>
<section id="proposal-distribution-refinement">
<h3><span class="section-number">14.3.2. </span>Proposal Distribution Refinement<a class="headerlink" href="#proposal-distribution-refinement" title="Link to this heading">#</a></h3>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-parallel-sampling-vs-sequential-revision">
<a class="reference internal image-reference" href="../../_images/parallel_sampling_vs_sequential_revision.png"><img alt="../../_images/parallel_sampling_vs_sequential_revision.png" src="../../_images/parallel_sampling_vs_sequential_revision.png" style="width: 633.0px; height: 480.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.3 </span><span class="caption-text">Parallel sampling generates N answers independently in parallel, whereas sequential revisions generates each one in sequence conditioned on previous attempts. Image from <span id="id10">[<a class="reference internal" href="#id1613" title="Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.">SLXK24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-parallel-sampling-vs-sequential-revision" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-reasoning-inference-time-method-revision-model-with-verifier">
<a class="reference internal image-reference" href="../../_images/revision_model_with_verifier.png"><img alt="../../_images/revision_model_with_verifier.png" src="../../_images/revision_model_with_verifier.png" style="width: 675.0px; height: 518.25px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.4 </span><span class="caption-text">In both the sequential and parallel cases, we can use the verifier to determine the best-of-N answers (e.g. by applying best-of-N
weighted). We can also allocate some of our budget to parallel and some to sequential, effectively enabling a combination of the
two sampling strategies. In this case, we use the verifier to first select the best answer within each sequential chain and then
select the best answer accross chains. Image from <span id="id11">[<a class="reference internal" href="#id1613" title="Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.">SLXK24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-reasoning-inference-time-method-revision-model-with-verifier" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">14.4. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<section id="software">
<h3><span class="section-number">14.4.1. </span>Software<a class="headerlink" href="#software" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/unslothai/unsloth">Unsloth</a></p>
<div class="docutils container" id="id12">
<div role="list" class="citation-list">
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">CTJ+21</a><span class="fn-bracket">]</span></span>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and others. Evaluating large language models trained on code. <em>arXiv preprint arXiv:2107.03374</em>, 2021.</p>
</div>
<div class="citation" id="id1531" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">CKB+21</a><span class="fn-bracket">]</span></span>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and others. Training verifiers to solve math word problems. <em>arXiv preprint arXiv:2110.14168</em>, 2021.</p>
</div>
<div class="citation" id="id1608" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">GEB+24</a><span class="fn-bracket">]</span></span>
<p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, and others. Frontiermath: a benchmark for evaluating advanced mathematical reasoning in ai. <em>arXiv preprint arXiv:2411.04872</em>, 2024.</p>
</div>
<div class="citation" id="id1610" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">HBB+20</a><span class="fn-bracket">]</span></span>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. <em>arXiv preprint arXiv:2009.03300</em>, 2020.</p>
</div>
<div class="citation" id="id1609" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">JYW+23</a><span class="fn-bracket">]</span></span>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: can language models resolve real-world github issues? <em>arXiv preprint arXiv:2310.06770</em>, 2023.</p>
</div>
<div class="citation" id="id1611" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">RHS+23</a><span class="fn-bracket">]</span></span>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: a graduate-level google-proof q&amp;a benchmark. <em>arXiv preprint arXiv:2311.12022</em>, 2023.</p>
</div>
<div class="citation" id="id1613" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SLXK24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id9">2</a>,<a role="doc-backlink" href="#id10">3</a>,<a role="doc-backlink" href="#id11">4</a>)</span>
<p>Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. <em>arXiv preprint arXiv:2408.03314</em>, 2024.</p>
</div>
<div class="citation" id="id1612" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">ZCG+23</a><span class="fn-bracket">]</span></span>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: a human-centric benchmark for evaluating foundation models. <em>arXiv preprint arXiv:2304.06364</em>, 2023.</p>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="alignment.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>LLM Alignement and Preference Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="accelerated_training.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>LLM Training Acceleration (WIP)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">14.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-benchmarks">14.2. Reasoning Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-benchmarks">14.2.1. Math benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coding-benchmark">14.2.2. Coding benchmark</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-knowledge-and-logical-thinking-benchmark">14.2.3. Language, knowledge, and logical thinking benchmark</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-time-scaling-strategies">14.3. Inference-Time Scaling Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-and-search">14.3.1. Sampling and Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proposal-distribution-refinement">14.3.2. Proposal Distribution Refinement</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">14.4. Bibliography</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">14.4.1. Software</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>