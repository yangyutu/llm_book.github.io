
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14. *Reinforcement Learning Essentials &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/reinforcement_learning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="15. LLM Training Acceleration" href="accelerated_training.html" />
    <link rel="prev" title="13. LLM Alignement and Preference Learning" href="alignment.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignment.html">13. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerated_training.html">15. LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">20. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">21. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">22. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>*Reinforcement Learning Essentials</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-framework">14.1. Reinforcement learning framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notations">14.1.1. Notations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">14.1.2. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-state-mdp">14.1.3. Finite-state MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-action-value-function-q-function">14.1.4. State-action Value function (<span class="math notranslate nohighlight">\(Q\)</span> function)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-and-value-iteration">14.1.5. Policy iteration and Value iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">14.1.5.1. Policy iteration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">14.1.5.2. Value iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="reinforcement-learning-essentials">
<span id="ch-reinforcement-learning"></span><h1><span class="section-number">14. </span>*Reinforcement Learning Essentials<a class="headerlink" href="#reinforcement-learning-essentials" title="Link to this heading">#</a></h1>
<section id="reinforcement-learning-framework">
<h2><span class="section-number">14.1. </span>Reinforcement learning framework<a class="headerlink" href="#reinforcement-learning-framework" title="Link to this heading">#</a></h2>
<section id="notations">
<h3><span class="section-number">14.1.1. </span>Notations<a class="headerlink" href="#notations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_t, s_t\)</span> :state at time <span class="math notranslate nohighlight">\(t\)</span>; <span class="math notranslate nohighlight">\(S_t\)</span> denotes random variable; <span class="math notranslate nohighlight">\(s_t \)</span> denotes one realization.</p></li>
<li><p><span class="math notranslate nohighlight">\(A_t, a_t\)</span> :action at time <span class="math notranslate nohighlight">\(t\)</span>; <span class="math notranslate nohighlight">\(A_t\)</span> denotes random variable; <span class="math notranslate nohighlight">\(a_t \)</span> denotes one realization.</p></li>
<li><p><span class="math notranslate nohighlight">\(R_t\)</span> :reward at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> :discount rate (<span class="math notranslate nohighlight">\(0 \leq \gamma \leq 1\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(G_t\)</span> :discounted return at time <span class="math notranslate nohighlight">\(t\)</span> (<span class="math notranslate nohighlight">\(\sum_{k=0}^\infty \gamma^k R_{t+k+1}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span> :set of all states, also known as state space.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}^T\)</span> :set of all terminal states.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span> :set of all actions, also known as action space .</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(s)\)</span> :set of all actions available in state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> :transition probability to reach next state <span class="math notranslate nohighlight">\(s'\)</span>, given current state <span class="math notranslate nohighlight">\(s\)</span> and current action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi, \mu\)</span> :policy.</p>
<ul>
<li><p><em>if deterministic</em>: <span class="math notranslate nohighlight">\(\pi(s) \in \mathcal{A}(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>.</p></li>
<li><p><em>if stochastic</em>: <span class="math notranslate nohighlight">\(\pi(a|s) = \mathbb{P}(A_t=a|S_t=s)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(a \in \mathcal{A}(s)\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V^\pi\)</span> :state-value function for policy <span class="math notranslate nohighlight">\(\pi\)</span> (<span class="math notranslate nohighlight">\(v_\pi(s) \doteq E[G_t|S_t=s]\)</span> for all <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(Q^\pi\)</span> :action-value function for policy <span class="math notranslate nohighlight">\(\pi\)</span> (<span class="math notranslate nohighlight">\(q_\pi(s,a) \doteq E[G_t|S_t=s, A_t=a]\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(a \in \mathcal{A}(s)\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(V^*\)</span> :optimal state-value function (<span class="math notranslate nohighlight">\(v_*(s) \doteq \max_\pi V^\pi(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(Q^*\)</span> :optimal action-value function (<span class="math notranslate nohighlight">\(q_*(s,a) \doteq \max_\pi Q^\pi(s,a)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(a \in \mathcal{A}(s)\)</span>).</p></li>
</ul>
</section>
<section id="overview">
<h3><span class="section-number">14.1.2. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>In essence, we can view <strong>reinforcement learning (RL)</strong> as a data-driven way to solve the policy optimization problem in a <strong>MDP (Markov Decision Process)</strong>. In RL, we use a setting that contains an <strong>agent</strong> and an <strong>environment</strong> [<a class="reference internal" href="#ch-reinforcement-learning-fig-agentenvmodel"><span class="std std-numref">Fig. 14.1</span></a>], where data are collected through agent-environment interactions and then further utilized to optimize policies. The agent iteratively takes action specified by a policy, interacts with the environment, and ultimately learn ‘knowledge or strategies’ from the interactions. At every step, the agent makes an observation of the environment (we called a state or an observation), then it chooses an action to take. The environment will respond to the agent’s action by changing its state and providing a reward signal to the agent, which serves as a gauge on the quality of the agent’s current action. The goal of the agent, however, is not to maximize the immediate reward of an action, but is to maximize its cumulative reward along the whole process.</p>
<figure class="align-default" id="ch-reinforcement-learning-fig-agentenvmodel">
<a class="reference internal image-reference" href="../../_images/AgentEnvInteraction.jpg"><img alt="../../_images/AgentEnvInteraction.jpg" src="../../_images/AgentEnvInteraction.jpg" style="width: 551.4px; height: 377.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.1 </span><span class="caption-text">One core component in reinforcement learning is agent environment interaction. The agent takes actions based on observations on the environment and a decision-making module that maps observations to action. The environment model updates system state  and provides rewards according to the action</span><a class="headerlink" href="#ch-reinforcement-learning-fig-agentenvmodel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For example, in the Atari game <em>Breakout</em> [<a class="reference internal" href="#ch-reinforcement-learning-fig-breakoutgame"><span class="std std-numref">Fig. 14.2</span></a>], we assume an agent controls the bat to deflect the ball to destroy  the bricks. The actions allowed are moving left and moving right; the state includes positions of the ball, the bat, and all the bricks; rewards will be given to the agent if bricks are hit. The environment, represented by a physical simulator, will simulate the ball’s trajectory and collision between the ball and the bricks. The ultimate goal is learn a control policy that specifies the action to take after observing a state.</p>
<figure class="align-default" id="ch-reinforcement-learning-fig-breakoutgame">
<a class="reference internal image-reference" href="../../_images/breakoutGame.PNG"><img alt="../../_images/breakoutGame.PNG" src="../../_images/breakoutGame.PNG" style="width: 259.6px; height: 309.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.2 </span><span class="caption-text">Scheme of the Atari game <em>breakout</em>.</span><a class="headerlink" href="#ch-reinforcement-learning-fig-breakoutgame" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In <strong>finite state MDP</strong>, we introduce the two step iterative framework consisting of <strong>policy evaluation</strong> and <strong>policy improvement</strong> to seek optimal control policies. Although the agent-environment interaction paradigm seems to be vastly different from the Markov decision framework, many reinforcement learning algorithms can also be interpreted under this two step framework. We can view the agent-environment interaction process under a specified policy as policy evaluation step based on the rewards received from the environment. With the estimated value functions, we can similarly apply the policy improvement methods [<a class="reference internal" href="#ch-reinforcement-learning-fig-rlpolicyiterationimprovement"><span class="std std-numref">Fig. 14.3</span></a>].</p>
<p>The sharp contrast between MDP and reinforcement learning is that reinforcement learning is <strong>model-free learning</strong>, the knowledge of the environment is from agent-environment interaction data. On the other hand, <strong>MDP is model-based learning</strong>, meaning that the agent already know beforehand all the responses and rewards from environment for every action it takes. For complex read-world problems where models are not available, reinforcement learning offers a viable approach to learning optimal control policy via gradually building up the knowledge of the environment.</p>
<p>How much knowledge of the environment should the agent gain before the agent starts to improve the policy? Exploring the environment sufficiently would be prohibitive; on the other hand, improving the policy based on limited knowledge can produce inferior policies. The balance of environment exploration and policy improvement is known as the <strong>exploration-exploitation dilemma</strong>. One common way out is the <span class="math notranslate nohighlight">\(\epsilon\)</span> greedy action selection, where the agent has <span class="math notranslate nohighlight">\((1-\epsilon)\)</span> probability to continue to collect more samples based on currently perceived optimal control policy to improve current policy and <span class="math notranslate nohighlight">\(\epsilon\)</span> probability to randomly explore the environment via random actions.</p>
<figure class="align-default" id="ch-reinforcement-learning-fig-rlpolicyiterationimprovement">
<a class="reference internal image-reference" href="../../_images/RL_policyIterationImprovement.jpg"><img alt="../../_images/RL_policyIterationImprovement.jpg" src="../../_images/RL_policyIterationImprovement.jpg" style="width: 640.1999999999999px; height: 321.3px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.3 </span><span class="caption-text">Policy evaluation and policy improvement framework in the context reinforcement learning.</span><a class="headerlink" href="#ch-reinforcement-learning-fig-rlpolicyiterationimprovement" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="finite-state-mdp">
<h3><span class="section-number">14.1.3. </span>Finite-state MDP<a class="headerlink" href="#finite-state-mdp" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 14.1 </span> (Finite state Markov decision process (MDP))</p>
<section class="definition-content" id="proof-content">
<p>A <strong>finite state Markov decision process (MDP)</strong> is characterized by a tuple <span class="math notranslate nohighlight">\((\mathcal{S}, \mathcal{A}, \mathcal{P})\)</span> where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the state space, <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is the action space, and <span class="math notranslate nohighlight">\(\mathcal{P} = \{p(s'|s, a), s,s'\in \mathcal{S}, a\in \mathcal{A}\}\)</span> is the state transition probability. We require <span class="math notranslate nohighlight">\(\mathcal{S}, \mathcal{A}\)</span> to have finite number of elements.<br />
The goal is compute an optimal control policy <span class="math notranslate nohighlight">\(\pi^*: \mathcal{S}\to \mathcal{A}\)</span> such that the expected  total reward in the process</p>
<div class="math notranslate nohighlight">
\[
J = E[\sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_t)]
\]</div>
<p>is maximized, where <span class="math notranslate nohighlight">\(R(s,a):\mathcal{S}\times\mathcal{A}\to{\mathbb(R)}\)</span> is the one-step reward function and <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span> is the discount factor.</p>
</section>
</div><div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 14.1 </span> (examples of reward functions)</p>
<section class="example-content" id="proof-content">
<ul class="simple">
<li><p>In a navigation task, we can set <span class="math notranslate nohighlight">\(r(s_t,a_t) = \mathbb{I}(s_t \in S_{target})\)</span>.</p></li>
<li><p>In a game, the state of gaining scores has a reward 1 and other states have a reward 0.</p></li>
</ul>
</section>
</div><div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 14.2 </span> (value functions)</p>
<section class="definition-content" id="proof-content">
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(\pi\)</span> be a given control policy. We can define a <strong>value function</strong> associated with this policy by</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = E^\pi[\sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_t)|s_0 = s, \pi],\]</div>
<p>which is the expected total rewards by following a given policy <span class="math notranslate nohighlight">\(\pi\)</span> starting from initial state <span class="math notranslate nohighlight">\(s\)</span>.</p>
</li>
<li><p>Given a value function associated with a policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can obtain <span class="math notranslate nohighlight">\(\pi\)</span> via</p>
<div class="math notranslate nohighlight">
\[\pi(s) = \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}p(s'|s,a)(r + \gamma V(s'))\]</div>
<p>where <span class="math notranslate nohighlight">\(r = R(s', a)\)</span> is the reward received at state <span class="math notranslate nohighlight">\(s'\)</span> after taking action <span class="math notranslate nohighlight">\(a\)</span> at <span class="math notranslate nohighlight">\(s\)</span>.</p>
</li>
<li><p>The <strong>optimal value function</strong> <span class="math notranslate nohighlight">\(V^*\)</span> and the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> are connected via</p>
<div class="math notranslate nohighlight">
\[V^*(s) = \max_{\pi} V^\pi(s), \pi^*(s) = \max_{\pi} V^\pi(s).\]</div>
</li>
</ul>
</section>
</div><div class="proof lemma admonition" id="ch:reinforcement-learning:th:recursiveRelationshipValueFunctionMDP">
<p class="admonition-title"><span class="caption-number">Lemma 14.1 </span> (recursive relationship of value functions)</p>
<section class="lemma-content" id="proof-content">
<p>Given a value function <span class="math notranslate nohighlight">\(V^\pi\)</span> associated with a control policy <span class="math notranslate nohighlight">\(\pi\)</span>.
The value function satisfies the following backward relationship:</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = E_{s'\sim P(s'|s, a = \pi(s))}[R(s',a)+ \gamma V^\pi(s')].\]</div>
<p>Particularly, we have the Bellman’s equation characterizing the optimal value function by</p>
<div class="math notranslate nohighlight">
\[V^*(s) = \max_aE_{s'\sim P(s'|s, a )}[R(s',a)+ \gamma V^*(s')].\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. 	The definition of <span class="math notranslate nohighlight">\(V^\pi\)</span> says</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
V^\pi(s) &amp;= E^\pi[\sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_t)|s_0 = s, \pi]\\
		&amp;=E_{s_1\sim P(s_1|s_0, a = \pi(s))} [R(s_{1}, a_0) + E^\pi[\sum_{t=1}^\infty \gamma^t R(s_{t+1}, a_t)|s_1 = s', \pi]|s_0 = s, \pi]\\
		&amp;=E_{s_1\sim P(s_1|s_0, a = \pi(s))} [R(s_{1}, a_0) + V(s_1)|s_0 = s, \pi]
\end{align*}\end{split}\]</div>
<p>where we have used the <strong>tower property of conditional expectation</strong>.</p>
</div>
</section>
<section id="state-action-value-function-q-function">
<h3><span class="section-number">14.1.4. </span>State-action Value function (<span class="math notranslate nohighlight">\(Q\)</span> function)<a class="headerlink" href="#state-action-value-function-q-function" title="Link to this heading">#</a></h3>
<p>Like value function in a finite state MDP, <span class="math notranslate nohighlight">\(Q\)</span> functions <span id="id1">[]</span> play the same critical role in reinforcement learning. Now we go through their formal definition and their recursive relations.</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 14.3 </span> (state-action value function - <span class="math notranslate nohighlight">\(Q\)</span> function)</p>
<section class="definition-content" id="proof-content">
<ul>
<li><p>The <strong>state-action value function</strong> <span class="math notranslate nohighlight">\(Q^\pi:S\times A\to \mathbb(R)\)</span> associated with a policy <span class="math notranslate nohighlight">\(\pi\)</span> is defined as the expected return starting from state <span class="math notranslate nohighlight">\(s\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span> and thereafter following the policy <span class="math notranslate nohighlight">\(\pi\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[Q^\pi(s,a) = E^\pi \{\sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_t)|s_0 = s,a_t = a\}.\]</div>
</li>
<li><p>The optimal state-action value function <span class="math notranslate nohighlight">\(Q^*:S\times A\to \mathbb(R)\)</span>  is defined as the expected return starting from state <span class="math notranslate nohighlight">\(s\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span> and thereafter following an optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[Q^*(s,a) = \max_{\pi} Q^\pi(s,a) \]</div>
</li>
<li><p>The optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> is related to <span class="math notranslate nohighlight">\(Q^*\)</span> as</p>
<div class="math notranslate nohighlight">
\[\pi^*(s) = \arg \max_a Q^*(s,a).\]</div>
</li>
<li><p>The value function associated with a policy <span class="math notranslate nohighlight">\(\pi\)</span>,</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = E^\pi \{\sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_t)|s_0 = s\}.\]</div>
</li>
<li><p>The value function <span class="math notranslate nohighlight">\(V(s)\)</span> is connected with <span class="math notranslate nohighlight">\(Q(s,a)\)</span> via</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = Q(s, \pi(s)).\]</div>
</li>
<li><p>The optimal state-action value function is connected to value function via</p>
<div class="math notranslate nohighlight">
\[V^*(s) = \max_{a} Q^*(s,a).\]</div>
</li>
</ul>
</section>
</div><div class="proof lemma admonition" id="lemma-5">
<p class="admonition-title"><span class="caption-number">Lemma 14.2 </span> (recursive relations of the Q function)</p>
<section class="lemma-content" id="proof-content">
<ul class="simple">
<li><p>The state-action value function will satisfy</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Q^\pi(s,a) &amp;= E^\pi_{s'\sim p(s'|s,\pi(s))} [R(s', a) + \gamma Q^\pi(s',\pi(s'))] \\
		&amp;=E^\pi_{s'\sim p(s'|s,\pi(s))} [R(s',a) + \gamma V^\pi(s')],
\end{align*}\end{split}\]</div>
<p>where the expectation is taken with respect to the distribution of <span class="math notranslate nohighlight">\(s'\)</span>(the state after taking <span class="math notranslate nohighlight">\(a\)</span> at <span class="math notranslate nohighlight">\(s\)</span>) and we use the definition <span class="math notranslate nohighlight">\(V^\pi(s') = Q^\pi(s',\pi(s'))\)</span>.</p>
<ul class="simple">
<li><p>Particularly, the optimal state-action value function will satisfy</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Q^*(s,a) &amp;= E^{\pi^*}_{s'\sim p(s'|s,\pi^*(s))}[R(s',a) + \gamma \max_{a\in A(s')} Q*(s',a)]\\
		&amp;= E^{\pi^*}_{s'\sim p(s'|s,\pi^*(s))}[R(s',a) + \gamma V^*(s')]
\end{align*}\end{split}\]</div>
<p>where the expectation is taken with respect to the distribution of <span class="math notranslate nohighlight">\(s'\)</span>(the state after taking <span class="math notranslate nohighlight">\(a\)</span> at <span class="math notranslate nohighlight">\(s\)</span>) we use the definition <span class="math notranslate nohighlight">\(V^*(s') = \max_a Q^*(s',a) = Q^*(s',\pi^*(s'))\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. (1)
From the definition <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Q^\pi(s,a) &amp;= E^\pi [\sum_{t=0}^\infty \gamma^t r_{t+1}|s_0 = s,a_t = a] \\
		   &amp;= E^\pi [r_{1} + \sum_{k=1}^\infty \gamma^k r_{t+1}|s_0 = s,a_t = a] \\
		   &amp;= E^\pi [r_{1} + E^{\pi}[\sum_{k=1}^\infty \gamma^k r_{t+1}|s_1 = s, a_1 = \pi(s_1)]|s_0 = s,a_t = a] \\
		   &amp;= E^\pi [r_{1}|s_0 = s,a_t = a\} + Q^\pi(s_1,\pi(s_1))|s_0 = s,a_t = a]
\end{align*}\end{split}\]</div>
<p>where we have used the tower property of conditional expectation.</p>
<p>(2)
From (1) we have</p>
<div class="math notranslate nohighlight">
\[Q^*(s,a) = {E}^*_{s'\sim p(s'|s,\pi(s))} [r + \gamma Q^*(s',\pi^*(s'))].\]</div>
<p>Further note that <span class="math notranslate nohighlight">\(\pi^*(s') = \arg\max_{a\in \mathcal(A)(s')} Q(s',a')\)</span></p>
</div>
<div class="proof remark admonition" id="remark-6">
<p class="admonition-title"><span class="caption-number">Remark 14.1 </span> (recursive relation for value functions)</p>
<section class="remark-content" id="proof-content">
<p>Recall that in <a class="reference internal" href="#ch:reinforcement-learning:th:recursiveRelationshipValueFunctionMDP">Lemma 14.1</a>, we have covered the recursive relation for value functions.</p>
<ul>
<li><p>Given a value function <span class="math notranslate nohighlight">\(V^\pi\)</span> associated with control policy <span class="math notranslate nohighlight">\(\pi\)</span>.
The value function satisfies the following backward relationship:</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = E_{s'\sim P(s'|s, a = \pi(s))}[R(s',a)+ \gamma V^\pi(s')].\]</div>
</li>
<li><p>Particularly, we have the <strong>Bellman</strong>’s equation saying that</p></li>
<li><div class="math notranslate nohighlight">
\[V^*(s) = \max_aE_{s'\sim P(s'|s, a ))}[R(s',a)+ \gamma V^*(s')].\]</div>
</li>
</ul>
<p>Note that in above two cases, <span class="math notranslate nohighlight">\(a\)</span> is determined by either the policy function <span class="math notranslate nohighlight">\(\pi\)</span> or the maximization. <span class="math notranslate nohighlight">\(a\)</span> is not a free variable.</p>
</section>
</div></section>
<section id="policy-iteration-and-value-iteration">
<span id="ch-reinforcement-learning-sec-policyiterationvalueiteration"></span><h3><span class="section-number">14.1.5. </span>Policy iteration and Value iteration<a class="headerlink" href="#policy-iteration-and-value-iteration" title="Link to this heading">#</a></h3>
<section id="policy-iteration">
<h4><span class="section-number">14.1.5.1. </span>Policy iteration<a class="headerlink" href="#policy-iteration" title="Link to this heading">#</a></h4>
<p>The core idaa underlying policy iteration is to iteratively carry out two procedures: <strong>policy evaluation</strong> and <strong>policy improvement</strong> [<a class="reference internal" href="#ch-reinforcement-learning-fig-policyiterationscheme"><span class="std std-numref">Fig. 14.4</span></a>]. Given a starting policy <span class="math notranslate nohighlight">\(\pi\)</span>, we perform policy evaluation to estimate the value function <span class="math notranslate nohighlight">\(V^\pi\)</span> associated with this policy; then we improve the policy via dynamic programming principles, or the <strong>Bellman Principle</strong>.</p>
<figure class="align-default" id="ch-reinforcement-learning-fig-policyiterationscheme">
<a class="reference internal image-reference" href="../../_images/policyIterationScheme.jpg"><img alt="../../_images/policyIterationScheme.jpg" src="../../_images/policyIterationScheme.jpg" style="width: 667.2px; height: 210.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14.4 </span><span class="caption-text">Policy iteration involves iteratively carrying out policy evaluation and policy improvement procedures.</span><a class="headerlink" href="#ch-reinforcement-learning-fig-policyiterationscheme" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>policy evaluation</strong> step involves estimating value function <span class="math notranslate nohighlight">\(V^\pi\)</span> given a policy <span class="math notranslate nohighlight">\(\pi\)</span>. We can use the following iterative steps:</p>
<div class="math notranslate nohighlight">
\[V^{k+1}(s) = \sum_{s' \in \mathcal{S}}p(s'|s,a = \pi(s))(r + \gamma V^{k}(s')), \forall s\in \mathcal{S}.\]</div>
<p>where superscript <span class="math notranslate nohighlight">\(k\)</span> is the iteration index.</p>
<p><span class="math notranslate nohighlight">\(V^{k}(s)\)</span> will converge to value function <span class="math notranslate nohighlight">\(V^\pi\)</span>, as we show in the following [<a class="reference internal" href="#ch:reinforcement-learning:th:convergenceIterativePolicyEvaluation">Theorem 14.1</a>].</p>
<div class="proof theorem admonition" id="ch:reinforcement-learning:th:convergenceIterativePolicyEvaluation">
<p class="admonition-title"><span class="caption-number">Theorem 14.1 </span> (convergence property of iterative policy evaluation)</p>
<section class="theorem-content" id="proof-content">
<p>For a finite state MDP, we can write the value function recursive relationship explicitly as</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = \sum_{s'\in \mathcal{S}}P(s'|s, a = \pi(s))[R(s',a)+ \gamma V^\pi(s')].\]</div>
<p>We can express the recursive relationship as a matrix form given by</p>
<div class="math notranslate nohighlight">
\[V = T(R + \gamma V),\]</div>
<p>where <span class="math notranslate nohighlight">\(R,V \in \mathbb(R)^{|\mathcal{S}|}, T\in \mathbb(R)^{|\mathcal{S}|\times |\mathcal{S}|}\)</span>.</p>
<p>We further define <span class="math notranslate nohighlight">\(H(V) \triangleq T(R + \gamma V)\)</span> as the policy evaluation operator.</p>
<p>We have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> is a contraction mapping.</p></li>
<li><p>In iterative policy evaluation, <span class="math notranslate nohighlight">\(V^{k}(s)\)</span> will converge to value function <span class="math notranslate nohighlight">\(V^\pi\)</span>. Or equivalently, <span class="math notranslate nohighlight">\(V^\pi\)</span> is the fixed point of <span class="math notranslate nohighlight">\(H\)</span>, and</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\lim_{n\to\infty} H^{n}(V) = V^\pi.\]</div>
<ul class="simple">
<li><p>(error bound) If <span class="math notranslate nohighlight">\(\lVert H^k(V) - H^{k-1 \rVert(V)}_\infty \leq \epsilon,\)</span> then</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\lVert {H^k(V) - V^\pi} \rVert_\infty \leq \frac{\epsilon}{1 - \gamma}.\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. (1)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\begin{array}{l}{\|H(\tilde{V})-H(V)\|_{\infty}} \\ {=\| TR+\gamma T \tilde{V}-TR-\gamma T V||_{\infty} \text { (by definition) }} \\ {=|| \gamma T(\tilde{V}-V)||_{\infty} \quad \text { (simplification) }} \\ {\left.\leq \gamma|| T||_{\infty}|| \tilde{V}-V||_{\infty} \quad \text { (since }\|A B\| \leq\|A\|\|B\|\right)} \\ {\left.=\gamma|| \tilde{V}-V||_{\infty} \quad \text { (since } \max _{s} \sum_{s^{\prime}} T\left(s, s^{\prime}\right)=1\right)}\end{array}
\end{align*}\end{split}\]</div>
<p>(2)
Note that from <strong>Fixed point Theorem</strong>, we have</p>
<div class="math notranslate nohighlight">
\[\lVert H^{n \rVert(V) - V^\pi}_\infty \leq \gamma^n \lVert V - V^\pi \rVert_\infty.\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\lim_{n\to\infty} H^{n}(V) = V^\pi.\]</div>
</div>
<!-- (3)
We have
```{math}
\begin{align*}
& \lVert {H^k(V) - V^\pi} \rVert_\infty \\
		=& \lVert {H^k(V) - H^\infty(V)} \rVert_\infty \\
		=& \lVert {\sum_{t=1  H^{t+k}(V) - H^{t+k + 1}(V)}_\infty} \rVert_\infty \\
		\leq& \sum_{t=1}^\infty \lVert H^{t+k} (V) - H^{t+k + 1}(V)} \rVert_\infty \\
		\leq& \sum_{t=1}^\infty \gamma^t \lVert {H^{k}(V) - H^{k + 1}(V)} \rVert_\infty \\
		\leq& \sum_{t=1}^\infty \gamma^t\epsilon
\end{align*}
``` -->
<div class="proof remark admonition" id="remark-8">
<p class="admonition-title"><span class="caption-number">Remark 14.2 </span> (error estimation and stopping criterion)</p>
<section class="remark-content" id="proof-content">
<p>The third property can be used as a stopping criterion during iterations. Suppose the tolerance is <span class="math notranslate nohighlight">\(Tol\)</span>, then we should iterate until the maximum change during consecutive iteration is small than <span class="math notranslate nohighlight">\((1 - \gamma)\times Tol\)</span>.</p>
</section>
</div><p>Given a learned value function <span class="math notranslate nohighlight">\(V^\pi\)</span> of a policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can derive its <span class="math notranslate nohighlight">\(Q\)</span> function counterpart via</p>
<div class="math notranslate nohighlight">
\[Q(s,a) =  \sum_{s' \in \mathcal{S}}p(s'|s,a)(R(s',a)+\gamma V^\pi(s')), \forall s, a.\]</div>
<p><span class="math notranslate nohighlight">\(Q\)</span> function offer a convenient way to improve current policy <span class="math notranslate nohighlight">\(\pi\)</span>. Indeed, by relying on following <strong>policy improvement theorem</strong> <span id="id2">[]</span>, we can consistently improve our policy towards the optimal one.</p>
<div class="proof theorem admonition" id="ch:reinforcement-learning:th:policyImprovementT">
<p class="admonition-title"><span class="caption-number">Theorem 14.2 </span> (policy improvement theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Define the <span class="math notranslate nohighlight">\(Q\)</span> function associated with a policy <span class="math notranslate nohighlight">\(\pi\)</span> as</p>
<div class="math notranslate nohighlight">
\[Q^\pi(s,a) =  \sum_{s' \in \mathcal{S}}p(s'|s,a)(R(s',a)+\gamma V^\pi(s')), \forall s, a.\]</div>
<p>Let <span class="math notranslate nohighlight">\(\pi\)</span> and <span class="math notranslate nohighlight">\(\pi'\)</span> be two policies. If <span class="math notranslate nohighlight">\(\forall s\in \mathcal(S)\)</span>,</p>
<div class="math notranslate nohighlight">
\[Q^\pi(s, \pi'(s)) \geq V^\pi(s),\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[V^{\pi'}(s) \geq V^\pi(s), \forall s\in \mathcal(S).\]</div>
<p>That is <span class="math notranslate nohighlight">\(\pi'\)</span> is a better policy than <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
</section>
</div><!-- ````{prf:proof}
We have
```{math}
\begin{align*}
V_{\pi}(s) & \leq Q^{\pi}\left(s, \pi^{\prime}(s)\right) \\
		&={E}\left[R_{t+1}+\gamma V^{\pi}\left(S_{t+1}\right) | S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
		&={E}^{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right] \\
		& \leq {E}^{\pi^{\prime}}\left[R_{t+1}+\gamma Q^{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) | S_{t}=s\right] \\
		&={E}^{\pi^{\prime}}\left[R_{t+1}+\gamma E^{\pi}\left[R_{t+2}+\gamma V^{\pi}\left(s_{t+2}\right)| S_{t+1}, A_{t+1}=\pi^{\prime}\left(S_{t+1}\right)\right]|s_t = s\right] \\
		&={E}^{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} V^{\pi}\left(S_{t+2}\right) | S_{t}=s\right] \\
		& \leq E^{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) | S_{t}=s\right] \\
		& \vdots \\
		& \leq E^{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots | S_{t}=s\right] \\
		&=V^{\pi^{\prime}}(s)
\end{align*}
```
```` -->
<p>Based on this theorem, we can create a better policy via following greedy manner (known as <strong>policy improvement procedure</strong>)</p>
<div class="math notranslate nohighlight">
\[\pi'(s) = \arg\max_{a\in\mathcal{A}(s)}Q(s,a), \forall s.\]</div>
<p>Given a value function <span class="math notranslate nohighlight">\(V\)</span>, we can improve the policy implicitly associated with the value function via two steps:</p>
<ul class="simple">
<li><p>First calculate the intermediate <span class="math notranslate nohighlight">\(Q\)</span> function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(s,a) =  \sum_{s' \in \mathcal{S}}p(s'|s,a)(r+\gamma V(s')), \forall s, a.\]</div>
<ul class="simple">
<li><p>Second improve the policy by
$<span class="math notranslate nohighlight">\(\pi'(s) = \arg\max_{a\in\mathcal{A}(s)}Q(s,a), \forall s.\)</span>$</p></li>
</ul>
<p>Notably, let <span class="math notranslate nohighlight">\(\pi'\)</span> be the improved greedy policy, if <span class="math notranslate nohighlight">\(V^{\pi'} = V^\pi\)</span>, then <span class="math notranslate nohighlight">\(\pi\)</span> is the optimal policy, based on the definition and recursive relation of <span class="math notranslate nohighlight">\(V\)</span> [<a class="reference internal" href="#ch:reinforcement-learning:th:recursiveRelationshipValueFunctionMDP">Lemma 14.1</a>].</p>
<p>The following algorithm summarizes the policy iteration method <span id="id3">[]</span>.</p>
<div class="proof algorithm admonition" id="ch:reinforcement-learning:alg:policyIterationMDP">
<p class="admonition-title"><span class="caption-number">Algorithm 14.1 </span> (The policy iteration algorithm for MDP)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> MDP model, a small positive number <span class="math notranslate nohighlight">\(\tau\)</span>.</p>
<p><strong>Output</strong> Policy <span class="math notranslate nohighlight">\(\pi \approx \pi^*\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\pi\)</span> arbitrarily (e.g., <span class="math notranslate nohighlight">\(\pi(a|s)=\frac{1}{|\mathcal{A}(s)|}\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(a \in \mathcal{A}(s)\)</span>)</p></li>
<li><p>Set policyStable = false</p></li>
<li><p>Repeat until policy_table = true:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(V \leftarrow \text{Policy_Evaluation}(\text{MDP}, \pi, \tau)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi' \leftarrow \text{Policy_Improvement}(\text{MDP}, V)\)</span></p></li>
<li><p>If{<span class="math notranslate nohighlight">\(\pi= \pi'\)</span>} policyStable = true$</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi = \pi'\)</span></p></li>
</ol>
</li>
</ol>
</section>
</div></section>
<section id="value-iteration">
<h4><span class="section-number">14.1.5.2. </span>Value iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h4>
<p>The <strong>policy iteration</strong> method iterates the <strong>two steps of evaluating policy and improving policy</strong>. Alternatively, we can also directly iteratively estimate the optimal value function, the value function associated the optimal policy, without evaluating the policy associated with value functions. In fact, since policies can be directly calculated from a given value function, having the optimal value function will just give us the optimal policy.</p>
<p>Let <span class="math notranslate nohighlight">\(V^{(0)}\)</span> be an initial value function.  Based on the <strong>value iteration theorem</strong> [<a class="reference internal" href="#ch:reinforcement-learning:th:valueIterationConvergence">Theorem 14.3</a>], we can design iteration like</p>
<div class="math notranslate nohighlight">
\[V^{(k+1)}(s) = \max_a\sum_{s'\in \mathcal{S}}P(s'|s, a )[R(s',a)+ \gamma V^{(k)}(s')], \forall s\in \mathcal{S}.\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the iteration number. The following theorem shows that this iteration procedure will lead to convergence to the optimal value function.</p>
<p>More formally, we have</p>
<div class="proof theorem admonition" id="ch:reinforcement-learning:th:valueIterationConvergence">
<p class="admonition-title"><span class="caption-number">Theorem 14.3 </span> (convergence property of value iteration)</p>
<section class="theorem-content" id="proof-content">
<p>For a finite state MDP, we can write the optimal value function recursive relationship as</p>
<div class="math notranslate nohighlight">
\[V^*(s) = \max_a\sum_{s'\in \mathcal{S}}P(s'|s, a )[R(s',a)+ \gamma V^*(s')], \forall s\in \mathcal{S}.\]</div>
<p>We can express the recursive relationship as a matrix form given by</p>
<div class="math notranslate nohighlight">
\[V = T(R + \gamma V),\]</div>
<p>where <span class="math notranslate nohighlight">\(R,V \in \mathbb(R)^{|\mathcal{S}|}, T\in \mathbb(R)^{|\mathcal{S}|\times |\mathcal{S}|}\)</span>.</p>
<p>We further define <span class="math notranslate nohighlight">\(H(V) \triangleq \max_a T(R + \gamma V)\)</span> as the value iteration operator.</p>
<p>We have</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> is a <strong>contraction mapping</strong>.</p></li>
<li><p>In iterative policy evaluation, <span class="math notranslate nohighlight">\(V^{k}(s)\)</span> will converge to the unique optimal value function <span class="math notranslate nohighlight">\(V^*\)</span>. Or equivalently, <span class="math notranslate nohighlight">\(V^*\)</span> is the <strong>fixed point</strong> of the contraction mapping <span class="math notranslate nohighlight">\(H\)</span>, and
$<span class="math notranslate nohighlight">\(\lim_{n\to\infty} H^{n}(V) = V^*.\)</span>$</p></li>
<li><p>(error bound) If <span class="math notranslate nohighlight">\(\lVert H^k(V) - H^{k-1 \rVert(V)}_\infty \leq \epsilon,\)</span> then
<span class="math notranslate nohighlight">\(\lVert H^k(V) - V^* \rVert_\infty \leq \frac{\epsilon}{1 - \gamma}.\)</span></p></li>
</ul>
</section>
</div><!-- ````{prf:proof}
(1) Without loss of generality, for each $s$, we assume $H(V')(s) \geq H(V)(s)$ and let 

$$a_s^* = \arg\max_{a}\sum_{s'\in \mathcal{S}}P(s'|s, a )[R(s',a)+ \gamma V^\pi(s')]. $$

Then
```{math}
\begin{align*}
0 \leq & H(V'(s) - H(V)(s) \\
		\leq & \sum_{s'\in \mathcal{S}}P(s'|s, a ) (R(s',a) + \gamma V'(s') - R(s',a) - \gamma V(s')) \\
		\leq & \gamma \sum_{s'\in \mathcal{S}}P(s'|s, a ) (V'(s') - V(s')) \\
		\leq & \gamma \sum_{s'\in \mathcal{S}}P(s'|s, a ) \lVert V'(s') - V(s') \rVert_\infty \\
		= & \gamma \lVert V'(s') - V(s') \rVert_\infty.
\end{align*}
```

(2) 
Because $H$ is a contraction mapping and $V^* = HV^*$, $V^*$ is the fixed point of $H$. Use Banach Fixed Point Theorem [{numref}`ch:functional-analysis:th:BanachFixedPointTheorem`], we have 
$$\lim_{n\to\infty} H^{n}(V) = V^\pi.$$
(3)
```{math}
\begin{align*}
& \lVert H^k(V) - V^\pi \rVert_\infty \\
		=& \lVert H^k(V) - H^\infty(V) \rVert_\infty \\
		=& \lVert \sum_{t=1 \rVert^\infty H^{t+k}(V) - H^{t+k + 1}(V)}_\infty \\
		\leq& \sum_{t=1}^\infty \lVert H^{t+k \rVert(V) - H^{t+k + 1}(V)}_\infty \quad \text{via triangle inequality}\\
		\leq& \sum_{t=1}^\infty \gamma^t \lVert H^{k \rVert(V) - H^{k + 1}(V)}_\infty \quad \text{via contraction mapping}\\
		\leq& \sum_{t=1}^\infty \gamma^t\epsilon \\
		\leq& \frac{\epsilon}{1 - \gamma}.
\end{align*}
```
We can further refer to similar proofs regarding that the dynamic programming operator is a contraction mapping.
```` -->
<p>A direct application of the value iteration theorem gives the following <strong>value iteration algorithm</strong> [<a class="reference internal" href="#ch:reinforcement-learning:alg:valueIterationAlg">Algorithm 14.2</a>].</p>
<p>\begin{algorithm}[H]
\KwIn{MDP, small positive number <span class="math notranslate nohighlight">\(\epsilon\)</span> as tolerance }
\KwOut{Value function <span class="math notranslate nohighlight">\(V \approx V^*\)</span> and policy <span class="math notranslate nohighlight">\(\pi \approx \pi^*\)</span>. }
Initialize <span class="math notranslate nohighlight">\(V\)</span> arbitrarily. Set (<span class="math notranslate nohighlight">\(V(s)=0\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}^T\)</span>.)\
\mathbb(R)epeat{<span class="math notranslate nohighlight">\(\Delta &lt; \epsilon\)</span>}{
<span class="math notranslate nohighlight">\(\Delta = 0\)</span>\
\For{<span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>}{
<span class="math notranslate nohighlight">\(v = V(s)\)</span>\
<span class="math notranslate nohighlight">\(V(s) = \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}P(s'|s,a)(R(s',a) + \gamma V(s'))\)</span>\
<span class="math notranslate nohighlight">\(\Delta = \max(\Delta, |v-V(s)|)\)</span>
}
}
Compute the policy
<span class="math notranslate nohighlight">\(\pi(s) = \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}P(s'|s,a)(R(s',a) + \gamma V(s'))\)</span>. \
\KwRet{<span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(\pi\)</span>}
\caption{Value iteration algorithm for a finite state MDP}\label{ch:reinforcement-learning:alg:valueIterationAlg}
\end{algorithm}</p>
<div class="proof algorithm admonition" id="ch:reinforcement-learning:alg:valueIterationAlg">
<p class="admonition-title"><span class="caption-number">Algorithm 14.2 </span> (Value iteration algorithm for a finite state MDP)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> MDP, a small positive number <span class="math notranslate nohighlight">\(\tau\)</span> as tolerance</p>
<p><strong>Output</strong> Value function <span class="math notranslate nohighlight">\(V \approx V^*\)</span> and policy <span class="math notranslate nohighlight">\(\pi \approx \pi^*\)</span>.</p>
<ol class="arabic">
<li><p>Initialize <span class="math notranslate nohighlight">\(V\)</span> arbitrarily. Set (<span class="math notranslate nohighlight">\(V(s)=0\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}^T\)</span>.)</p></li>
<li><p>Set policyStable = false</p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\(\Delta &lt; \tau\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\Delta = 0\)</span>.
For <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(v = V(s)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V(s) = \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}P(s'|s,a)(R(s',a) + \gamma V(s'))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta = \max(\Delta, |v-V(s)|)\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Compute the policy</p>
<div class="math notranslate nohighlight">
\[\pi(s) = \max_{a\in\mathcal{A}(s)}\sum_{s' \in \mathcal{S}}P(s'|s,a)(R(s',a) + \gamma V(s')).\]</div>
</li>
</ol>
</section>
</div><div class="proof remark admonition" id="remark-13">
<p class="admonition-title"><span class="caption-number">Remark 14.3 </span> (value iteration vs. policy iteration; model-based vs. model-free)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p>At the first glance, it may seem the simplicity of the value iteration method will make the policy iteration method obsolete. It is critical to know that value iteration requires the knowledge of model, which is represented by the transition probabilities <span class="math notranslate nohighlight">\(p(s'|s, a)\)</span>.</p></li>
<li><p>Later we will see that for many complicated real-world decision-making problems, a model is a luxury and often unavailable. In such situations, we usually turn to reinforcement learning, a model-free, data-driven approach to learn control policies. Most reinforcement learning algorithms, from a high-level abstraction, are comprised of the two steps of policy evaluation and policy improvement.</p></li>
</ul>
</section>
</div></section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="alignment.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>LLM Alignement and Preference Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="accelerated_training.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>LLM Training Acceleration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-framework">14.1. Reinforcement learning framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notations">14.1.1. Notations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">14.1.2. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-state-mdp">14.1.3. Finite-state MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-action-value-function-q-function">14.1.4. State-action Value function (<span class="math notranslate nohighlight">\(Q\)</span> function)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-and-value-iteration">14.1.5. Policy iteration and Value iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">14.1.5.1. Policy iteration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">14.1.5.2. Value iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>