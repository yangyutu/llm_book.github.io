
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>16. LLM Training Acceleration (WIP) &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/accelerated_training';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="17. *Lab: LLM Pretraining" href="../notebooks/chapter_LLM_training/annotated_pretraining.html" />
    <link rel="prev" title="15. *Reinforcement Learning Essentials" href="reinforcement_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_arch/annotated_llama_custom.html">11. *Lab: Minimal LLama</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">12. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">13. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alignment.html">14. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_learning.html">15. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">16. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_pretraining.html">17. *Lab: LLM Pretraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_llama_custom_for_finetuning.html">18. *Lab: Annotated Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_llama_custom_for_DPO.html">19. *Annotated DPO Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">20. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">21. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">22. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">23. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">24. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">25. Advanced RAG (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_multimodality/vision_transformers.html">Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">26. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">27. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Training Acceleration (WIP)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-requirement-for-training-llm">16.1. The Memory Requirement For Training LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-optimizer-states">16.1.1. Model and Optimizer States</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">16.1.2. Activations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-checkpointing-techniques">16.1.3. Activation Checkpointing Techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-training">16.2. Mixed Precision Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">16.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">16.2.2. Training Process</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-parallel-training">16.3. Distributed Parallel Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-parallel-training-techniques">16.3.1. Overview of parallel training techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelism-tensor-parallelism">16.3.2. Model parallelism (tensor parallelism)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-via-deepspeed">16.4. ZeRO Via DeepSpeed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-allocation">16.4.1. GPU Memory Allocation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-stage-one">16.4.2. ZeRO-Stage-One</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">16.5. Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-data-types">16.5.1. Floating Data Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-parallel-operations">16.5.2. GPU Parallel Operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">16.6. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-training-acceleration-wip">
<h1><span class="section-number">16. </span>LLM Training Acceleration (WIP)<a class="headerlink" href="#llm-training-acceleration-wip" title="Link to this heading">#</a></h1>
<section id="the-memory-requirement-for-training-llm">
<h2><span class="section-number">16.1. </span>The Memory Requirement For Training LLM<a class="headerlink" href="#the-memory-requirement-for-training-llm" title="Link to this heading">#</a></h2>
<!-- % https://medium.com/@maxshapp/understanding-and-estimating-gpu-memory-demands-for-training-llms-in-practise-c5ef20a4baff -->
<p>We will discuss the following in this section</p>
<ul class="simple">
<li><p>How much GPU memory do you need to train <span class="math notranslate nohighlight">\(X\)</span> billion Transformer based LLM per each GPU device.</p></li>
<li><p>What is the formula to estimate memory requirements.</p></li>
<li><p>What would you do in practise to reduce the memory needs if the model does not fit.</p></li>
</ul>
<section id="model-and-optimizer-states">
<h3><span class="section-number">16.1.1. </span>Model and Optimizer States<a class="headerlink" href="#model-and-optimizer-states" title="Link to this heading">#</a></h3>
<p>Consider the case that we train a LLM using Adam optimizer, we need to have enough GPU memory to store</p>
<ul class="simple">
<li><p>Copy of model parameter</p></li>
<li><p>Copy of model parameter gradients</p></li>
<li><p>Copy of optimizer states, include copy of the model parameters, momentum, and variance.</p></li>
</ul>
<p>Assume that</p>
<ul class="simple">
<li><p>Model parameters and graidents are stored in FP16 (2 bytes),</p></li>
<li><p>Optimizer states are stored in FP32 (4 bytes) for stable training
then training a <span class="math notranslate nohighlight">\(X\)</span> billion model requires following GPU memory amount just to store the model and training states</p></li>
</ul>
<div class="math notranslate nohighlight">
\[(2 + 2 + 12) X ~\text{(GB)}.\]</div>
<p>The following table gives the example for the memory requirement for models of different sizes.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model Size</p></th>
<th class="head text-right"><p>GPU Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>0.5B</p></td>
<td class="text-right"><p>8 GB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3B</p></td>
<td class="text-right"><p>48 GB</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>7B</p></td>
<td class="text-right"><p>112 GB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>70B</p></td>
<td class="text-right"><p>1120 GB</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="activations">
<h3><span class="section-number">16.1.2. </span>Activations<a class="headerlink" href="#activations" title="Link to this heading">#</a></h3>
<p>Let’s have the following notations: <span class="math notranslate nohighlight">\(b\)</span> is the batch size, <span class="math notranslate nohighlight">\(s\)</span> is the sequence length, <span class="math notranslate nohighlight">\(d\)</span> is the model hidden dim, <span class="math notranslate nohighlight">\(L\)</span> is number of layers, <span class="math notranslate nohighlight">\(p\)</span> is the byte size per model paraemters (e.g., 2 for float16). The multiplier 2 is because of both K and V are cached.</p>
<p>Based on the <strong>FFN</strong> architecture detailed in <a class="reference internal" href="../chapter_foundation/transformers.html#chapter-foundation-sec-pretrained-lm-transformer-arch-ffn"><span class="std std-ref">Pointwise FeedForward Layer</span></a>, we can estimate the memory requireement for FFN activations</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Component</p></th>
<th class="head text-right"><p>Memory</p></th>
<th class="head text-right"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>First Layer</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(4bsdp\)</span></p></td>
<td class="text-right"><p>Output dimension is <span class="math notranslate nohighlight">\(4h\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Activation</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(4bsdp\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Second Layer</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(4bsdp\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Dropout Layer</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(sbd\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(9bsdp + bsd\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>Based on the <strong>MHA</strong> architecture detailed in <a class="reference internal" href="../chapter_foundation/transformers.html#chapter-foundation-sec-pretrained-lm-transformer-arch-mha"><span class="std std-ref">Multihead Attention with Masks</span></a>, we can estimate the memory requireement for MHA activations.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Component</p></th>
<th class="head text-right"><p>Memory</p></th>
<th class="head text-right"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Q/K/V projection</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(3bsdp\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Softmax input and output</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(2bs^2Hp\)</span></p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(s\times s\)</span> attention matrix for each head</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Dropout after Softmax</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(bs^2H\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Output from <span class="math notranslate nohighlight">\(H\)</span> attention head</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(bsdp\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Output layer (<span class="math notranslate nohighlight">\(W_O\)</span>)</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(bsdp\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Dropout</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(bsd\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(5bsdp + 2bs^2Hp + bsd + bs^2H\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>Additionally, there are two <strong>Normalization Layers</strong> in each Transformer Layer, the output from each such layer will require in total <span class="math notranslate nohighlight">\(2bsdp\)</span> bytes.</p>
<p>Now we arrive at the <strong>total amount of bytes required to store the activations</strong> for a <span class="math notranslate nohighlight">\(L\)</span> layer Transformer:</p>
<div class="math notranslate nohighlight">
\[M_{\text{per layer}} =  \underbrace{17bsdp}_{Linear Layer} + \underbrace{2bs^2Hp}_{Softmax} + \underbrace{2bsd + bs^2H}_{Dropout}\]</div>
<p>If we ignore the small quantity <span class="math notranslate nohighlight">\(2bsd\)</span> and take <span class="math notranslate nohighlight">\(p = 2\)</span> (which is float16, 2bypte), we have</p>
<div class="math notranslate nohighlight">
\[M_{approx} = (34bsd + 5bs^2H)\times L\]</div>
<p>The implication on activation memory requirement are</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(M\)</span> scales linearly with batch size</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> scales linearly with layer number</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> scales quadratically with sequence length. During training, we cannot afford large context windows.</p></li>
<li><p>Using technique like GQA [<a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html#chapter-llm-arch-sec-self-attention-variant-gqa"><span class="std std-ref">Grouped Query Attention (GQA)</span></a>] can help save training memory.</p></li>
</ul>
<!-- ```
def activations_memory(num_layers, seq_len, batch_size, hidden_dim, num_heads, precision=2):
    "Returns amount of GPU VRAM (in GB) required to store intermediate activations for traditional Transformer Encoder block"
    mem_bytes = num_layers * precision * seq_len * batch_size * hidden_dim * (
        16 + 2/precision + 2*num_heads*seq_len/hidden_dim + num_heads*seq_len/(precision*hidden_dim))
    return round(mem_bytes / 10**9, 2)
``` -->
<p>Using the model training config from Qwen2 model <span id="id1">[<a class="reference internal" href="#id30" title="An Yang, Baosong Yang, Binyuan Hui, and et al. Bo Zheng. Qwen2 technical report. 2024. URL: https://arxiv.org/abs/2407.10671, arXiv:2407.10671.">YYHBZ24</a>]</span>, we have the following summary on the activation GPU memory requirement for setting batch size to 1.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model Size</p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(L\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(d\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(s\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(H\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(b\)</span></p></th>
<th class="head text-right"><p>GPU Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>0.5B</p></td>
<td class="text-right"><p>24</p></td>
<td class="text-right"><p>896</p></td>
<td class="text-right"><p>4096</p></td>
<td class="text-right"><p>14</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>2.9 GB + 2 GB = 4.9 GB</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>7B</p></td>
<td class="text-right"><p>28</p></td>
<td class="text-right"><p>3584</p></td>
<td class="text-right"><p>4096</p></td>
<td class="text-right"><p>28</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>3.9 GB + 4 GB = 7.9GB</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>72B</p></td>
<td class="text-right"><p>64</p></td>
<td class="text-right"><p>8192</p></td>
<td class="text-right"><p>4096</p></td>
<td class="text-right"><p>64</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>71 GB + 8 GB = 79GB</p></td>
</tr>
</tbody>
</table>
</div>
<!-- | 3B    | 48 GB    |
| 7B    | 112 GB    |
| 70B    | 1120 GB    | -->
</section>
<section id="activation-checkpointing-techniques">
<h3><span class="section-number">16.1.3. </span>Activation Checkpointing Techniques<a class="headerlink" href="#activation-checkpointing-techniques" title="Link to this heading">#</a></h3>
<p>LLM have an enormous number of parameters. In the typical backpropogation during training, we save all the activation values from the forward pass to compute gradient, which consumes a large amount of GPU memory.</p>
<p>On one extreme, we can completely discard the activation values from the forward pass and recalculate the necessary activation values when computing gradients. While this mitigates the activation memory footprint issue, it increases the computational load and slows down training.</p>
<p><strong>Gradient Checkpointing</strong> <span id="id2">[<a class="reference internal" href="#id29" title="Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. 2016. URL: https://arxiv.org/abs/1604.06174, arXiv:1604.06174.">CXZG16</a>]</span> sits in the middle of these two approaches. This method employs a strategy that selects and saves a portion of the activation values from the computational graph, discarding the rest. The discarded activation values need to be recalculated during gradient computation.</p>
<p>Specifically, during the forward pass, activation values of computational nodes are calculated and saved. After computing the next node, the activation values of intermediate nodes are <strong>selectively discarded</strong>. During backpropagation, saved activations for gradient computation are used directly. If not, the actitions of the current node are recalculated using the saved activations from the previous node.</p>
</section>
</section>
<section id="mixed-precision-training">
<h2><span class="section-number">16.2. </span>Mixed Precision Training<a class="headerlink" href="#mixed-precision-training" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">16.2.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>Training billion-scale LLM requires huge number of memory, which include model loading, optimizer state storage, and gradient storage. The idea of using low-precision for precision-insensitive computation and high-recision for precision sensitive computation leads to Mixed-precision training<span id="id3">[<a class="reference internal" href="#id1526" title="Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.">MNA+17</a>]</span>. Mixed-precision training lowers the required resources by using lower-precision arithmetic, and it therefore widely used in LLM training. It has the following benefits.</p>
<p><strong>Reduced memory footprint</strong>: Mixed precision training leverages half-precision floating point format (FP16), which uses only 16 bits per number, in contrast to the 32 bits used by single precision (FP32). This significant reduction in memory usage offers two key advantages:</p>
<ol class="arabic simple">
<li><p>Enables training of larger models: With the same memory constraints, developers can design and train models with more parameters or greater complexity.</p></li>
<li><p>Allows for larger minibatches: Increased batch sizes can lead to more stable gradients and potentially faster convergence in some cases.</p></li>
</ol>
<p><strong>Accelerated training and inference</strong>: The performance gains from mixed precision training stem from two main factors:</p>
<ol class="arabic simple">
<li><p>Reduced memory bandwidth usage: Since FP16 requires half the memory bandwidth of FP32, layers that are memory-bound can see substantial speed improvements.</p></li>
<li><p>Faster arithmetic operations: Many modern GPUs have specialized hardware for FP16 (and lower precision like int8, FP8) operations , allowing them to perform these calculations much faster than FP32 operations.
These factors combine to potentially shorten both training and inference times, especially for large models or when processing substantial amounts of data.</p></li>
</ol>
</section>
<section id="training-process">
<h3><span class="section-number">16.2.2. </span>Training Process<a class="headerlink" href="#training-process" title="Link to this heading">#</a></h3>
<p>This section describes three techniques for successful training of DNNs with half precision: accumulation of FP16 products into FP32; loss scaling; and an FP32 master copy of weights. With these techniques NVIDIA and Baidu Research were able to match single-precision result accuracy for all networks that were trained.<span id="id4">[<a class="reference internal" href="#id1526" title="Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.">MNA+17</a>]</span></p>
<p>As shown in <a class="reference internal" href="#chapter-training-fig-efficient-training-mixed-precision-process-demo"><span class="std std-numref">Fig. 16.1</span></a>, key steps in the mixed-precision training are</p>
<ul class="simple">
<li><p>Maintain a master copy of model parameters, optimizer momentums and variances with fp32 precision.</p></li>
<li><p>Before the model forward pass begins, allocate new storage to save model parameters in the fp16 format.</p></li>
<li><p>Perform forward pass, the produced activations will be saved as fp16.</p></li>
<li><p>Perform backward pass, the produced gradients will be saved as fp16.</p></li>
<li><p>Use fp16 gradients to update model parameters that are saved as fp32.</p></li>
</ul>
<figure class="align-default" id="chapter-training-fig-efficient-training-mixed-precision-process-demo">
<a class="reference internal image-reference" href="../../_images/mixed_precision_process_demo.png"><img alt="../../_images/mixed_precision_process_demo.png" src="../../_images/mixed_precision_process_demo.png" style="width: 484.04999999999995px; height: 452.54999999999995px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.1 </span><span class="caption-text">Model training step with mixed precision using classifical Adam algorithm [<a class="reference internal" href="training_fundamentals.html#Adam_stochastic_gradient_descent_algorithm">Algorithm 12.2</a>].</span><a class="headerlink" href="#chapter-training-fig-efficient-training-mixed-precision-process-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We can estimate the memory storage consumption according to the following table. Denote the model parameter size by <span class="math notranslate nohighlight">\(\Phi\)</span>. Let the storage unit be byte. We need <span class="math notranslate nohighlight">\(16\Phi\)</span> memory storage in total.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1560">
<caption><span class="caption-number">Table 16.1 </span><span class="caption-text">Storage requirement for different components during LLM training using Adam.</span><a class="headerlink" href="#id1560" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Type</p></th>
<th class="head text-left"><p>Storage Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Parameter (fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Momentum(fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Variance (fp32)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(4 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Parameter (fp16)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 \Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Gradients (fp16)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2 \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Total</strong>:</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(16 \Phi\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<!-- 
\begin{remark}[the size of activations and model parameters]
The number of activations is typically much smaller than the size of the model parameters. 
Take an MLP with input dim $D_0$, hidden dim $D_1$, and output dim $D_2$ as an example. The total number of activations, including initial inputs, are $D_0 + D_1 + D_2$. 

The total number of model parameters are $D_0\times D_1 + D_1\times D_2$.
\end{remark}

\begin{remark}[Precision options fp16, bf16, and fp8]
Both BF16 and FP16 are 16-bit floating-point formats used in deep learning models. The difference between them is how the bits are divided between the exponent and fraction. BF16 has the same exponent range as FP32 but with fewer bits for the fraction1. Each number has one sign bit.

Although having similar theoretical performance benefits, BF16 and FP16 can have different speeds in practice. It’s recommended to try both formats and use the one with best speed while maintaining the desired numeric behavior.	

% https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/
Out-of-the-box mixed precision training with either float16 or bfloat16 is effective at speeding up the convergence of many deep learning models, but some models may require more careful numerical accuracy management. Here are Best practice 
Figure out by experimentation if your network is sensitive to range and/or precision of a format. For example fine-tuning bfloat16-pretrained models in float16 can easily run into range issues in float16 because of the potentially large range from training in bfloat16, so users should stick with bfloat16 fine-tuning if the model was trained in bfloat16.

\end{remark} -->
</section>
</section>
<section id="distributed-parallel-training">
<h2><span class="section-number">16.3. </span>Distributed Parallel Training<a class="headerlink" href="#distributed-parallel-training" title="Link to this heading">#</a></h2>
<section id="overview-of-parallel-training-techniques">
<h3><span class="section-number">16.3.1. </span>Overview of parallel training techniques<a class="headerlink" href="#overview-of-parallel-training-techniques" title="Link to this heading">#</a></h3>
</section>
<section id="model-parallelism-tensor-parallelism">
<span id="chapter-training-sec-distributed-parallel-training-model-parallelism"></span><h3><span class="section-number">16.3.2. </span>Model parallelism (tensor parallelism)<a class="headerlink" href="#model-parallelism-tensor-parallelism" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="zero-via-deepspeed">
<h2><span class="section-number">16.4. </span>ZeRO Via DeepSpeed<a class="headerlink" href="#zero-via-deepspeed" title="Link to this heading">#</a></h2>
<p>Data parallelism is the most widely used technique because it is simple and easy to implement. However, tt is typically challenging to apply the vanilla flavor data parallelism since it requires each GPU to store the parameters of the whole model. As a result, the size of GPU memory becomes the ceiling of the model scale we can train.</p>
<p>Model parallelism (e.g, Megatron) [<a class="reference internal" href="#chapter-training-sec-distributed-parallel-training-model-parallelism"><span class="std std-ref">Model parallelism (tensor parallelism)</span></a>], desipte its success in T5 (11B) and Megatron-LM (8.3B) is hard to scale beyond model sizes that cannot fit into a single GPU node. This is because model parallelism typically partitions the model weights or layers across GPU devices, incurring a significant communication between devices.</p>
<p>ZeRO (Zero Redundancy Optimizer) <span id="id5">[<a class="reference internal" href="#id22" title="Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. 2020. URL: https://arxiv.org/abs/1910.02054, arXiv:1910.02054.">RRRH20</a>]</span> adopt the data parallism paradigm, and optimize memory efficieny and commnication efficiency.</p>
<section id="gpu-memory-allocation">
<h3><span class="section-number">16.4.1. </span>GPU Memory Allocation<a class="headerlink" href="#gpu-memory-allocation" title="Link to this heading">#</a></h3>
<p>GPU memory is mainly allocated into two parts: <strong>model states</strong> and <strong>residual states</strong>.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-zero-gpu-memory-allocation">
<a class="reference internal image-reference" href="../../_images/gpu_memory_allocation.png"><img alt="../../_images/gpu_memory_allocation.png" src="../../_images/gpu_memory_allocation.png" style="width: 697.2px; height: 278.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.2 </span><span class="caption-text">Model training step with mixed precision.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-zero-gpu-memory-allocation" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Model states refer to the content that is closely related to the model itself and must be stored. Specifically, they include:
\begin{itemize}
\item Optimizer states: momentum and variance in the Adam optimization algorithm.
\item Gradients: model gradients
\item Parameters: model parameters
\end{itemize}</p>
<p>Residual States refer to the content that is not necessary for the model, but is generated during the training process. Specifically, they include:
\begin{itemize}
\item Activation: activation values. We have discussed this in detail in pipeline parallelism. It is used when calculating gradients using the chain rule in the backward process. It can speed up gradient calculation, but it is not necessary to store because it can be calculated by redoing the Forward process.
\item Temporary buffers: temporary storage. For example, storage generated when aggregating gradients sent to a GPU for summation.
\item Unusable fragment memory: fragmented storage space. Although the total storage space is sufficient, if contiguous storage space cannot be obtained, related requests will also fail. Memory defragmentation can solve this type of space waste.
\end{itemize}</p>
</section>
<section id="zero-stage-one">
<h3><span class="section-number">16.4.2. </span>ZeRO-Stage-One<a class="headerlink" href="#zero-stage-one" title="Link to this heading">#</a></h3>
<p>Here’s the English translation of the provided text:</p>
<p>(1) <span class="math notranslate nohighlight">\(P_{os}\)</span> (Optimizer State Partitioning)
ZeRO-Stage-One reduces the required memory on each device by partitioning the optimizer state across <span class="math notranslate nohighlight">\(N_d\)</span> data-parallel processes. Each process only stores and updates its corresponding partition of the optimizer state, which is <span class="math notranslate nohighlight">\(\frac{1}{N_d}\)</span> of the total optimizer state. At the end of each training step, results from each process are collected to obtain the overall updated state parameters.</p>
<p>(2) The result after ZeRO-Stage1 memory optimization, mainly targeting the optimizer state :</p>
<div class="math notranslate nohighlight">
\[
(2+2) \Psi+\frac{K \times \Psi}{N_d}
\]</div>
<p>As can be seen, the optimizer state memory has a divisor of <span class="math notranslate nohighlight">\(N_d\)</span> compared to the original.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 16.1 </span></p>
<section class="example-content" id="proof-content">
<p>For a 7.5B parameter model, the standard case requires 120 GB of memory, but using <span class="math notranslate nohighlight">\(P_{os}\)</span> with <span class="math notranslate nohighlight">\(N_d=64\)</span> only requires 31.4 GB of memory.
When <span class="math notranslate nohighlight">\(N_d\)</span> is very large, the memory consumption approaches:</p>
<div class="math notranslate nohighlight">
\[
(2+2) \Psi+\frac{K \times \Psi}{N_d} \approx 4 \Psi
\]</div>
<p>The ratio compared to the original:</p>
<div class="math notranslate nohighlight">
\[
\frac{4}{4+K}
\]</div>
<p>When <span class="math notranslate nohighlight">\(K=12\)</span>, this becomes <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span>, meaning the memory usage is <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span> of the original.</p>
</section>
</div><!-- 
## Flash Attention


### Online Softmax Motivation

A typical computation of Softmax 
$$a_i = \mathbf{Softmax} (x_i | x_1,...,x_N) = \frac{\exp(x_i)}{\sum_{j}^N \exp(x_j)}$$
within the self-attention module involves the following three steps:

1. Motivated by preventing overflow, we first find the maximum value $m$ of $\{x_1,...,x_N\}$
$$
\begin{aligned}
\text{for }  i & \leftarrow 1, N \text{ do} \\
m & = \max(m, x_i)
\end{aligned}
$$

2. Calculate the denominator of Softmax (with offset to $m$)
$$
\begin{aligned}
\text{for } i & \leftarrow 1, N \text{ do} \\
d_i & = d_{i-1} + \exp(x_i-m)
\end{aligned}
$$

3. Calculate the Softmax for each corresponding position

$$
\begin{aligned}
\text{for } i & \leftarrow 1, N \text{ do} \\
a_i & = \frac{\exp(x_i-m)}{d_N}
\end{aligned}
$$

Without any optimization, at least six communications with the GPU are required (three writes and three reads). If we apply some parallel partitioning to each step of the for loop, we would also need to add the communication costs of operations like reduce_sum and reduce_max. Is it possible to fuse certain operations to reduce communication? 

Based on previous experience with layernorm parallelization, we need to look for an Online Algorithm.

### Online Softmax Algorithm

Nvidia :cite:`milakov2018onlinenormalizercalculationsoftmax`

Since we're looking for an Online algorithm, we need to find a recursive expression.
For the second step, $d_i=d_{i-1}+e^{x_i-m_N}$, we aim to remove its dependency on $m_N$.
Let $d_i^{\prime}=\sum_j^i e^{x_j-m_i}$, note that here we subtract the current maximum instead of the global maximum. This expression has the following property:

$$
\begin{aligned}
d_i^{\prime} & =\sum_j^i e^{x_j-m_i} \\
& =\sum_j^{i-1} e^{x_j-m_i}+e^{x_i-m_i} \\
& =\sum_j^{i-1} e^{x_j-m_{i-1}+m_{i-1}-m_i}+e^{x_i-m_i} \\
& =\left(\sum_j^{i-1} e^{x_j-m_{i-1}}\right) e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
& =d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}
$$

We can see that the calculation of $d_i^{\prime}$ depends on $d_{i-1}^{\prime}, m_i, m_{i-1}$, allowing us to merge the first two steps. The previous three steps can be reduced to two:

Find the maximum value $m$ of $x$, calculate the denominator of softmax.

$$
\begin{aligned}
& \text { for } i \leftarrow 1, N \text { do } \\
& \quad m_i=\max \left(m_i, x_i\right) \\
& \quad d_i^{\prime}=d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}
$$

Calculate the softmax for each position

$$
\begin{aligned}
\text { for } i & \leftarrow 1, N \text { do } \\
a_i & =\frac{e^{x_i-m_N}}{d_N}
\end{aligned}
$$

Can we further fuse operators? Not really, because the denominator in the second step depends on the calculation from the first step.
However, we can use GPU shared memory to store intermediate results and implement the above two steps in a single kernel. This way, we only need to communicate with global memory twice: once to write data and once to read results.

### From Online Softmax To Flash Attention

Using a method similar to Online Softmax, we can place all Attention operations into a single for loop (implementable in one Kernel).
Let's first look at how it's calculated:

$$
\begin{aligned}
\text { for } i & \leftarrow 1, N \text { do } \\
a_i & =\frac{e^{x_i-m_N}}{d_N^{\prime}} \\
o_i & =\sum_j^i a_j v_j=\sum_j^i \frac{e^{x_j-m_N}}{d_N^{\prime}} v_j
\end{aligned}
$$

We can see that $o_i$ contains $m_N$ and $d_N^{\prime}$. We want to eliminate these dependencies. Similar to Online Softmax, we define:

$$
o_i^{\prime}=\sum_j^i \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j
$$

Let's find the recursive expression. The key is to identify $o_{i-1}^{\prime}$:

$$
\begin{aligned}
\sigma_i^{\prime} & =\sum_j^i \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j \\
& =\sum_j^{i-1} \frac{e^{x_j-m_i}}{d_i^{\prime}} v_j+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
& =\sum_j^{i-1} \frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}} \frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}} \frac{d_{i-1}^{\prime}}{d_i^{\prime}} v_j+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
& =\left(\sum_j^{i-1} \frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}} v_j\right) \frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}} \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i \\
& =o_{i-1}^{\prime}\left(e^{m_{i-1}-m_i}\right) \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i
\end{aligned}
$$

We can see that the calculation of $o_i$ depends on $o_{i-1}, m_i, m_{i-1}, d_i^{\prime}, d_{i-1}^{\prime}$. This allows us to place the entire Attention calculation into a single for loop:

$$
\begin{aligned}
\text { for } i & \leftarrow 1, N  \text{ do } \\
m_i & =\max \left(m_i, x_i\right) \\
d_i^{\prime} & =d_{i-1}^{\prime} e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
o_i^{\prime} & =o_{i-1}^{\prime}\left(e^{m_{i-1}-m_i}\right) \frac{d_{i-1}^{\prime}}{d_i^{\prime}}+\frac{e^{x_i-m_i}}{d_i^{\prime}} v_i
\end{aligned}
$$

From the above, we can see that all operations within the for loop satisfy the associative property. This means the for loop can be block-processed, allowing for more efficient parallel computation on GPUs.

 -->
</section>
</section>
<section id="appendix">
<h2><span class="section-number">16.5. </span>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h2>
<section id="floating-data-types">
<h3><span class="section-number">16.5.1. </span>Floating Data Types<a class="headerlink" href="#floating-data-types" title="Link to this heading">#</a></h3>
<p>Float32 (FP32) stands for the standardized IEEE 32-bit floating point representation. With this data type it is possible to represent a wide range of floating numbers. In FP32, 8 bits are reserved for the “exponent”, 23 bits for the “mantissa” and 1 bit for the sign of the number. In addition to that, most of the hardware supports FP32 operations and instructions.</p>
<p>In the float16 (FP16) data type, 5 bits are reserved for the exponent and 10 bits are reserved for the mantissa. This makes the representable range of FP16 numbers much lower than FP32. This exposes FP16 numbers to the risk of overflowing (trying to represent a number that is very large) and underflowing (representing a number that is very small).</p>
<p>For example, if you do 10k * 10k you end up with 100M which is not possible to represent in FP16, as the largest number possible is 64k. And thus you’d end up with NaN (Not a Number) result and if you have sequential computation like in neural networks, all the prior work is destroyed. Usually, loss scaling is used to overcome this issue, but it doesn’t always work well.</p>
<p>A new format, bfloat16 (BF16), was created to avoid these constraints. In BF16, 8 bits are reserved for the exponent (which is the same as in FP32) and 7 bits are reserved for the fraction.</p>
<p>This means that in BF16 we can retain the same dynamic range as FP32. But we lose 3 bits of precision with respect to FP16. Now there is absolutely no problem with huge numbers, but the precision is worse than FP16 here.</p>
<p>In the Ampere architecture, NVIDIA also introduced TensorFloat-32 (TF32) precision format, combining the dynamic range of BF16 and precision of FP16 to only use 19 bits. It’s currently only used internally during certain operations.</p>
<p>In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are referred to as half-precision (2 bytes). On top of that, the int8 (INT8) data type consists of an 8-bit representation that can store <span class="math notranslate nohighlight">\(2^8\)</span> different values (between [0, 255] or [-128, 127] for signed integers).</p>
<p>While, ideally the training and inference should be done in FP32, it is two times slower than FP16/BF16 and therefore a mixed precision approach is used where the weights are held in FP32 as a precise “main weights” reference, while computation in a forward and backward pass are done for FP16/BF16 to enhance training speed. The FP16/BF16 gradients are then used to update the FP32 main weights.</p>
<p>During training, the main weights are always stored in FP32, but in practice, the half-precision weights often provide similar quality during inference as their FP32 counterpart – a precise reference of the model is only needed when it receives multiple gradient updates. This means we can use the half-precision weights and use half the GPUs to accomplish the same outcome.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-different-precision-demo">
<a class="reference internal image-reference" href="../../_images/different_precision_demo.png"><img alt="../../_images/different_precision_demo.png" src="../../_images/different_precision_demo.png" style="width: 387.90000000000003px; height: 297.90000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.3 </span><span class="caption-text">Comparison of different float number types.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-different-precision-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="gpu-parallel-operations">
<h3><span class="section-number">16.5.2. </span>GPU Parallel Operations<a class="headerlink" href="#gpu-parallel-operations" title="Link to this heading">#</a></h3>
<p>References <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#:~:text=The%20ReduceScatter%20operation%20performs%20the%20same%20operation%20as,mapping%20since%20the%20ranks%20determine%20the%20data%20layout">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#:~:text=The ReduceScatter operation performs the same operation as,mapping since the ranks determine the data layout</a>.</p>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-broadcast">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_broadcast.png"><img alt="../../_images/gpu_parallel_operation_broadcast.png" src="../../_images/gpu_parallel_operation_broadcast.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.4 </span><span class="caption-text">Broadcast operation: data in one device is sent to all other devices.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-broadcast" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-scatter">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_scatter.png"><img alt="../../_images/gpu_parallel_operation_scatter.png" src="../../_images/gpu_parallel_operation_scatter.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.5 </span><span class="caption-text">Scatter operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-scatter" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-gather">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_gather.png"><img alt="../../_images/gpu_parallel_operation_gather.png" src="../../_images/gpu_parallel_operation_gather.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.6 </span><span class="caption-text">Gather operation: every device broadcasts their data patition to a designated devices. Eventually, this desigated device has the complete data.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-gather" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-reduce">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_reduce.png"><img alt="../../_images/gpu_parallel_operation_reduce.png" src="../../_images/gpu_parallel_operation_reduce.png" style="width: 801.0px; height: 199.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.7 </span><span class="caption-text">Reduce operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-reduce" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allgather">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allgather.png"><img alt="../../_images/gpu_parallel_operation_allgather.png" src="../../_images/gpu_parallel_operation_allgather.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.8 </span><span class="caption-text">AllGather operation: every device broadcasts their chuck of data to all other devices. Eventually, every device has a complete data copy.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allgather" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allgather-ring">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allgather_ring.png"><img alt="../../_images/gpu_parallel_operation_allgather_ring.png" src="../../_images/gpu_parallel_operation_allgather_ring.png" style="width: 639.0px; height: 459.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.9 </span><span class="caption-text">Communication efficient implementation for AllGather via ring style. Every device sends its chuck of data to the next device in the ring.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allgather-ring" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-reducescatter">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_reducescatter.png"><img alt="../../_images/gpu_parallel_operation_reducescatter.png" src="../../_images/gpu_parallel_operation_reducescatter.png" style="width: 800.25px; height: 198.75px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.10 </span><span class="caption-text">ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks across devices.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-reducescatter" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="chapter-training-fig-efficient-training-gpu-parallel-operation-allreduce">
<a class="reference internal image-reference" href="../../_images/gpu_parallel_operation_allreduce.png"><img alt="../../_images/gpu_parallel_operation_allreduce.png" src="../../_images/gpu_parallel_operation_allreduce.png" style="width: 799.5px; height: 198.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.11 </span><span class="caption-text">AllReduce operation.</span><a class="headerlink" href="#chapter-training-fig-efficient-training-gpu-parallel-operation-allreduce" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>A <strong>naive AllReduce</strong> implementation would be two steps:</p>
<ol class="arabic simple">
<li><p>Reduce: All devices first send their data to Rank0 device, and performing reduce operation on Rank0.</p></li>
<li><p>Broadcast: the reduce results are sent to all other devices.
This amounts to a total <span class="math notranslate nohighlight">\(2(N_d-1)\Phi\)</span> communication volume, in which the step 1 has <span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span> and step 2 has <span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span>. The naive implementation has the communication load imbalance issue as all data are sent into and sent out from Rank0 device.</p></li>
</ol>
<p>The <strong>RingAllReduce</strong> address the load imbalance issue by engaging all devices in data communication and reduction (i.e., more parallelism). The RingAllReduce is equivalent to first RingReduceScatter and then AllGather.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1561">
<caption><span class="caption-number">Table 16.2 </span><span class="caption-text">Communication volumne summary for different operations. Let <span class="math notranslate nohighlight">\(\Phi\)</span> be the total data size in one device and <span class="math notranslate nohighlight">\(N\)</span> be the total number of devices.</span><a class="headerlink" href="#id1561" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Type</p></th>
<th class="head text-left"><p>Storage Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Broadcast</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((N_d-1) \Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Scatter</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Reduce</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\((N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Gather</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>AllGather</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi \times N_d = (N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ReduceScatter</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{N_d-1}{N_d}\Phi \times N_d = (N_d-1)\Phi\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>AllReduce(Ring)</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(2(N_d-1) \Phi\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">16.6. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">CXZG16</a><span class="fn-bracket">]</span></span>
<p>Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. 2016. URL: <a class="reference external" href="https://arxiv.org/abs/1604.06174">https://arxiv.org/abs/1604.06174</a>, <a class="reference external" href="https://arxiv.org/abs/1604.06174">arXiv:1604.06174</a>.</p>
</div>
<div class="citation" id="id1526" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MNA+17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and others. Mixed precision training. <em>arXiv preprint arXiv:1710.03740</em>, 2017.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">RRRH20</a><span class="fn-bracket">]</span></span>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. 2020. URL: <a class="reference external" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a>, <a class="reference external" href="https://arxiv.org/abs/1910.02054">arXiv:1910.02054</a>.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">YYHBZ24</a><span class="fn-bracket">]</span></span>
<p>An Yang, Baosong Yang, Binyuan Hui, and et al. Bo Zheng. Qwen2 technical report. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.10671">https://arxiv.org/abs/2407.10671</a>, <a class="reference external" href="https://arxiv.org/abs/2407.10671">arXiv:2407.10671</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="reinforcement_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">15. </span>*Reinforcement Learning Essentials</p>
      </div>
    </a>
    <a class="right-next"
       href="../notebooks/chapter_LLM_training/annotated_pretraining.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>*Lab: LLM Pretraining</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-requirement-for-training-llm">16.1. The Memory Requirement For Training LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-optimizer-states">16.1.1. Model and Optimizer States</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">16.1.2. Activations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-checkpointing-techniques">16.1.3. Activation Checkpointing Techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-training">16.2. Mixed Precision Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">16.2.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">16.2.2. Training Process</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-parallel-training">16.3. Distributed Parallel Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-parallel-training-techniques">16.3.1. Overview of parallel training techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelism-tensor-parallelism">16.3.2. Model parallelism (tensor parallelism)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-via-deepspeed">16.4. ZeRO Via DeepSpeed</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-allocation">16.4.1. GPU Memory Allocation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-stage-one">16.4.2. ZeRO-Stage-One</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">16.5. Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-data-types">16.5.1. Floating Data Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-parallel-operations">16.5.2. GPU Parallel Operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">16.6. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>