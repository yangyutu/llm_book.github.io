
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>13. LLM Alignement and Preference learning &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_training/alignment';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="14. LLM Training Acceleration" href="accelerated_training.html" />
    <link rel="prev" title="12. LLM Finetuning" href="finetuning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training_fundamentals.html">11. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">12. LLM Finetuning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. LLM Alignement and Preference learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerated_training.html">14. LLM Training Acceleration</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">16. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">17. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting and RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">18. Basic prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">19. Advanced prompt techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">20. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">21. Information Retrieval Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">22. Application of LLM in IR</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Alignement and Preference learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">13.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-via-ppo">13.2. RLHF via PPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-methodology">13.2.1. Overall methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sft">13.2.2. SFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-collection">13.2.3. Preference data collection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">13.2.4. Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">13.2.5. Reinforcement learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ppo-algorithm">13.2.6. The PPO algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-mdp">13.2.7. Preliminary: MDP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">13.3. DPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-preference-modeling">13.3.1. Preliminary: Preference modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#driving-the-dpo">13.3.2. Driving the DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-variants">13.4. DPO variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-preference-label">13.4.1. Smoothing preference label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-dpo">13.4.2. Simple DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-remark-rl-vs-sft-vs-dpo">13.4.3. Additional remark RL vs SFT vs DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">13.5. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llm-alignement-and-preference-learning">
<h1><span class="section-number">13. </span>LLM Alignement and Preference learning<a class="headerlink" href="#llm-alignement-and-preference-learning" title="Link to this heading">#</a></h1>
<section id="motivation">
<h2><span class="section-number">13.1. </span>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>The objective function in LLM pretraining is predicting the next token on a webpage from the internet. When the trained model is properly and carefully prompted as demonstration (i.e., in-context learning as in GPT-3), the model can largely accomplish useful tasks by following these demonstrations. However, these model can often generate un-desired outputs, including un-factual content, biased and harmful text, or simply do not follow the instructions in the prompt.</p>
<p>This is because the pretraining task of <em>predicting the next token</em> is inherently different from the objective training an LLM to be an instruction-following assistant that avoids generating unintended text. While continuing SFT instruction tuning data, which are (prompt, completion) pairs, can expose the LLM to what humans like to see for given prompts, it is often not enough to prevent model from producing unintended texts. Instead, we need a training methodology to explictly reward the model when it is well-behaved and penalize the model when it is mis-behaved. Training the model to learn the human preference using rewards and penalities are the core of LLM alignment and preference learning. The pioneering approach is using reinforcement learning via the PPO algorithm <span id="id1">[<a class="reference internal" href="#id1505" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</p>
<p>As shown in the <a class="reference internal" href="#chapter-training-fig-alignment-model-alignment-motivation"><span class="std std-numref">Fig. 13.1</span></a>, SFT on instruction tuning dataset and reinforcement learning via PPO can improve model helpfulness and instruction following abilities.</p>
<figure class="align-default" id="chapter-training-fig-alignment-model-alignment-motivation">
<a class="reference internal image-reference" href="../../_images/model_alignment_motivation.png"><img alt="../../_images/model_alignment_motivation.png" src="../../_images/model_alignment_motivation.png" style="width: 556.8000000000001px; height: 324.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">Human evaluations of various models outputs show that how often outputs from each model were preferred to those from the 175B GPT-3 SFT model. The aligned models InstructGPT models (PPO-ptx) as well as variant (PPO) significantly outperform the GPT-3 baselines. Image from <span id="id2">[<a class="reference internal" href="#id1505" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignment-model-alignment-motivation" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="rlhf-via-ppo">
<h2><span class="section-number">13.2. </span>RLHF via PPO<a class="headerlink" href="#rlhf-via-ppo" title="Link to this heading">#</a></h2>
<section id="overall-methodology">
<h3><span class="section-number">13.2.1. </span>Overall methodology<a class="headerlink" href="#overall-methodology" title="Link to this heading">#</a></h3>
<p>Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains. We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers (see Sections 3.4 for details). We then apply the following three steps (Figure 2).</p>
<p><strong>Step 1: SFT on demeonstration data</strong>: Collect demonstration data, and train a supervised policy. Our labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning.</p>
<p><strong>Step 2: Preference labeling and reward modeling</strong>: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input. We then train a reward model to predict the human-preferred output.</p>
<p><strong>Step 3 Optimize policy with reward model</strong>: Optimize a policy against the reward model using PPO. We use the output of the RM as a scalar reward. We fine-tune the supervised policy to optimize this reward using the PPO algorithm (Schulman et al., 2017).</p>
<p>Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy. In practice, most of our comparison data comes from our supervised policies, with some coming from our PPO policies.</p>
<p><span id="id3">[<a class="reference internal" href="#id1505" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span></p>
<figure class="align-default" id="chapter-training-fig-alignment-rlhf-demo">
<a class="reference internal image-reference" href="../../_images/RLHF_demo.png"><img alt="../../_images/RLHF_demo.png" src="../../_images/RLHF_demo.png" style="width: 699.5px; height: 413.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.2 </span><span class="caption-text">A diagram illustrating the three steps of our method: (1) supervised fine-tuning (SFT), (2)
reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO)
on this reward model. Image from <span id="id4">[<a class="reference internal" href="#id1505" title="Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: https://arxiv.org/abs/2203.02155, arXiv:2203.02155.">OWJ+22</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignment-rlhf-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="sft">
<h3><span class="section-number">13.2.2. </span>SFT<a class="headerlink" href="#sft" title="Link to this heading">#</a></h3>
</section>
<section id="preference-data-collection">
<h3><span class="section-number">13.2.3. </span>Preference data collection<a class="headerlink" href="#preference-data-collection" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table" id="id1531">
<caption><span class="caption-number">Table 13.1 </span><span class="caption-text">How labeler evaluates the response quality</span><a class="headerlink" href="#id1531" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metadata</p></th>
<th class="head text-right"><p>Scale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Overall quality</p></td>
<td class="text-right"><p>Likert scale; 1-7</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Fails to follow the correct instruction / task</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Inappropriate for customer assistant</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Hallucination</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Satisifies constraint provided in the instruction</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Contains sexual content</p></td>
<td class="text-right"><p>Binary</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>…</p></td>
<td class="text-right"><p>…</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="reward-modeling">
<h3><span class="section-number">13.2.4. </span>Reward modeling<a class="headerlink" href="#reward-modeling" title="Link to this heading">#</a></h3>
<p>The objective of reward modeling is to train a model that take prompt <span class="math notranslate nohighlight">\(x\)</span> and one completion <span class="math notranslate nohighlight">\(y\)</span> as input and output a scalar score that align with human preference.
More specificlly, let <span class="math notranslate nohighlight">\(r(x, y)\)</span> be the model’s scalar output, we have</p>
<div class="math notranslate nohighlight">
\[r(x, y_w) &gt; r(x, y_l) ~\text{if} ~ y_w \succ w_l\]</div>
<p>where <span class="math notranslate nohighlight">\(y_w\)</span> and <span class="math notranslate nohighlight">\(y_l\)</span> are two completions of prompt <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(y_w\)</span> is the preferred completion compared to <span class="math notranslate nohighlight">\(y_l\)</span>.</p>
<p>Specifically, the loss function for the reward model (parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>) is given by:</p>
<div class="math notranslate nohighlight">
\[
L_\theta=-\frac{1}{\binom{K}{2}} E_{\left(x, y_w, y_l\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)\right]
\]</div>
<p>Starting from the SFT model with the final unembedding layer removed,
we trained a model to take in a prompt and response, and output a scalar reward. In this paper we
only use 6B RMs</p>
<p>Tends to overfitting to highly scored completions.</p>
<p>Instead, we train on all <span class="math notranslate nohighlight">\(\binom{K}{2}\)</span> comparisons from each prompt as a single batch element. This is much more computationally efficient because it only requires a single forward pass of the RM for each completion (rather than <span class="math notranslate nohighlight">\(\binom{K}{2}\)</span> forward passes for <span class="math notranslate nohighlight">\(K\)</span> completions) and, because it no longer overfits, it achieves much improved validation accuracy and log loss.</p>
<p>每一个输入token最终都能够生成一个标量值。对于LLM来说，最后一个输入token的处理结果会采样变成next_token，现在变成了score，作为所有输入token的打分结果（其实也可以取所有token生成的score进行平均，通常是直接取最后一个score，训练的效果更好一些）。
预训练好的Reward模型可以参考：<a class="reference external" href="https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward">https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward</a></p>
</section>
<section id="reinforcement-learning">
<h3><span class="section-number">13.2.5. </span>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading">#</a></h3>
<p>Given the prompt and response, it produces a reward determined by the reward model and ends the episode.
In addition, we add a per-token KL penalty from the SFT model at each token to mitigate over-optimization of the reward model.</p>
<p>We <strong>maximize</strong> the following objective function in the PPO RL training:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}_{\text{PPO}}(\phi)=  E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]
\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi_\phi^{\mathrm{RL}}\)</span> is the learned RL policy, <span class="math notranslate nohighlight">\(\pi^{\mathrm{SFT}}\)</span> is the supervised trained model</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{\text {pretrain }}\)</span> is the pretraining distribution. The KL reward coefficient, <span class="math notranslate nohighlight">\(\beta\)</span>, and the pretraining loss coefficient, <span class="math notranslate nohighlight">\(\gamma\)</span>, control the strength of the KL penalty and pretraining gradients respectively.</p></li>
</ul>
<p>如果仅仅使用reward loss，容易导致模型在偏好数据上过拟合，损失原本的能力。因此增加了和原始模型输出的KL损失（KL越小越好）</p>
<p>To prevent language modeling performance regression, we can add an auxillary objective to maximize the likelihood on texts sampled from pretraining datasets. The final objective, named <strong>PPO-ptx</strong>, is given by</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Objective}_{\text{PPO-ptx}}(\phi)= \operatorname{Objective}_{\text{PPO}}(\phi) + \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{\text {pretrain }}\)</span> is the pretraining distribution.</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../../_images/RLHF_PPO_training_demo.png"><img alt="../../_images/RLHF_PPO_training_demo.png" src="../../_images/RLHF_PPO_training_demo.png" style="width: 464.5px; height: 234.5px;" />
</a>
</figure>
</section>
<section id="the-ppo-algorithm">
<h3><span class="section-number">13.2.6. </span>The PPO algorithm<a class="headerlink" href="#the-ppo-algorithm" title="Link to this heading">#</a></h3>
<p>There are four models needed in the PPO algorithm:</p>
<ul class="simple">
<li><p>Actor LLM</p></li>
<li><p>Frozen Actor LLM</p></li>
<li><p>Value function model</p></li>
<li><p>Frozen Reward model</p></li>
</ul>
</section>
<section id="preliminary-mdp">
<h3><span class="section-number">13.2.7. </span>Preliminary: MDP<a class="headerlink" href="#preliminary-mdp" title="Link to this heading">#</a></h3>
<p>Each environment is an NLP task: we are given a supervised dataset <span class="math notranslate nohighlight">\(\mathcal{D}=\left\{\left(\boldsymbol{x}^i, \boldsymbol{y}^i\right)\right\}_{i=1}^N\)</span> of <span class="math notranslate nohighlight">\(N\)</span> examples, where <span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathcal{X}\)</span> is an language input and <span class="math notranslate nohighlight">\(\boldsymbol{y} \in \mathcal{Y}\)</span> is the target string.</p>
<p>Generation can be viewed as a Markov Decision Process (MDP) <span class="math notranslate nohighlight">\(\langle\mathcal{S}, \mathcal{A}, \mathcal{R}, P, \gamma, T\rangle\)</span> using a finite vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
<p>Each episode in the MDP begins by sampling a datapoint <span class="math notranslate nohighlight">\((\boldsymbol{x}, \boldsymbol{y})\)</span> from our dataset and ends when the current time step <span class="math notranslate nohighlight">\(t\)</span> exceeds the horizon <span class="math notranslate nohighlight">\(T\)</span> or an end of sentence (EOS) token is generated.</p>
<ul class="simple">
<li><p>The initial state <span class="math notranslate nohighlight">\(\boldsymbol{s}_0 \in \mathcal{S}\)</span> is a task-specific prompt represented by <span class="math notranslate nohighlight">\(\boldsymbol{x}=\left(x_0, \cdots, x_m\right)\)</span>. That is, <span class="math notranslate nohighlight">\(\boldsymbol{s}_0 = \boldsymbol{x}\)</span>.</p></li>
<li><p>An action in the environment <span class="math notranslate nohighlight">\(a_t \in \mathcal{A}\)</span> consists of a token from our vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p>The transition function <span class="math notranslate nohighlight">\(P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span> deterministically appends an action <span class="math notranslate nohighlight">\(a_t\)</span> to the end of the state <span class="math notranslate nohighlight">\(s_{t-1}=\left(x_0, \cdots, x_m, a_0, \cdots, a_{t-1}\right)\)</span>.</p></li>
<li><p>At the end of an episode a reward <span class="math notranslate nohighlight">\(\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}^1\)</span> is provided by the reward model.</p></li>
</ul>
<p>The input <span class="math notranslate nohighlight">\(\boldsymbol{x}=\left(x_0, \cdots, x_m\right)\)</span> is a task-specific prompt that is used as our initial state <span class="math notranslate nohighlight">\(\boldsymbol{s}_0=\left(x_0, \cdots, x_m\right)\)</span>, where <span class="math notranslate nohighlight">\(s_0 \in \mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the state space with <span class="math notranslate nohighlight">\(x_m \in \mathcal{V}\)</span>.  The transition function <span class="math notranslate nohighlight">\(P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span> deterministically appends an action <span class="math notranslate nohighlight">\(a_t\)</span> to the end of the state <span class="math notranslate nohighlight">\(s_{t-1}=\left(x_0, \cdots, x_m, a_0, \cdots, a_{t-1}\right)\)</span>. This continues until the end of the horizon <span class="math notranslate nohighlight">\(t \leq T\)</span> and we obtain a state <span class="math notranslate nohighlight">\(s_T=\left(x_0, \cdots, x_m, a_0, \cdots, a_T\right)\)</span>. At the end of an episode a reward <span class="math notranslate nohighlight">\(\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{Y} \rightarrow \mathbb{R}^1\)</span> that depends on the <span class="math notranslate nohighlight">\(\left(s_T, \boldsymbol{y}\right)\)</span> (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style</p>
</section>
</section>
<section id="dpo">
<h2><span class="section-number">13.3. </span>DPO<a class="headerlink" href="#dpo" title="Link to this heading">#</a></h2>
<p>DPO (Direct Preference Optimization) <span id="id6">[<a class="reference internal" href="#id1504" title="Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: https://arxiv.org/abs/2305.18290, arXiv:2305.18290.">RSM+24</a>]</span> improves the classical RLHF-PPO algorithm from the following two aspects:</p>
<ul class="simple">
<li><p>Reward model is no longer need; Instead, preference data is directly used to train an aligned model in one step.</p></li>
<li><p>Reinforcement learning is simplified .
It no longer uses reinforcement learning methods. Through mathematical reasoning, it simplifies the original preference alignment objective step by step, and finally trains an aligned model using simpler steps similar to SFT (Supervised Fine-Tuning).</p></li>
</ul>
<p>The following illustrates from the DPO paper to visually compare the differences between RLHF-PPO and DPO</p>
<figure class="align-default" id="chapter-training-fig-alignmenet-dpo-ppo-comparison">
<a class="reference internal image-reference" href="../../_images/DPO_PPO_comparison.png"><img alt="../../_images/DPO_PPO_comparison.png" src="../../_images/DPO_PPO_comparison.png" style="width: 814.95px; height: 167.85px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.3 </span><span class="caption-text">DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and
human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.
In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification
objective. Image from <span id="id7">[<a class="reference internal" href="#id1504" title="Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: https://arxiv.org/abs/2305.18290, arXiv:2305.18290.">RSM+24</a>]</span>.</span><a class="headerlink" href="#chapter-training-fig-alignmenet-dpo-ppo-comparison" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>DPO pipelines</p>
<ul class="simple">
<li><p>Sample completions <span class="math notranslate nohighlight">\(y_1, y_2 \sim \pi_{\text {ref }}(\cdot \mid x)\)</span> for every prompt <span class="math notranslate nohighlight">\(x\)</span>, label with human preferences to construct the offline dataset of preferences <span class="math notranslate nohighlight">\(\left.\mathcal{D}=\left\{x^{(i)}, y_w^{(i)}, y_l\right)^{(i)}\right\}_{i=1}^N\)</span></p></li>
<li><p>optimize the language model <span class="math notranslate nohighlight">\(\pi_\theta\)</span> to minimize <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{DPO}}\)</span> for the given <span class="math notranslate nohighlight">\(\pi_{\text {ref }}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and desired <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
</ul>
<p>Note: Since the preference datasets are sampled using <span class="math notranslate nohighlight">\(\pi^{\mathrm{SFT}}\)</span>, we initialize <span class="math notranslate nohighlight">\(\pi_{\text {ref }}=\pi^{\mathrm{SFT}}\)</span> whenever available.</p>
<section id="preliminary-preference-modeling">
<h3><span class="section-number">13.3.1. </span>Preliminary: Preference modeling<a class="headerlink" href="#preliminary-preference-modeling" title="Link to this heading">#</a></h3>
<p>The Bradley-Terry model <span id="id8">[<a class="reference internal" href="#id1507" title="Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: i. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. URL: http://www.jstor.org/stable/2334029 (visited on 2024-09-20).">BT52</a>]</span> is a probability model for the outcome of pairwise comparisons between items, teams, or objects. Given a pair of items <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> drawn from some population, it estimates the probability that the pairwise comparison <span class="math notranslate nohighlight">\(i&gt;j\)</span> turns out true, as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Pr}(i\succ j)=\frac{s_i}{s_i+s_j}
\]</div>
<p>where <span class="math notranslate nohighlight">\(s_i\)</span> is a positive real-valued score assigned to individual <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="math notranslate nohighlight">
\[
P\left(y_1 \succ y_2 \mid x\right)=\frac{\exp \left[r\left(x, y_1\right)\right]}{\exp \left[r\left(x, y_1\right)\right]+\exp \left[r\left(x, y_2\right)\right]}\]</div>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 13.1 </span> (Relationship to logistic regression)</p>
<section class="remark-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\operatorname{logit} \operatorname{Pr}(i&gt;j)=\log \frac{\operatorname{Pr}(i&gt;j)}{1-\operatorname{Pr}(i&gt;j)}=\log \frac{\operatorname{Pr}(i&gt;j)}{\operatorname{Pr}(j&gt;i)}=\beta_i-\beta_j
\]</div>
</section>
</div></section>
<section id="driving-the-dpo">
<h3><span class="section-number">13.3.2. </span>Driving the DPO<a class="headerlink" href="#driving-the-dpo" title="Link to this heading">#</a></h3>
<p>Here we outline the key steps to derive the DPO objective function.</p>
<p>First we start with the objective of LLM alignment with a given <strong>fixed reward function</strong> <span class="math notranslate nohighlight">\(r\)</span> with a KL constraint,</p>
<div class="math notranslate nohighlight">
\[
\max _\pi \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi}[r(x, y)]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi(y \mid x) \| \pi_{\text {ref }}(y \mid x)\right].
\]</div>
<p>It turns out that we can obtain the theoretical solution of <span class="math notranslate nohighlight">\(\pi_r(y|x)\)</span> given by</p>
<div class="math notranslate nohighlight">
\[\pi_r(y|x) = \frac{1}{Z(x)}_{\text{ref}}(y|x) \exp(\frac{1}{\beta}r(x, y)),\]</div>
<p>where <span class="math notranslate nohighlight">\(Z(x)\)</span> is partition funciton dependent only on <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\pi_{\text{ref}}\)</span>.</p>
<p>With some algebra, we can also represent the reward funciton with <span class="math notranslate nohighlight">\(\pi_r(y|x)\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[r(x, y) = \beta \log \frac{\pi_r (y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z.\]</div>
<p>Note that we have just shown that that reward function <span class="math notranslate nohighlight">\(r(x, y)\)</span> and its corresponding optimal policy <span class="math notranslate nohighlight">\(\pi_{\text{ref}}(y|x)\)</span> are inter-convertable, with a funciton <span class="math notranslate nohighlight">\(Z(x)\)</span> independent of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>This means that instead of numerically optimizing policy <span class="math notranslate nohighlight">\(\pi_r\)</span>, we can also choose optimize the reward function. Given the available preference data, one formulation to optimize the reward function is the Bradley-Terry (BT) objective, that is</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{BT} = - -\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma (r(y_w, x) - r(r_l, x)) \right].\]</div>
<p>By leveraging the relationship between reward <span class="math notranslate nohighlight">\(r\)</span> and policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can arrive at the DPO loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right].
\]</div>
<p>where the terms <span class="math notranslate nohighlight">\(\beta \log Z\)</span> are canceled during subtraction.</p>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13.2 </span> (How DPO loss work)</p>
<section class="remark-content" id="proof-content">
<p>The gradient of DPO loss function is given by:</p>
<div class="math notranslate nohighlight">
\[\nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_w, y_l\right)=-\left(1-\hat{p}_\theta\right) [\underbrace{\nabla_\theta \log \pi_\theta\left(y_w\right)}_{\text {upweight } y_w}-\underbrace{\nabla_\theta \log \pi_\theta\left(y_l\right)}_{\text {downweight } y_l}]
\]</div>
<p>where for the preference completion pair <span class="math notranslate nohighlight">\(y_w \succ y_l\)</span>, as long as <span class="math notranslate nohighlight">\(\hat{p} &lt; 1\)</span>, there will gradients to upweight the probability of generating <span class="math notranslate nohighlight">\(y_w\)</span> and downweight the probability of generating <span class="math notranslate nohighlight">\(y_l\)</span>.</p>
</section>
</div></section>
</section>
<section id="dpo-variants">
<h2><span class="section-number">13.4. </span>DPO variants<a class="headerlink" href="#dpo-variants" title="Link to this heading">#</a></h2>
<section id="smoothing-preference-label">
<h3><span class="section-number">13.4.1. </span>Smoothing preference label<a class="headerlink" href="#smoothing-preference-label" title="Link to this heading">#</a></h3>
<p><span id="id9">[<a class="reference internal" href="#id1506" title="Eric Mitchell. A note on dpo with noisy preferences &amp; relationship to ipo. 2023. URL: https://ericmitchell.ai/cdpo.pdf.">Mit23</a>]</span>
What if preference labels are noisy? Say the labels have been flipped with some small probability <span class="math notranslate nohighlight">\(\epsilon \in(0,0.5)\)</span>. We can use a conservative target distribution instead, <span class="math notranslate nohighlight">\(p\left(y_w \succ y_l\right)=1-\epsilon\)</span>, giving BCE loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right) &amp; =-(1-\epsilon) \log \hat{p}_\theta\left(y_w \succ y_l\right)-\epsilon \log \left(1-\hat{p}_\theta\left(y_w \succ y_l\right)\right) \\
&amp; =(1-\epsilon) \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_w, y_l\right)+\epsilon \mathcal{L}_{\mathrm{DPO}}\left(\theta, y_l, y_w\right)
\end{aligned}
\end{split}\]</div>
<p>The gradient of <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right)\)</span> is simply the weighted sum of gradients <span class="math notranslate nohighlight">\((1-\epsilon) \nabla_\theta \mathcal{L}\left(\theta, y_w, y_l\right)+\epsilon \nabla_\theta \mathcal{L}\left(\theta, y_l, y_w\right)\)</span>, which reduces to the simplified form (ignoring constants; see [3] for the gradient of the original DPO loss):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}^\epsilon\left(\theta, y_w, y_l\right) &amp; =-\left((1-\epsilon)\left(1-\hat{p}_\theta\right)-\epsilon \hat{p}_\theta\right)[\underbrace{\nabla_\theta \log \pi_\theta\left(y_w\right)}_{\text {upweight } y_w}-\underbrace{\nabla_\theta \log \pi_\theta\left(y_l\right)}_{\text {downweight } y_l}] \\
&amp; =\quad\left(\hat{p}_\theta-(1-\epsilon)\right)\left[\nabla_\theta \log \pi_\theta\left(y_w\right)-\nabla_\theta \log \pi_\theta\left(y_l\right)\right]
\end{aligned}
\end{split}\]</div>
<p>The gradient is zero when <span class="math notranslate nohighlight">\(\hat{p}_\theta\left(y_w \succ y_l\right)=(1-\epsilon)\)</span>, i.e., our (implicit) reward assigns the desired confidence level in this training example under the Bradley-Terry model [2]. For normal DPO, the gradient is never zero! Using the shorthand <span class="math notranslate nohighlight">\(h_{\pi_\theta}^{y_w, y_l}=\log \frac{\pi_\theta\left(y_w\right)}{\pi_{\text {ref }}\left(y_w\right)}-\log \frac{\pi_\theta\left(y_l\right)}{\pi_{\text {ref }}\left(y_l\right)}\)</span>, let’s compare the conservative DPO (cDPO?)</p>
</section>
<section id="simple-dpo">
<h3><span class="section-number">13.4.2. </span>Simple DPO<a class="headerlink" href="#simple-dpo" title="Link to this heading">#</a></h3>
<p><span id="id10">[<a class="reference internal" href="#id1508" title="Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: simple preference optimization with a reference-free reward. 2024. URL: https://arxiv.org/abs/2405.14734, arXiv:2405.14734.">MXC24</a>]</span></p>
</section>
<section id="additional-remark-rl-vs-sft-vs-dpo">
<h3><span class="section-number">13.4.3. </span>Additional remark RL vs SFT vs DPO<a class="headerlink" href="#additional-remark-rl-vs-sft-vs-dpo" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="bibliography">
<h2><span class="section-number">13.5. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id11">
<div role="list" class="citation-list">
<div class="citation" id="id1507" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">BT52</a><span class="fn-bracket">]</span></span>
<p>Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: i. the method of paired comparisons. <em>Biometrika</em>, 39(3/4):324–345, 1952. URL: <a class="reference external" href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a> (visited on 2024-09-20).</p>
</div>
<div class="citation" id="id1508" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">MXC24</a><span class="fn-bracket">]</span></span>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: simple preference optimization with a reference-free reward. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2405.14734">https://arxiv.org/abs/2405.14734</a>, <a class="reference external" href="https://arxiv.org/abs/2405.14734">arXiv:2405.14734</a>.</p>
</div>
<div class="citation" id="id1506" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Mit23</a><span class="fn-bracket">]</span></span>
<p>Eric Mitchell. A note on dpo with noisy preferences &amp; relationship to ipo. 2023. URL: <a class="reference external" href="https://ericmitchell.ai/cdpo.pdf">https://ericmitchell.ai/cdpo.pdf</a>.</p>
</div>
<div class="citation" id="id1505" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>OWJ+22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>,<a role="doc-backlink" href="#id4">4</a>)</span>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>, <a class="reference external" href="https://arxiv.org/abs/2203.02155">arXiv:2203.02155</a>.</p>
</div>
<div class="citation" id="id1504" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RSM+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly a reward model. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>, <a class="reference external" href="https://arxiv.org/abs/2305.18290">arXiv:2305.18290</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="finetuning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>LLM Finetuning</p>
      </div>
    </a>
    <a class="right-next"
       href="accelerated_training.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>LLM Training Acceleration</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">13.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rlhf-via-ppo">13.2. RLHF via PPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-methodology">13.2.1. Overall methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sft">13.2.2. SFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preference-data-collection">13.2.3. Preference data collection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">13.2.4. Reward modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">13.2.5. Reinforcement learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ppo-algorithm">13.2.6. The PPO algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-mdp">13.2.7. Preliminary: MDP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo">13.3. DPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-preference-modeling">13.3.1. Preliminary: Preference modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#driving-the-dpo">13.3.2. Driving the DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpo-variants">13.4. DPO variants</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothing-preference-label">13.4.1. Smoothing preference label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-dpo">13.4.2. Simple DPO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-remark-rl-vs-sft-vs-dpo">13.4.3. Additional remark RL vs SFT vs DPO</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">13.5. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>