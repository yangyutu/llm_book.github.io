
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>26. Vision Language Pretraining &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_multimodality/vision_transformers';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="27. Information Retrieval and Text Ranking" href="../chapter_application_IR/information_retrieval_fundamentals.html" />
    <link rel="prev" title="25. Advanced RAG (WIP)" href="../chapter_rag/advanced_rag.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">10. MoE Sparse Architectures (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_arch/annotated_llama_custom.html">11. *Lab: Minimal LLama</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">12. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">13. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">14. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">15. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">16. LLM Training Acceleration (WIP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_pretraining.html">17. *Lab: LLM Pretraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_llama_custom_for_finetuning.html">18. *Lab: Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_training/annotated_llama_custom_for_DPO.html">19. *Lab: DPO Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">20. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">21. Inference Acceleration (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/basic_prompt.html">22. Basic Prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_prompt/advanced_prompt.html">23. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">24. RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/advanced_rag.html">25. Advanced RAG (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Vision LLM</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">26. Vision Language Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">27. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">28. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Vision Language Pretraining</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clip">26.1. CLIP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">26.1.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-implementations">26.1.2. Detailed Implementations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blip">26.2. BLIP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">26.2.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">26.2.2. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-strategy">26.2.3. Training Strategy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downstream-application">26.2.4. Downstream Application</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blip-2">26.3. BLIP-2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">26.3.1. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">26.3.2. Training Strategy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instruct-blip">26.4. Instruct BLIP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">26.5. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="vision-language-pretraining">
<h1><span class="section-number">26. </span>Vision Language Pretraining<a class="headerlink" href="#vision-language-pretraining" title="Link to this heading">#</a></h1>
<section id="clip">
<h2><span class="section-number">26.1. </span>CLIP<a class="headerlink" href="#clip" title="Link to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">26.1.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h3>
<p>CLIP, which stands for “Contrastive Language-Image Pre-training,” is a multimodal model that uses contrastive learning. CLIP is trained on text-image pairs with the goal of teaching the model to learn matching relationships between text and images via contrastive learning [<a class="reference internal" href="#chapter-vllm-visionlanguagepretrain-clip-method"><span class="std std-numref">Fig. 26.1</span></a>]. The model consists of two parts: a <strong>text encoder</strong> and an <strong>image encoder</strong>, which are used to extract text and image features and produce semantic vectors.</p>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-clip-method">
<a class="reference internal image-reference" href="../../_images/CLIP_method.png"><img alt="../../_images/CLIP_method.png" src="../../_images/CLIP_method.png" style="width: 500.4px; height: 363.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.1 </span><span class="caption-text">Using contrastive learning, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training
examples. Image from <span id="id1">[<a class="reference internal" href="#id458" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748–8763. PMLR, 2021.">RKH+21</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-clip-method" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The trained text encode and image encodder can be directly used in zero-shot application setting, as shown in <a class="reference internal" href="#chapter-vllm-visionlanguagepretrain-clip-zero-shot-prediction"><span class="std std-numref">Fig. 26.2</span></a>.</p>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-clip-zero-shot-prediction">
<a class="reference internal image-reference" href="../../_images/CLIP_zero_shot_prediction.png"><img alt="../../_images/CLIP_zero_shot_prediction.png" src="../../_images/CLIP_zero_shot_prediction.png" style="width: 502.79999999999995px; height: 363.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.2 </span><span class="caption-text">At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the
target dataset’s classes. Image from <span id="id2">[<a class="reference internal" href="#id458" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748–8763. PMLR, 2021.">RKH+21</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-clip-zero-shot-prediction" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The key conclusion is that CLIP model learns from natural language supervision and is capable of performing various tasks during pre-training, such as OCR, geo-localization, and action recognition, while demonstrating competitive performance in zero-shot transfer tasks.</p>
</section>
<section id="detailed-implementations">
<h3><span class="section-number">26.1.2. </span>Detailed Implementations<a class="headerlink" href="#detailed-implementations" title="Link to this heading">#</a></h3>
<p>In the original study, two architectures were considered for the image encoder:</p>
<ul class="simple">
<li><p><strong>ResNet</strong> (such as ResNet-50, ResNet-101, etc., with modifications like ResNetD improvements, pooling, attention pooling)</p></li>
<li><p><strong>Vision Transformer (ViT)</strong>; the text encoder uses a Transformer architecture.</p></li>
</ul>
<p>A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. At the then time of this research, the high-quality image-text parallel data available is on the scale ~10 million. CLIP employed the following data collection strategy:
They created a new dataset called WIT (WebImageText), containing 400M image-text pairs. The data was collected by searching for text-image pairs containing 500,000 queries (the query list was based on frequent words from English Wikipedia, high-traffic article names, two-word phrases, and WordNet synonym sets), ensuring the data covers a wide range of visual concepts. Each query included up to 20,000 image-text pairs, with the final dataset having a total word count comparable to the WebText dataset used to train GPT-2.</p>
<p>Given a mini-batch of <span class="math notranslate nohighlight">\(N\)</span> image-text pairs whose embeddings are <span class="math notranslate nohighlight">\(\{e_{I,1}, e_{I,2},..., e_{I,N}\}\)</span> and <span class="math notranslate nohighlight">\(\{e_{T,1}, e_{T,2},..., e_{T,N}\}\)</span>. The contrastive loss is given by</p>
<div class="math notranslate nohighlight">
\[L_{CLIP} = -\sum_{i=1}^N \left(\log \frac{\exp(\operatorname{CosSim}(e_{I,i},e_{T,i})/\tau)}{\sum_{j=1}^N \exp(\operatorname{CosSim}(e_{I,i},e_{T,j})/\tau)} + \log \frac{\exp(\operatorname{CosSim}(e_{T,i},e_{T,i})/\tau)}{\sum_{j=1}^N \exp(\operatorname{CosSim}(e_{T,i},e_{I,j})/\tau)}\right).\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> is the temperature coefficient.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># image_encoder - ResNet or Vision Transformer</span>
<span class="c1"># text_encoder - CBOW or Text Transformer</span>
<span class="c1"># I[n, h, w, c] - minibatch of aligned images</span>
<span class="c1"># T[n, l] - minibatch of aligned texts</span>
<span class="c1"># W_i[d_i, d_e] - learned proj of image to embed</span>
<span class="c1"># W_t[d_t, d_e] - learned proj of text to embed</span>
<span class="c1"># t - learned temperature parameter</span>
<span class="c1"># extract feature representations of each modality</span>
<span class="n">I_f</span> <span class="o">=</span> <span class="n">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]</span>
<span class="n">T_f</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]</span>
<span class="c1"># joint multimodal embedding [n, d_e]</span>
<span class="n">I_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">T_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># scaled pairwise cosine similarities [n, n]</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="c1"># symmetric loss function</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">loss_i</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss_t</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</pre></div>
</div>
</section>
</section>
<section id="blip">
<h2><span class="section-number">26.2. </span>BLIP<a class="headerlink" href="#blip" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3><span class="section-number">26.2.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<p><strong>BLIP</strong> (Bootstrapping Language-Image Pretraining) is a multimodal framework proposed by <strong>Salesforce</strong> in 2022. It <strong>unifies understanding and generation</strong> by introducing cross-modal encoders and decoders, enabling cross-modal information flow, and achieving SOTA results in multiple vision and language tasks. In AIGC, it’s commonly used to generate prompts for images. Good prompts are crucial for fine-tuning cross-attention, for example, the Automatic Prompt in ControlNet is generated by BLIP.</p>
<p>The term <strong>Bootstrapping</strong> refers to the fact that the training data comes from web image-text pairs, which contain a lot of noise. Therefore, <strong>an online data labeling and cleaning task was added</strong>, where the processed data is used to iterate on the original model.</p>
</section>
<section id="model-architecture">
<h3><span class="section-number">26.2.2. </span>Model Architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h3>
<p>BLIP introduces MED (Multimodal mixture of Encoder-Decoder), a multimodal hybrid architecture that enables effective multi-task pre-training and transfer learning. MED includes</p>
<ul class="simple">
<li><p>Two unimodal encoders (<strong>Image Encoder</strong> and <strong>Text Encoder</strong>);</p></li>
<li><p>One image-grounded text encoder, which uses additional cross-attention layers to model vision-language interactions,</p></li>
<li><p>One image-grounded text decoder, which uses causal self-attention layers on text interactions and a shared cross-attention layers to model vision-language interactions.</p></li>
</ul>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip-arch">
<a class="reference internal image-reference" href="../../_images/BLIP_arch.png"><img alt="../../_images/BLIP_arch.png" src="../../_images/BLIP_arch.png" style="width: 790.8px; height: 324.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.3 </span><span class="caption-text">Pre-training model architecture and objectives of BLIP (same parameters have the same color). Image from <span id="id3">[<a class="reference internal" href="#id1566" title="Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, 12888–12900. PMLR, 2022.">LLXH22</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip-arch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There are three different loss functions to align different modules:</p>
<ul class="simple">
<li><p>An image-text contrastive (ITC) loss to align the image and text encoders.</p></li>
<li><p>An image-text matching (ITM) loss to guide the image-grounded encoder to distinguish between positive and negative image-text pairs.</p></li>
<li><p>A language modeling (LM) loss to guide the generation of text captions given images via autoregression.</p></li>
</ul>
</section>
<section id="training-strategy">
<h3><span class="section-number">26.2.3. </span>Training Strategy<a class="headerlink" href="#training-strategy" title="Link to this heading">#</a></h3>
<p>To effectively utilize the large amount of image-text pairs obtained from the web, which often contain inaccurate or even incorrect information, BLIP proposes the <strong>CapFilt</strong> (Captioning and Filtering) module for caption generation and filtering. It first learns from noisy image-text pairs, then generates and filters to create new datasets, which are used to iteratively optimize the original model.</p>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip-training-strategy">
<a class="reference internal image-reference" href="../../_images/BLIP_training_strategy.png"><img alt="../../_images/BLIP_training_strategy.png" src="../../_images/BLIP_training_strategy.png" style="width: 801.6px; height: 268.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.4 </span><span class="caption-text">Learning framework of BLIP. A captioner is used to produce synthetic captions for web images, and a filter is used to remove
noisy image-text pairs. The resulted bootstrapped dataset is used to pre-train a new model. Image from <span id="id4">[<a class="reference internal" href="#id1566" title="Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, 12888–12900. PMLR, 2022.">LLXH22</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip-training-strategy" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>CapFilt consists of</strong> two modules:</p>
<ul class="simple">
<li><p><strong>captioner</strong>, which generates captions for web images,</p></li>
<li><p><strong>Filter</strong>, which filters out noisy captions from both original web text and synthetic text.</p></li>
</ul>
<p>Experimental results show that by generating more training data from captioner and filter, <strong>the BLIP model achieves consistent performance improvements across various downstream tasks</strong>, including image-text retrieval, image captioning, visual question answering, visual reasoning, and visual dialogue.</p>
<p>Both Captioner and Filter are initialized from pre-trained models and separately fine-tuned on manually annotated datasets.</p>
<p><strong>Captioner is an image-grounded text decoder,</strong> which is fine-tuned on manually annotated datasets with an LM objective, decoding text for given images. Here, given web images, the Captioner generates synthetic captions.</p>
<p><strong>Filter is an image-grounded text encoder</strong> that is fine-tuned with <strong>ITC</strong> and <strong>ITM</strong> objectives to learn whether text matches images, removing noisy text from both original web text and synthetic text.</p>
</section>
<section id="downstream-application">
<h3><span class="section-number">26.2.4. </span>Downstream Application<a class="headerlink" href="#downstream-application" title="Link to this heading">#</a></h3>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip-downstream">
<a class="reference internal image-reference" href="../../_images/BLIP_downstream.png"><img alt="../../_images/BLIP_downstream.png" src="../../_images/BLIP_downstream.png" style="width: 395.4px; height: 445.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.5 </span><span class="caption-text">Model architecture for the downstream tasks. Q: question; C: caption; QA: question-answer pair. Image from <span id="id5">[<a class="reference internal" href="#id1566" title="Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, 12888–12900. PMLR, 2022.">LLXH22</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip-downstream" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Visual Question Answering (VQA)</strong> requires the model to predict an answer given an image and a question. During downstream finetuning, we can first encode the image-question input into multimodal embeddings using an image encoder and a text encoder with cross-attention to image embedding. The resulted multimodal embeddings are then given to an text to produce answer. The VQA model is finetuned with the LM loss using ground-truth answers as targets.</p>
<p><strong>Natural Language Visual Reasoning (NLVR2)</strong> asks the model to predict whether a sentence describes a pair of images. First, each image is encoded into embeddings using unimodal image encoder. These image embeddings are then sent to the image-grounded text encoder, in which
there exist two cross-attention layers to process the two input images, and their outputs are merged and fed to the FFN. The merge layer performs simple average pooling in the first 6 layers of the encoder, and performs concatenation followed by a linear projection in layer 6-12. An MLP classifier is applied on the output embedding of the [Encode] token.</p>
<p><strong>Visual Dialog (VisDial)</strong> extends VQA in a natural conversational setting, where the model needs to predict an answer not only based on the image-question pair, but also considering the dialog history and the image’s caption. <a class="reference internal" href="#chapter-vllm-visionlanguagepretrain-blip-downstream"><span class="std std-numref">Fig. 26.5</span></a> adopts the discriminative setting where the model ranks a pool of answer candidates. Image and caption text are encoded separately using unimodal encoders, then image and text embeddings are concatenated and passed to the dialog encoder through cross-attention. The dialog encoder is trained with the ITM loss to discriminate whether the answer is true or false for a question, given the entire dialog history and the image-caption embeddings.</p>
</section>
</section>
<section id="blip-2">
<h2><span class="section-number">26.3. </span>BLIP-2<a class="headerlink" href="#blip-2" title="Link to this heading">#</a></h2>
<section id="id6">
<h3><span class="section-number">26.3.1. </span>Model Architecture<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>BLIP-2 <span id="id7">[<a class="reference internal" href="#id1567" title="Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 19730–19742. PMLR, 2023.">LLSH23</a>]</span> from Saleforces, which aimed to improve multimodal performance and reduces training costs by leveraging pre-trained vision models and language models. The pre-trained vision models provide high-quality visual representations, while the pre-trained language models offer powerful language generation capabilities.</p>
<p>BLIP-2 consists of a pre-trained Image Encoder, a pre-trained Large Language Model, and a learnable Q-Former.</p>
<ul class="simple">
<li><p><strong>Image Encoder</strong>: Responsible for extracting visual features from input images. The paper experimented with two network architectures: ViT-L/14 trained with CLIP and ViT-g/14 trained with EVA-CLIP.</p></li>
<li><p><strong>Large Language Model</strong>: Responsible for text generation. The paper experimented with both decoder-based LLM and encoder-decoder-based LLM.</p></li>
<li><p><strong>Q-Former</strong>: Responsible for bridging the gap between visual and language modalities. It consists of two sub-modules: <strong>Image Transformer</strong> and <strong>Text Transformer</strong>, which share the same self-attention layers.</p>
<ul>
<li><p><strong>Image Transformer</strong> extracts visual features by interacting with the image encoder. Its input consists of learnable Queries, which interact with each other through self-attention layers and with frozen image features through cross-attention layers. It can also interact with text through shared self-attention layers.</p></li>
<li><p><strong>Text Transformer</strong> serves as both text encoder and decoder. Its self-attention layers are shared with the Image Transformer. Depending on the pre-training task, different self-attention masks are applied to control how Queries and text interact.</p></li>
</ul>
</li>
</ul>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip2-blip2-overview">
<a class="reference internal image-reference" href="../../_images/BLIP2_overview.png"><img alt="../../_images/BLIP2_overview.png" src="../../_images/BLIP2_overview.png" style="width: 386.09999999999997px; height: 179.1px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.6 </span><span class="caption-text">Overview of BLIP-2 framework. A lightweight Querying Transformer is trained via a two-stage strategy to bridge the modality gap. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen LLM, which enables zero-shot instructed image-totext generation. Image from <span id="id8">[<a class="reference internal" href="#id1567" title="Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 19730–19742. PMLR, 2023.">LLSH23</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip2-blip2-overview" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 26.1 </span> (Design considerations for fusing image understanding signals)</p>
<section class="remark-content" id="proof-content">
<p>One way to fuse image understanding signal into text LLM is first going through a <strong>semantic conversion stage</strong> - using caption models to generate textual descriptions of images/videos (scenes, events, entities, etc.), then feeding these textual descriptions (or perform some preprocessing, like summarization beforehand) into the LLM.</p>
<p>This approach involves indirectly converting visual semantics into textual semantics through a visual Captioner (<strong>semantic conversion stage</strong>), then better adapting the textual semantics through Prompt+LLM (<strong>semantic adaptation stage</strong>), and finally using this as input for the target LLM to create an MLLM (<strong>semantic fusion stage</strong>).</p>
<p>While this approach is straightforward and easy to implement, its limitations are obvious - there is information loss at the <strong>semantic conversion stage</strong>, in which the final LLM model cannot perceive fine-grained information from the original visual input, which severely limits the MLLM’s potential.</p>
<p>One design idea is Q-Former is to use a learnable query vector plus cross-attention mechanism to directly fuse image signals features in the semantic vector space. The usage of learnable query vector also resembles the idea of <strong>learnable soft prompt</strong> <span id="id9">[<a class="reference internal" href="../chapter_training/finetuning.html#id1553" title="Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.">LARC21</a>]</span> in LLM fine-tuning.</p>
</section>
</div></section>
<section id="id10">
<h3><span class="section-number">26.3.2. </span>Training Strategy<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>To reduce computational costs and avoid catastrophic forgetting, BLIP-2 freezes both pre-trained image and language models during pre-training. However, simply freezing pre-trained model parameters makes it difficult to align visual and text features. To address this, BLIP-2 proposes a two-stage Q-Former pre-training approach to bridge the modality gap:</p>
<ul class="simple">
<li><p>Representation learning stage</p></li>
<li><p>Generative learning stage.</p></li>
</ul>
<p><strong>Representation Learning Stage</strong>: In the representation learning stage, Q-Former is connected to the frozen Image Encoder, using image-text pairs as the training set. Through jointly optimizing three pre-training objectives, different attention mask strategies are applied between Query and Text to control the interaction between Image Transformer and Text Transformer.
Fig.4 Model architecture of Q-Former and Vision-Language Representation Learning</p>
<ul class="simple">
<li><p><strong>ITC (Image-Text Contrastive Learning)</strong>: ITC’s optimization objective is to align image embeddings with text embeddings, aligning Query embeddings from Image Transformer output with text embeddings from Text Transformer output. To prevent information leakage, <strong>ITC uses unimodal self-attention masks, not allowing Query and Text to attend to each other</strong>. Specifically, Text Transformer’s text embedding is the output embedding of the [CLS] token, while Query embeddings contain multiple output embeddings. Therefore, it first computes the similarity between each Query output embedding and the text embedding, then selects the highest one as the image-text similarity.</p></li>
<li><p><strong>ITG (Image-grounded Text Generation)</strong>: ITG trains Q-Former to generate text conditioned on the input image, forcing Query to extract visual features containing textual information. Since Q-Former’s architecture doesn’t allow direct interaction between the frozen image encoder and text tokens, the information needed for generating text must first be extracted by Query and then passed to text tokens through self-attention layers. <strong>ITG uses multimodal Causal Attention masks to control Query and Text interaction</strong> - Queries can attend to each other but not to Text tokens, while each Text token can process all Queries and its preceding Text tokens. Here, the [CLS] token is replaced with a new [DEC] token as the first text token to indicate the decoding task.</p></li>
<li><p><strong>ITM (Image-Text Matching)</strong>: ITM is a binary classification task that learns fine-grained alignment between image and text representations by predicting whether an image-text pair is a positive or negative match. Each Query embedding from Image Transformer output is input into a binary linear classifier to obtain corresponding logits, which are then averaged to calculate the matching score. <strong>ITM uses bidirectional self-attention masks, allowing all Queries and Text to attend to each other</strong>.</p></li>
</ul>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip2-representative-training">
<a class="reference internal image-reference" href="../../_images/representative_training.png"><img alt="../../_images/representative_training.png" src="../../_images/representative_training.png" style="width: 859.0px; height: 185.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.7 </span><span class="caption-text">(Left) Model architecture of Q-Former and BLIP-2’s first-stage vision-language representation learning objectives. We jointly
optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the
text. (Right) The self-attention masking strategy for each objective to control query-text interaction. Image from <span id="id11">[<a class="reference internal" href="#id1567" title="Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 19730–19742. PMLR, 2023.">LLSH23</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip2-representative-training" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Generative Learning</strong>: In the generative pre-training stage, Q-Former is connected to the frozen LLM to leverage the LLM’s language generation capabilities. Here, a fully connected layer linearly projects the output Query embeddings to the same dimension as the LLM’s text embeddings, then adds these projected Query embeddings before the input text embeddings. Since Q-Former has been pre-trained to extract visual representations containing language information, it can effectively serve as an information bottleneck, providing the most useful information to the LLM while removing irrelevant visual information, reducing the burden on the LLM to learn vision-language alignment.</p>
<p>BLIP-2 experimented with two types of LLMs: decoder-based LLMs and encoder-decoder-based LLMs. For decoder-based LLMs, language modeling loss is used for pre-training, where the frozen LLM’s task is to generate text based on Q-Former’s visual representations. For encoder-decoder-based LLMs, prefix language modeling loss is used for pre-training, where the text is split into two parts: the prefix text is concatenated with visual representations as input to the LLM encoder, while the suffix text serves as the generation target for the LLM decoder.</p>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-blip2-generative-training">
<a class="reference internal image-reference" href="../../_images/generative_training.png"><img alt="../../_images/generative_training.png" src="../../_images/generative_training.png" style="width: 774.5px; height: 238.5px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.8 </span><span class="caption-text">BLIP-2 second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs).
(Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). The
fully-connected layer adapts from the output dimension of the Q-Former to the input dimension of the chosen LLM. Image from <span id="id12">[<a class="reference internal" href="#id1567" title="Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 19730–19742. PMLR, 2023.">LLSH23</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-blip2-generative-training" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="instruct-blip">
<h2><span class="section-number">26.4. </span>Instruct BLIP<a class="headerlink" href="#instruct-blip" title="Link to this heading">#</a></h2>
<p>Large-scale pre-training and instruction tuning <span id="id13">[<a class="reference internal" href="../chapter_training/finetuning.html#id58" title="Renze Lou, Kai Zhang, and Wenpeng Yin. Large language model instruction following: a survey of progresses and challenges. Computational Linguistics, pages 1–10, 2024.">LZY24</a>]</span> have been successful at creating general-purpose language models. InstructBLIP <span id="id14">[<a class="reference internal" href="#id1568" title="Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2024.">LLWL24</a>]</span>, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visuallanguage tasks through a unified natural language interface.</p>
<p>InstructBLIP adopts a model architecture similar to BLIP-2, which consists of a visual encoder, Q-Former, and LLM. The visual encoder extracts features from the input image and feeds them into the Q-Former. Q-Former’s input includes both <strong>learnable Queries</strong> and <strong>Instruction</strong>. The internal structure of Q-Former is shown in <span id="id15">[]</span>, where the learnable Queries interact with the Instruction through Self-Attention, and interact with the input image features through Cross-Attention, encouraging the extraction of task-relevant image features.</p>
<p>Q-Former’s output is fed into the LLM through an FC layer. The pre-training process of Q-Former follows BLIP-2’s two steps:</p>
<ul class="simple">
<li><p>Without using LLM, pre-train Q-Former’s parameters while keeping visual encoder parameters fixed, with vision-language modeling as the training objective.</p></li>
<li><p>Train Q-Former’s parameters while keeping LLM parameters fixed, with text generation as the training objective.</p></li>
</ul>
<figure class="align-default" id="chapter-vllm-visionlanguagepretrain-instruct-blip-demo">
<a class="reference internal image-reference" href="../../_images/instruct_BLIP_demo.png"><img alt="../../_images/instruct_BLIP_demo.png" src="../../_images/instruct_BLIP_demo.png" style="width: 786.0999999999999px; height: 333.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26.9 </span><span class="caption-text">Model architecture of InstructBLIP. The Q-Former extracts instruction-aware visual features
from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozen LLM. We instruction-tune the model with the language modeling loss to generate the response. Image from <span id="id16">[<a class="reference internal" href="#id1568" title="Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2024.">LLWL24</a>]</span>.</span><a class="headerlink" href="#chapter-vllm-visionlanguagepretrain-instruct-blip-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Task</p></th>
<th class="head text-left"><p>Instruction Template</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Image Captioning</p></td>
<td class="text-left"><p><Image>A short image caption:</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image> A short image description:</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image> Write a short description for the image.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Can you briefly explain what you see in the image?</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Could you use a few words to describe what you perceive in the photo?</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Use a few words to illustrate what is happening in the picture.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>VQA</p></td>
<td class="text-left"><p><Image> Question</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Question: Question <Image> Question A short answer to the question is</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Q: Question A:</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>Use the provided image to answer the question: Question Provide your answer as short as possible:</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>VQG</p></td>
<td class="text-left"><p><Image>Given the image, generate a question whose answer is: {Answer}.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p>Question: <Image>Based on the image, provide a question with the answer: {Answer}.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-left"><p>Question: <Image>Given the visual representation, create a question for which the answer is “{Answer}”.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-left"><p><Image>From the image provided, craft a question that leads to the reply: {Answer}.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-left"><p>Question: <Image>Considering the picture, come up with a question where the answer is: {Answer}.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="bibliography">
<h2><span class="section-number">26.5. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id17">
<div role="list" class="citation-list">
<div class="citation" id="id1548" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">LARC21</a><span class="fn-bracket">]</span></span>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. <em>arXiv preprint arXiv:2104.08691</em>, 2021.</p>
</div>
<div class="citation" id="id1567" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LLSH23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id8">2</a>,<a role="doc-backlink" href="#id11">3</a>,<a role="doc-backlink" href="#id12">4</a>)</span>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In <em>International conference on machine learning</em>, 19730–19742. PMLR, 2023.</p>
</div>
<div class="citation" id="id1566" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LLXH22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>)</span>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. In <em>International conference on machine learning</em>, 12888–12900. PMLR, 2022.</p>
</div>
<div class="citation" id="id1568" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LLWL24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id14">1</a>,<a role="doc-backlink" href="#id16">2</a>)</span>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. <em>Advances in neural information processing systems</em>, 2024.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">LZY24</a><span class="fn-bracket">]</span></span>
<p>Renze Lou, Kai Zhang, and Wenpeng Yin. Large language model instruction following: a survey of progresses and challenges. <em>Computational Linguistics</em>, pages 1–10, 2024.</p>
</div>
<div class="citation" id="id458" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RKH+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>)</span>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and et al. Learning transferable visual models from natural language supervision. In <em>International Conference on Machine Learning</em>, 8748–8763. PMLR, 2021.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_multimodality"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_rag/advanced_rag.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">25. </span>Advanced RAG (WIP)</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_application_IR/information_retrieval_fundamentals.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">27. </span>Information Retrieval and Text Ranking</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clip">26.1. CLIP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">26.1.1. Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-implementations">26.1.2. Detailed Implementations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blip">26.2. BLIP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">26.2.1. Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">26.2.2. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-strategy">26.2.3. Training Strategy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downstream-application">26.2.4. Downstream Application</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blip-2">26.3. BLIP-2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">26.3.1. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">26.3.2. Training Strategy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instruct-blip">26.4. Instruct BLIP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">26.5. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>