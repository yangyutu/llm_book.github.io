
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>20. Advanced Prompting Techniques &#8212; LLM Foundations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/chapter_prompt/advanced_prompt';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="21. RAG" href="../chapter_rag/basic_rag.html" />
    <link rel="prev" title="19. Basic Prompting" href="basic_prompt.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/qe-logo-large.png" class="logo__image only-light" alt="LLM Foundations - Home"/>
    <script>document.write(`<img src="../../_static/qe-logo-large.png" class="logo__image only-dark" alt="LLM Foundations - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">1. Introduction: LLM in the Age of AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/language_models.html">2. Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/neural_language_models.html">3. Early Neural Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/word_embeddings.html">4. Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/transformers.html">5. Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/bert.html">6. BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/t5.html">7. Seq2Seq: T5 and BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_foundation/GPT_series.html">8. GPT Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_dense_architectures.html">9. LLM Architectures Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chapter_LLM_arch/annotated_llama.html">10. *Annotated LLama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_LLM_arch/LLM_moe_sparse_architectures.html">11. MOE Sparse Architectures (WIP)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/training_fundamentals.html">12. LLM Training Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/finetuning.html">13. LLM Finetuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/alignment.html">14. LLM Alignement and Preference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/reinforcement_learning.html">15. *Reinforcement Learning Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_training/accelerated_training.html">16. LLM Training Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_fundamentals.html">17. Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_inference/inference_acceleration.html">18. Inference Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prompting</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="basic_prompt.html">19. Basic Prompting</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">20. Advanced Prompting Techniques</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG and Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_rag/basic_rag.html">21. RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Application in Information Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/information_retrieval_fundamentals.html">22. Information Retrieval and Text Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_application_IR/application_LLM_in_IR.html">23. Application of LLM in IR (WIP)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advanced Prompting Techniques</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cot-with-self-consistency">20.1. CoT with Self-Consistency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-generated-cot">20.2. Self-Generated CoT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-back-prompting">20.3. Step Back Prompting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-shuffling-ensembling">20.4. Choice Shuffling Ensembling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-in-context-learning">20.5. Dynamic In-Context Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-together-example-med-prompt">20.6. Combining Together Example: Med Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.7. Bibliography</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-prompting-techniques">
<h1><span class="section-number">20. </span>Advanced Prompting Techniques<a class="headerlink" href="#advanced-prompting-techniques" title="Link to this heading">#</a></h1>
<section id="cot-with-self-consistency">
<h2><span class="section-number">20.1. </span>CoT with Self-Consistency<a class="headerlink" href="#cot-with-self-consistency" title="Link to this heading">#</a></h2>
<p><strong>Chain-of-thought</strong> prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. <span id="id1">[<a class="reference internal" href="#id533" title="Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.">WWS+22</a>]</span> propose a <strong>self-consistency</strong> strategy to further improve the performance of chain-of-thought prompting.
The key idea is that <a class="reference internal" href="#chapter-prompt-fig-advanced-prompt-cot-self-consistency"><span class="std std-numref">Fig. 20.1</span></a>:</p>
<ol class="arabic simple">
<li><p>First we samples a diverse set of reasoning paths</p></li>
<li><p>Then we select the most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency is based on the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.</p></li>
</ol>
<figure class="align-default" id="chapter-prompt-fig-advanced-prompt-cot-self-consistency">
<a class="reference internal image-reference" href="../../_images/cot_self_consistency.png"><img alt="../../_images/cot_self_consistency.png" src="../../_images/cot_self_consistency.png" style="width: 853.6px; height: 397.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.1 </span><span class="caption-text">CoT with self-consistency. Image from <span id="id2">[<a class="reference internal" href="#id1543" title="Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. 2023. URL: https://arxiv.org/abs/2203.11171, arXiv:2203.11171.">WWS+23</a>]</span>.</span><a class="headerlink" href="#chapter-prompt-fig-advanced-prompt-cot-self-consistency" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- Sampling scheme:
    - for UL2-20B and LaMDA-137B we applied temperature sampling with T = 0.5 and truncated at the top-k (k = 40) tokens with the highest probability, 
    - for PaLM-540B we applied T = 0.7, k = 40, 
    - for GPT-3 we use T = 0.7 without top-k truncation.

```{figure} ../img/chapter_prompt/advanced_prompt/cot_self_consistency_example.png
---
scale: 80%
name: chapter_prompt_fig_advanced_prompt_cot_self_consistency_example
---
CoT with self-consistency examples.
``` -->
<p>As shown in the following [<a class="reference internal" href="#chapter-prompt-fig-advanced-prompt-cot-self-consistency-num-paths"><span class="std std-numref">Fig. 20.2</span></a>], using over LaMDA-137B, self-consistency (blue) significantly improves accuracy over CoT-prompting with greedy decoding (orange) across arithmetic and commonsense reasoning tasks. Sampling a higher number of diverse reasoning paths consistently improves reasoning accuracy.</p>
<figure class="align-default" id="chapter-prompt-fig-advanced-prompt-cot-self-consistency-num-paths">
<a class="reference internal image-reference" href="../../_images/cot_self_consistency_num_paths.png"><img alt="../../_images/cot_self_consistency_num_paths.png" src="../../_images/cot_self_consistency_num_paths.png" style="width: 855.2px; height: 146.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.2 </span><span class="caption-text">The effect of the number of sampled reasoning paths.</span><a class="headerlink" href="#chapter-prompt-fig-advanced-prompt-cot-self-consistency-num-paths" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="self-generated-cot">
<h2><span class="section-number">20.2. </span>Self-Generated CoT<a class="headerlink" href="#self-generated-cot" title="Link to this heading">#</a></h2>
<p>When using CoT in prompt writing, we often need experts to manually compose CoT example reasoning steps. Self-Generated CoT aims to automate the CoT examples using LLM itself. The finding is that CoT rationales generated by GPT-4 are longer and provide finer-grained step-by-step reasoning logic. The following examples compare the manually crafted CoT and self-generated CoT <a class="reference internal" href="#chapter-prompt-fig-advanced-prompt-self-cot"><span class="std std-numref">Fig. 20.3</span></a>.</p>
<!-- ![Self-generated CoT demonstration. Comparison of expert-crafted and GPT-4-generated chain-of-thought (CoT) prompts.](../img/chapter_prompt/advanced_prompt/medprompt_cot_example.png)
:label:`chapter_prompt_fig_advanced_prompt_self_cot` -->
<figure class="align-default" id="chapter-prompt-fig-advanced-prompt-self-cot">
<a class="reference internal image-reference" href="../../_images/medprompt_cot_example.png"><img alt="../../_images/medprompt_cot_example.png" src="../../_images/medprompt_cot_example.png" style="width: 599.4px; height: 453.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.3 </span><span class="caption-text">Self-generated CoT demonstration. Comparison of expert-crafted and GPT-4-generated chain-of-thought (CoT) prompts.</span><a class="headerlink" href="#chapter-prompt-fig-advanced-prompt-self-cot" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Note that a key challenge with the self-generated CoT rationales are they can be incorrect reasoning chains due to halluciation. This can be mitigated by asking LLM to produce both reasoning chain as well the likelihood answer (similar to two step zero-shot CoT <a class="reference internal" href="basic_prompt.html#chapter-prompt-fig-basic-prompt-zero-shot-cot"><span class="std std-numref">Fig. 19.3</span></a>). If this answer does not match the ground truth label, then the reasoning sample can be discarded. Note that there can still be cases that incorrect reasoning chain leading to correct answers.</p>
</section>
<section id="step-back-prompting">
<span id="chapter-prompt-sec-step-back-prompting"></span><h2><span class="section-number">20.3. </span>Step Back Prompting<a class="headerlink" href="#step-back-prompting" title="Link to this heading">#</a></h2>
<p>Complex multi-step reasoning tasks, particularly domain specific reasoning such as Physics and Chemistry remain challenge to LLM. These questions are usually  knowledge-intensive question answering, involving many low-level details, thus requiring factual knowledge, multi-hop commonsense reasoning.</p>
<p>For example, a question from chemical physics -  <em>What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8 ?</em> would require the understanding the physical principle on gas first, and then perform reasoning to get the answer.</p>
<p>The idea of <strong>Step Back prompting</strong> <span id="id3">[<a class="reference internal" href="#id1542" title="Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023.">ZMC+23</a>]</span> is inspired by how human approaches these type of taks - humans often step back and do abstractions to arrive at
high-level principles to guide the reasoning process [<a class="reference internal" href="#chapter-prompt-fig-advanced-prompt-step-back-demo"><span class="std std-numref">Fig. 20.4</span></a>].</p>
<p>The prompting involves two steps:</p>
<ul class="simple">
<li><p><strong>Abstraction</strong>: Instead of addressing the question directly, we first prompt the LLM to ask a generic step-back question about a higher-level principle and relevant context, which encourages the model to produce relevant facts and principles.</p></li>
<li><p><strong>Reasoning</strong>: Grounding on the facts produced in the first stepregarding the high-level concept or principle, the LLM then reason about the solution to the original question.</p></li>
</ul>
<figure class="align-default" id="chapter-prompt-fig-advanced-prompt-step-back-demo">
<a class="reference internal image-reference" href="../../_images/step_back_demo.png"><img alt="../../_images/step_back_demo.png" src="../../_images/step_back_demo.png" style="width: 880.8000000000001px; height: 360.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.4 </span><span class="caption-text">Illustration of Step-Back prompting, which consists of two steps of <strong>Abstraction</strong> and <strong>Reasoning</strong>. Here shows an example of MMLU high-school physics where the first principle of Ideal Gas Law is retrieved via abstraction before generating answers. Image from <span id="id4">[<a class="reference internal" href="#id1542" title="Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023.">ZMC+23</a>]</span>.</span><a class="headerlink" href="#chapter-prompt-fig-advanced-prompt-step-back-demo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The following table shows the performance on MMLU tasks, Step-Back prompting shows furhter improvement on CoT prompting.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id1551">
<caption><span class="caption-number">Table 20.1 </span><span class="caption-text">Performance of Step-Back prompting on MMLU tasks.</span><a class="headerlink" href="#id1551" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Method</p></th>
<th class="head text-center"><p>MMLU Physics</p></th>
<th class="head text-center"><p>MMLU Chemistry</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>GPT-4</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(69.4 \%(2.0 \%)\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(80.9 \%(0.7 \%)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>GPT-4 1-shot</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(78.4 \%(2.4 \%)\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(80.5 \%(1.6 \%)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>GPT-4 + CoT</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(82.9 \%(0.5 \%)\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(85.3 \%(1.0 \%)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>GPT-4 + CoT 1-shot</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(79.3 \%(1.0 \%)\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(82.8 \%(0.5 \%)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>GPT-4 + Step-Back</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 4 . 5 \% ( 1 . 2 \% )}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{8 5 . 6 \% ( 1 . 4 \% )}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="choice-shuffling-ensembling">
<h2><span class="section-number">20.4. </span>Choice Shuffling Ensembling<a class="headerlink" href="#choice-shuffling-ensembling" title="Link to this heading">#</a></h2>
<p>Choice shuffling ensembling combines two key ideas to improve prompting effectiveness for multi-choice questions:</p>
<ul class="simple">
<li><p>Ensembling - combines outputs of multiple model runs to achieve a more robust or accurate result through methods like averaging or majority vote.</p></li>
<li><p>Bias mitigation - shuffle the choices can mitigate the position bias that LLM or GPT-4 has (a tendency to favor certain options in multiple choice answers over others regardless of the option content)</p></li>
</ul>
<p>With choice shuffling, we shuffle the relative order of the answer choices before generating each reasoning path. We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling.</p>
<p>Choice shuffling has an additional benefit of increasing the diversity of each reasoning path beyond temperature sampling, thereby also improving the quality of the final ensemble.</p>
</section>
<section id="dynamic-in-context-learning">
<h2><span class="section-number">20.5. </span>Dynamic In-Context Learning<a class="headerlink" href="#dynamic-in-context-learning" title="Link to this heading">#</a></h2>
<p>In the most common form of in-context learning (also known as few-shot learning),LLM is prompted with a few demonstrations, and produce responses following the task format in the prompt. These few-shot examples used in prompting for a particular task are typically fixed; they are unchanged across test examples.</p>
<p>To achieve the best testing performance, it is necessary that these few-shot examples selected are broadly representative and relevant to a wide distribution of text examples.</p>
<p>In the dynamic few-shot prompting setting, we can select can select different few-shot examples for different task inputs. The selection criterion can be based on the simiarlity to the testing case at hand.</p>
<p>For example, at inference time, given a test question, we re-embed the test sample with the same embedding model used during pre-processing, and utilize kNN to retrieve similar examples from the preprocessed examples.</p>
</section>
<section id="combining-together-example-med-prompt">
<h2><span class="section-number">20.6. </span>Combining Together Example: Med Prompt<a class="headerlink" href="#combining-together-example-med-prompt" title="Link to this heading">#</a></h2>
<p>By using advanced prompting techniques, GPT-4 demonstrated significant capabilities in areas involving significant domain knowledge, such as medicine, which challenges the assumption that it requires intensive domain-specific training to match specialist capabilities​.</p>
<p>In <span id="id5">[<a class="reference internal" href="#id532" title="Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, and others. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023.">NLZ+23</a>]</span>, authors carried out a systematic exploration of prompt engineering strategies that significantly enhance GPT-4’s performance in medical question-answering tasks. Medprompt integrates techniques like in-context learning and chain-of-thought reasoning, leading to a 27% reduction in error rate on the MedQA dataset compared to specialist models.</p>
<p>Medprompt employs dynamic few-shot selection, self-generated chain of thought, and choice shuffle ensembling. These techniques collectively contribute to its high performance in medical benchmarks.</p>
<p>An ablation study in the following highlighted the relative contributions of Medprompt’s components.Each technique incrementally improves the model’s performance, with the final accuracy reaching 90.2%. The most significant improvements come from GPT Self-Generated CoT (+3.4%) and the 5x Choice-Shuffle Ensemble Layer (+2.1%).</p>
<figure class="align-default" id="chapter-prompt-fig-advanced-prompt-med-prompt-abalation-study">
<a class="reference internal image-reference" href="../../_images/med_prompt_abalation_study.png"><img alt="../../_images/med_prompt_abalation_study.png" src="../../_images/med_prompt_abalation_study.png" style="width: 795.0px; height: 419.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20.5 </span><span class="caption-text">Relative contributions of different components of Medprompt via an ablation study.</span><a class="headerlink" href="#chapter-prompt-fig-advanced-prompt-med-prompt-abalation-study" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="bibliography">
<h2><span class="section-number">20.7. </span>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id532" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">NLZ+23</a><span class="fn-bracket">]</span></span>
<p>Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, and others. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. <em>arXiv preprint arXiv:2311.16452</em>, 2023.</p>
</div>
<div class="citation" id="id533" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">WWS+22</a><span class="fn-bracket">]</span></span>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. <em>arXiv preprint arXiv:2203.11171</em>, 2022.</p>
</div>
<div class="citation" id="id1543" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">WWS+23</a><span class="fn-bracket">]</span></span>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2203.11171">https://arxiv.org/abs/2203.11171</a>, <a class="reference external" href="https://arxiv.org/abs/2203.11171">arXiv:2203.11171</a>.</p>
</div>
<div class="citation" id="id1542" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZMC+23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: evoking reasoning via abstraction in large language models. <em>arXiv preprint arXiv:2310.06117</em>, 2023.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/chapter_prompt"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="basic_prompt.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Basic Prompting</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_rag/basic_rag.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>RAG</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cot-with-self-consistency">20.1. CoT with Self-Consistency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-generated-cot">20.2. Self-Generated CoT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-back-prompting">20.3. Step Back Prompting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-shuffling-ensembling">20.4. Choice Shuffling Ensembling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-in-context-learning">20.5. Dynamic In-Context Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-together-example-med-prompt">20.6. Combining Together Example: Med Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliography">20.7. Bibliography</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yuguang Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>